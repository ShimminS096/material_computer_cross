{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c1f6440-13e4-432f-9613-5cb4a84dbb95",
   "metadata": {},
   "source": [
    "시각자료 plot 코드만 참고하셔서 학습 진행 후 결과 자료 톡방에 공유해주시면 될 것 같습니다. 쓸데없는 주석처리 해둔 코드들은 지웠고, 추가적으로 코멘트를 달아야 할 정도의 코드는 없어보여서 작성 안했는데, 궁금하신 부분 있으시면 바로바로 톡방에 말씀 남겨주시면 확인 후 답 드리겠습니다 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34216c51-9f1b-45e6-8ff5-c8e713ce1d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### USER SETTING ########\n",
    "train_samples_ratio = 0.8 # train and test split ratio setting\n",
    "ParameterNum = 301 # 301 : Vth / 302 : mobility / 303 : ss / 304 : on/off ratio # 추출하고자 하는 Parameter setting\n",
    "n_epochs = 30000\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "patience = 300\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d65cb7f0-0183-4adb-8d19-abd3fcf17ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "import os\n",
    "import klib\n",
    "import natsort\n",
    "from scipy.stats import skew\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, CSVLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6ce915-db6a-40ad-bc62-892e1a8a9b44",
   "metadata": {},
   "source": [
    "load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4274b3a-1d58-4f83-8b45-fcf76b8e5eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = np.load(r'D:\\TFT(IGZO_SAL)_DL\\DL\\input dataset\\Filtered_Dataset_(20240520 종합).npz')['dataset']\n",
    "dataset_parameter = np.load(r'D:\\TFT(IGZO_SAL)_DL\\Ground Truth\\Ground Truth_(20240520 종합).npz')['dataset']\n",
    "\n",
    "# Drain Current dataset + Parameter(Ground Truth) dataset 병합\n",
    "## col0 to 300 : drain current / col301 : Vth / col302 : mobility / col303 : S-slope / col304 : onoff ratio\n",
    "dataset_id_parameter = np.concatenate((dataset_id, dataset_parameter), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa787b3-4f90-442a-b74b-5b230f8a5bf1",
   "metadata": {},
   "source": [
    "train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "418a4bf9-29e6-4388-8780-a9475ccc00cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainDataset : (485, 305) TestDataset : (121, 305)\n",
      "train_input : (485, 301) / train_target : (485, 1) test_input : (121, 301) / test_target : (121, 1)\n"
     ]
    }
   ],
   "source": [
    "# train and test data split_02 - Applied RandomShuffle Split ---- 작성하신 코드 그대로 참고해서 작성한 부분입니다.\n",
    "\n",
    "df_id_parameter = pd.DataFrame(data = dataset_id_parameter)\n",
    "TrainDataFrame = df_id_parameter.sample(frac = train_samples_ratio, random_state = 24)\n",
    "TestDataFrame = df_id_parameter.drop(TrainDataFrame.index)\n",
    "TrainDataset = np.array(TrainDataFrame)\n",
    "TestDataset = np.array(TestDataFrame)\n",
    "\n",
    "train_input = TrainDataset[:, :301]\n",
    "train_target = TrainDataset[:, ParameterNum]\n",
    "train_target = np.expand_dims(train_target, axis = 1)\n",
    "\n",
    "test_input = TestDataset[:, :301]\n",
    "test_target = TestDataset[:, ParameterNum]\n",
    "test_target = np.expand_dims(test_target, axis = 1)\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "print(f'TrainDataset : {TrainDataset.shape} TestDataset : {TestDataset.shape}')\n",
    "print(f'train_input : {train_input.shape} / train_target : {train_target.shape} \\\n",
    "test_input : {test_input.shape} / test_target : {test_target.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11e2f728-798f-4e05-b033-674abfc7aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressor 성능 평가 지표\n",
    "def RMSE(test_target, test_pred) :\n",
    "    return np.sqrt(mean_squared_error(test_target, test_pred))\n",
    "\n",
    "def MAPE(test_target, test_pred) :\n",
    "\treturn np.mean(np.abs((test_target - test_pred) / test_target)) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad32f09-4413-4741-9391-54be06efa626",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bec338d4-bbd2-45b8-a031-5a58bc0c3c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization - 적용해볼 만한 기법 3가지\n",
    "\n",
    "# log10 transform\n",
    "\n",
    "train_input = np.log10(train_input)\n",
    "test_input = np.log10(test_input)\n",
    "\n",
    "# standardization\n",
    "\n",
    "mean = train_input.mean(axis=0)\n",
    "std = train_input.std(axis=0)\n",
    "train_input = (train_input - mean) / std\n",
    "test_input = (test_input - mean) / std\n",
    "\n",
    "############# On/Off Ratio 학습 시 ! #############\n",
    "# OnOff Ratio ground truth Normalization\n",
    "# train_target = np.log10(train_target)\n",
    "# test_target = np.log10(test_target)\n",
    "############# On/Off Ratio 학습 시 ! #############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e153b9-397b-4a6c-8963-1b03e3fad97c",
   "metadata": {},
   "source": [
    "model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "600087e5-2a04-4805-98b7-99ea77b9d5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               30200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,401\n",
      "Trainable params: 45,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# DNN Model setting\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(train_input.shape[1],)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "model.compile(optimizer = optimizer, loss = 'mse', metrics = ['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9d03831-5f75-4b62-970d-fb4fd4ad1dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping Setting\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor = 'val_loss',\n",
    "                  patience = patience,\n",
    "                 )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "f8aed42f-a03d-4616-97a2-9bd4e3ee5220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13501372768926564352.0000 - mae: 1199677696.0000\n",
      "Epoch 1: val_loss improved from inf to 1880093964996968448.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch01-val_Loss1880093964996968448.000-mae1038418176.000.h5\n",
      "3/3 [==============================] - 1s 162ms/step - loss: 7819563419266187264.0000 - mae: 1038418176.0000 - val_loss: 1880093964996968448.0000 - val_mae: 928032256.0000\n",
      "Epoch 2/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9833831887712288768.0000 - mae: 1173413376.0000\n",
      "Epoch 2: val_loss did not improve from 1880093964996968448.00000\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 7819562319754559488.0000 - mae: 1038418176.0000 - val_loss: 1880093964996968448.0000 - val_mae: 928032256.0000\n",
      "Epoch 3/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7867433406760484864.0000 - mae: 1028594560.0000\n",
      "Epoch 3: val_loss did not improve from 1880093964996968448.00000\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 7819563419266187264.0000 - mae: 1038418176.0000 - val_loss: 1880093964996968448.0000 - val_mae: 928032256.0000\n",
      "Epoch 4/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11921373460300824576.0000 - mae: 1186863872.0000\n",
      "Epoch 4: val_loss did not improve from 1880093964996968448.00000\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 7819562319754559488.0000 - mae: 1038418176.0000 - val_loss: 1880093964996968448.0000 - val_mae: 928032256.0000\n",
      "Epoch 5/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 916105391681568768.0000 - mae: 825106240.0000\n",
      "Epoch 5: val_loss did not improve from 1880093964996968448.00000\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 7819562319754559488.0000 - mae: 1038418112.0000 - val_loss: 1880093964996968448.0000 - val_mae: 928032256.0000\n",
      "Epoch 6/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11552702813461020672.0000 - mae: 1097179008.0000\n",
      "Epoch 6: val_loss did not improve from 1880093964996968448.00000\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 7819562319754559488.0000 - mae: 1038418176.0000 - val_loss: 1880093964996968448.0000 - val_mae: 928032256.0000\n",
      "Epoch 7/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 855267901488234496.0000 - mae: 805280768.0000\n",
      "Epoch 7: val_loss did not improve from 1880093964996968448.00000\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 7819563419266187264.0000 - mae: 1038418112.0000 - val_loss: 1880093964996968448.0000 - val_mae: 928032256.0000\n",
      "Epoch 8/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13592442018520367104.0000 - mae: 1255588608.0000\n",
      "Epoch 8: val_loss did not improve from 1880093964996968448.00000\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 7819562319754559488.0000 - mae: 1038418112.0000 - val_loss: 1880093964996968448.0000 - val_mae: 928032256.0000\n",
      "Epoch 9/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20692942975162908672.0000 - mae: 1517145472.0000\n",
      "Epoch 9: val_loss improved from 1880093964996968448.00000 to 1880093827558014976.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch09-val_Loss1880093827558014976.000-mae1038418112.000.h5\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 7819562319754559488.0000 - mae: 1038418112.0000 - val_loss: 1880093827558014976.0000 - val_mae: 928032256.0000\n",
      "Epoch 10/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20345213626785726464.0000 - mae: 1452314880.0000\n",
      "Epoch 10: val_loss did not improve from 1880093827558014976.00000\n",
      "3/3 [==============================] - 0s 49ms/step - loss: 7819563419266187264.0000 - mae: 1038418112.0000 - val_loss: 1880093827558014976.0000 - val_mae: 928032256.0000\n",
      "Epoch 11/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20114881333929443328.0000 - mae: 1390860672.0000\n",
      "Epoch 11: val_loss did not improve from 1880093827558014976.00000\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 7819562319754559488.0000 - mae: 1038418112.0000 - val_loss: 1880093827558014976.0000 - val_mae: 928032128.0000\n",
      "Epoch 12/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1082007746259189760.0000 - mae: 843176448.0000\n",
      "Epoch 12: val_loss improved from 1880093827558014976.00000 to 1880093690119061504.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch12-val_Loss1880093690119061504.000-mae1038417920.000.h5\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 7819562319754559488.0000 - mae: 1038417920.0000 - val_loss: 1880093690119061504.0000 - val_mae: 928032128.0000\n",
      "Epoch 13/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 826415823022194688.0000 - mae: 795016704.0000\n",
      "Epoch 13: val_loss improved from 1880093690119061504.00000 to 1880093552680108032.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch13-val_Loss1880093552680108032.000-mae1038417920.000.h5\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 7819562319754559488.0000 - mae: 1038417920.0000 - val_loss: 1880093552680108032.0000 - val_mae: 928032064.0000\n",
      "Epoch 14/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 995944710545604608.0000 - mae: 844157696.0000\n",
      "Epoch 14: val_loss improved from 1880093552680108032.00000 to 1880093415241154560.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch14-val_Loss1880093415241154560.000-mae1038417920.000.h5\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 7819561769998745600.0000 - mae: 1038417920.0000 - val_loss: 1880093415241154560.0000 - val_mae: 928032000.0000\n",
      "Epoch 15/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18614844008433713152.0000 - mae: 1368935168.0000\n",
      "Epoch 15: val_loss improved from 1880093415241154560.00000 to 1880093277802201088.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch15-val_Loss1880093277802201088.000-mae1038417856.000.h5\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 7819562319754559488.0000 - mae: 1038417856.0000 - val_loss: 1880093277802201088.0000 - val_mae: 928031936.0000\n",
      "Epoch 16/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 737980247374299136.0000 - mae: 750741760.0000\n",
      "Epoch 16: val_loss improved from 1880093277802201088.00000 to 1880093140363247616.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch16-val_Loss1880093140363247616.000-mae1038417728.000.h5\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 7819562319754559488.0000 - mae: 1038417728.0000 - val_loss: 1880093140363247616.0000 - val_mae: 928031872.0000\n",
      "Epoch 17/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7819346815475515392.0000 - mae: 1055472128.0000\n",
      "Epoch 17: val_loss improved from 1880093140363247616.00000 to 1880092728046387200.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch17-val_Loss1880092728046387200.000-mae1038417536.000.h5\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 7819561769998745600.0000 - mae: 1038417536.0000 - val_loss: 1880092728046387200.0000 - val_mae: 928031744.0000\n",
      "Epoch 18/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 954945846091186176.0000 - mae: 776937856.0000\n",
      "Epoch 18: val_loss improved from 1880092728046387200.00000 to 1880092315729526784.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch18-val_Loss1880092315729526784.000-mae1038417472.000.h5\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 7819561220242931712.0000 - mae: 1038417472.0000 - val_loss: 1880092315729526784.0000 - val_mae: 928031616.0000\n",
      "Epoch 19/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20684089707536056320.0000 - mae: 1521354624.0000\n",
      "Epoch 19: val_loss improved from 1880092315729526784.00000 to 1880092040851619840.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch19-val_Loss1880092040851619840.000-mae1038417280.000.h5\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 7819561769998745600.0000 - mae: 1038417280.0000 - val_loss: 1880092040851619840.0000 - val_mae: 928031296.0000\n",
      "Epoch 20/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7764211804900687872.0000 - mae: 1036640512.0000\n",
      "Epoch 20: val_loss improved from 1880092040851619840.00000 to 1880091628534759424.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch20-val_Loss1880091628534759424.000-mae1038417088.000.h5\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 7819560120731303936.0000 - mae: 1038417088.0000 - val_loss: 1880091628534759424.0000 - val_mae: 928031040.0000\n",
      "Epoch 21/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18369118552788566016.0000 - mae: 1318889728.0000\n",
      "Epoch 21: val_loss improved from 1880091628534759424.00000 to 1880090941339992064.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch21-val_Loss1880090941339992064.000-mae1038416832.000.h5\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 7819560120731303936.0000 - mae: 1038416832.0000 - val_loss: 1880090941339992064.0000 - val_mae: 928030784.0000\n",
      "Epoch 22/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1049603283162759168.0000 - mae: 808624000.0000\n",
      "Epoch 22: val_loss improved from 1880090941339992064.00000 to 1880090391584178176.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch22-val_Loss1880090391584178176.000-mae1038416448.000.h5\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 7819559570975490048.0000 - mae: 1038416448.0000 - val_loss: 1880090391584178176.0000 - val_mae: 928030464.0000\n",
      "Epoch 23/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7784972233700540416.0000 - mae: 993717184.0000\n",
      "Epoch 23: val_loss improved from 1880090391584178176.00000 to 1880089429511503872.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch23-val_Loss1880089429511503872.000-mae1038416128.000.h5\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 7819556822196420608.0000 - mae: 1038416128.0000 - val_loss: 1880089429511503872.0000 - val_mae: 928029952.0000\n",
      "Epoch 24/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11841072827589459968.0000 - mae: 1139058176.0000\n",
      "Epoch 24: val_loss improved from 1880089429511503872.00000 to 1880088467438829568.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch24-val_Loss1880088467438829568.000-mae1038415680.000.h5\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 7819556272440606720.0000 - mae: 1038415680.0000 - val_loss: 1880088467438829568.0000 - val_mae: 928029504.0000\n",
      "Epoch 25/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7777929861724635136.0000 - mae: 1013300032.0000\n",
      "Epoch 25: val_loss improved from 1880088467438829568.00000 to 1880087642805108736.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch25-val_Loss1880087642805108736.000-mae1038415040.000.h5\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 7819555722684792832.0000 - mae: 1038415040.0000 - val_loss: 1880087642805108736.0000 - val_mae: 928028928.0000\n",
      "Epoch 26/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 765559709850664960.0000 - mae: 782199808.0000\n",
      "Epoch 26: val_loss improved from 1880087642805108736.00000 to 1880086130976620544.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch26-val_Loss1880086130976620544.000-mae1038414464.000.h5\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 7819554073417351168.0000 - mae: 1038414464.0000 - val_loss: 1880086130976620544.0000 - val_mae: 928028288.0000\n",
      "Epoch 27/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1320705192299069440.0000 - mae: 884367104.0000\n",
      "Epoch 27: val_loss improved from 1880086130976620544.00000 to 1880084756587085824.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch27-val_Loss1880084756587085824.000-mae1038413760.000.h5\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 7819552424149909504.0000 - mae: 1038413760.0000 - val_loss: 1880084756587085824.0000 - val_mae: 928027584.0000\n",
      "Epoch 28/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 975137483782094848.0000 - mae: 829498368.0000\n",
      "Epoch 28: val_loss improved from 1880084756587085824.00000 to 1880082969880690688.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch28-val_Loss1880082969880690688.000-mae1038412864.000.h5\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 7819550774882467840.0000 - mae: 1038412864.0000 - val_loss: 1880082969880690688.0000 - val_mae: 928026688.0000\n",
      "Epoch 29/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11592777813270200320.0000 - mae: 1119342720.0000\n",
      "Epoch 29: val_loss improved from 1880082969880690688.00000 to 1880080633418481664.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch29-val_Loss1880080633418481664.000-mae1038411968.000.h5\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 7819546926591770624.0000 - mae: 1038411968.0000 - val_loss: 1880080633418481664.0000 - val_mae: 928025664.0000\n",
      "Epoch 30/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1138948842121592832.0000 - mae: 842772864.0000\n",
      "Epoch 30: val_loss improved from 1880080633418481664.00000 to 1880078296956272640.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch30-val_Loss1880078296956272640.000-mae1038410816.000.h5\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 7819543628056887296.0000 - mae: 1038410816.0000 - val_loss: 1880078296956272640.0000 - val_mae: 928024448.0000\n",
      "Epoch 31/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1053081656916705280.0000 - mae: 867985024.0000\n",
      "Epoch 31: val_loss improved from 1880078296956272640.00000 to 1880075960494063616.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch31-val_Loss1880075960494063616.000-mae1038409536.000.h5\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 7819540879277817856.0000 - mae: 1038409536.0000 - val_loss: 1880075960494063616.0000 - val_mae: 928023296.0000\n",
      "Epoch 32/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7887431874002288640.0000 - mae: 1024154624.0000\n",
      "Epoch 32: val_loss improved from 1880075960494063616.00000 to 1880073074276040704.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch32-val_Loss1880073074276040704.000-mae1038408064.000.h5\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 7819537580742934528.0000 - mae: 1038408064.0000 - val_loss: 1880073074276040704.0000 - val_mae: 928021760.0000\n",
      "Epoch 33/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1275791516693954560.0000 - mae: 851545984.0000\n",
      "Epoch 33: val_loss improved from 1880073074276040704.00000 to 1880069363424296960.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch33-val_Loss1880069363424296960.000-mae1038406336.000.h5\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 7819534831963865088.0000 - mae: 1038406336.0000 - val_loss: 1880069363424296960.0000 - val_mae: 928019968.0000\n",
      "Epoch 34/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 683097299840139264.0000 - mae: 743893312.0000\n",
      "Epoch 34: val_loss improved from 1880069363424296960.00000 to 1880065790011506688.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch34-val_Loss1880065790011506688.000-mae1038404544.000.h5\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 7819528234894098432.0000 - mae: 1038404544.0000 - val_loss: 1880065790011506688.0000 - val_mae: 928018240.0000\n",
      "Epoch 35/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7923830656683999232.0000 - mae: 1043417536.0000\n",
      "Epoch 35: val_loss improved from 1880065790011506688.00000 to 1880061666842902528.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch35-val_Loss1880061666842902528.000-mae1038402560.000.h5\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 7819525486115028992.0000 - mae: 1038402560.0000 - val_loss: 1880061666842902528.0000 - val_mae: 928016192.0000\n",
      "Epoch 36/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18151051312140124160.0000 - mae: 1271628544.0000\n",
      "Epoch 36: val_loss improved from 1880061666842902528.00000 to 1880056719040577536.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch36-val_Loss1880056719040577536.000-mae1038400128.000.h5\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 7819518889045262336.0000 - mae: 1038400128.0000 - val_loss: 1880056719040577536.0000 - val_mae: 928013888.0000\n",
      "Epoch 37/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 914484436664320000.0000 - mae: 820556224.0000\n",
      "Epoch 37: val_loss improved from 1880056719040577536.00000 to 1880051633799299072.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch37-val_Loss1880051633799299072.000-mae1038397696.000.h5\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 7819512291975495680.0000 - mae: 1038397696.0000 - val_loss: 1880051633799299072.0000 - val_mae: 928011264.0000\n",
      "Epoch 38/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 794931444999782400.0000 - mae: 784158656.0000\n",
      "Epoch 38: val_loss improved from 1880051633799299072.00000 to 1880045998802206720.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch38-val_Loss1880045998802206720.000-mae1038394816.000.h5\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 7819505145149915136.0000 - mae: 1038394816.0000 - val_loss: 1880045998802206720.0000 - val_mae: 928008448.0000\n",
      "Epoch 39/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 699321143663788032.0000 - mae: 741863872.0000\n",
      "Epoch 39: val_loss improved from 1880045998802206720.00000 to 1880039539171393536.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch39-val_Loss1880039539171393536.000-mae1038391744.000.h5\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 7819497998324334592.0000 - mae: 1038391744.0000 - val_loss: 1880039539171393536.0000 - val_mae: 928005248.0000\n",
      "Epoch 40/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20307438805301854208.0000 - mae: 1434636288.0000\n",
      "Epoch 40: val_loss improved from 1880039539171393536.00000 to 1880032392345812992.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch40-val_Loss1880032392345812992.000-mae1038388288.000.h5\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 7819490301742940160.0000 - mae: 1038388288.0000 - val_loss: 1880032392345812992.0000 - val_mae: 928001728.0000\n",
      "Epoch 41/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3015266053396627456.0000 - mae: 947193088.0000\n",
      "Epoch 41: val_loss improved from 1880032392345812992.00000 to 1880024695764418560.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch41-val_Loss1880024695764418560.000-mae1038384448.000.h5\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 7819480406138290176.0000 - mae: 1038384448.0000 - val_loss: 1880024695764418560.0000 - val_mae: 927998016.0000\n",
      "Epoch 42/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1045185445442355200.0000 - mae: 818816512.0000\n",
      "Epoch 42: val_loss improved from 1880024695764418560.00000 to 1880016586866163712.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch42-val_Loss1880016586866163712.000-mae1038380480.000.h5\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 7819469411022012416.0000 - mae: 1038380480.0000 - val_loss: 1880016586866163712.0000 - val_mae: 927993856.0000\n",
      "Epoch 43/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 835717485234749440.0000 - mae: 801895936.0000\n",
      "Epoch 43: val_loss improved from 1880016586866163712.00000 to 1880007515895234560.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch43-val_Loss1880007515895234560.000-mae1038375936.000.h5\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 7819458415905734656.0000 - mae: 1038375936.0000 - val_loss: 1880007515895234560.0000 - val_mae: 927989440.0000\n",
      "Epoch 44/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11526378306068807680.0000 - mae: 1098744320.0000\n",
      "Epoch 44: val_loss improved from 1880007515895234560.00000 to 1879997620290584576.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch44-val_Loss1879997620290584576.000-mae1038370880.000.h5\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 7819446321277829120.0000 - mae: 1038370880.0000 - val_loss: 1879997620290584576.0000 - val_mae: 927984448.0000\n",
      "Epoch 45/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18334176073257844736.0000 - mae: 1302856064.0000\n",
      "Epoch 45: val_loss improved from 1879997620290584576.00000 to 1879986350296399872.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch45-val_Loss1879986350296399872.000-mae1038365632.000.h5\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 7819433676894109696.0000 - mae: 1038365632.0000 - val_loss: 1879986350296399872.0000 - val_mae: 927979072.0000\n",
      "Epoch 46/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9944324010210754560.0000 - mae: 1184423296.0000\n",
      "Epoch 46: val_loss improved from 1879986350296399872.00000 to 1879974393107447808.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch46-val_Loss1879974393107447808.000-mae1038359680.000.h5\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 7819418283731320832.0000 - mae: 1038359680.0000 - val_loss: 1879974393107447808.0000 - val_mae: 927973120.0000\n",
      "Epoch 47/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13246542257491804160.0000 - mae: 1152193536.0000\n",
      "Epoch 47: val_loss improved from 1879974393107447808.00000 to 1879961748723728384.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch47-val_Loss1879961748723728384.000-mae1038353280.000.h5\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 7819402890568531968.0000 - mae: 1038353280.0000 - val_loss: 1879961748723728384.0000 - val_mae: 927966720.0000\n",
      "Epoch 48/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8084627085420658688.0000 - mae: 1110281216.0000\n",
      "Epoch 48: val_loss improved from 1879961748723728384.00000 to 1879947867389427712.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch48-val_Loss1879947867389427712.000-mae1038346304.000.h5\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 7819386397894115328.0000 - mae: 1038346304.0000 - val_loss: 1879947867389427712.0000 - val_mae: 927959936.0000\n",
      "Epoch 49/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18334638967653138432.0000 - mae: 1335279616.0000\n",
      "Epoch 49: val_loss improved from 1879947867389427712.00000 to 1879932749104545792.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch49-val_Loss1879932749104545792.000-mae1038338816.000.h5\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 7819368805708070912.0000 - mae: 1038338816.0000 - val_loss: 1879932749104545792.0000 - val_mae: 927952576.0000\n",
      "Epoch 50/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 932134484508147712.0000 - mae: 790435392.0000\n",
      "Epoch 50: val_loss improved from 1879932749104545792.00000 to 1879916806185943040.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch50-val_Loss1879916806185943040.000-mae1038330688.000.h5\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 7819346265719701504.0000 - mae: 1038330688.0000 - val_loss: 1879916806185943040.0000 - val_mae: 927944640.0000\n",
      "Epoch 51/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2895814560399228928.0000 - mae: 947982208.0000\n",
      "Epoch 51: val_loss improved from 1879916806185943040.00000 to 1879899626316759040.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch51-val_Loss1879899626316759040.000-mae1038322240.000.h5\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 7819327574022029312.0000 - mae: 1038322240.0000 - val_loss: 1879899626316759040.0000 - val_mae: 927936064.0000\n",
      "Epoch 52/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 865897223991328768.0000 - mae: 799080448.0000\n",
      "Epoch 52: val_loss improved from 1879899626316759040.00000 to 1879880659741179904.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch52-val_Loss1879880659741179904.000-mae1038312960.000.h5\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 7819301735498776576.0000 - mae: 1038312960.0000 - val_loss: 1879880659741179904.0000 - val_mae: 927926848.0000\n",
      "Epoch 53/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2995322836614119424.0000 - mae: 934524864.0000\n",
      "Epoch 53: val_loss improved from 1879880659741179904.00000 to 1879860731092926464.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch53-val_Loss1879860731092926464.000-mae1038302720.000.h5\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 7819276996487151616.0000 - mae: 1038302720.0000 - val_loss: 1879860731092926464.0000 - val_mae: 927916992.0000\n",
      "Epoch 54/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13483699219021692928.0000 - mae: 1185129472.0000\n",
      "Epoch 54: val_loss improved from 1879860731092926464.00000 to 1879839290616184832.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch54-val_Loss1879839290616184832.000-mae1038291904.000.h5\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 7819250608208084992.0000 - mae: 1038291904.0000 - val_loss: 1879839290616184832.0000 - val_mae: 927906304.0000\n",
      "Epoch 55/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 884687396673683456.0000 - mae: 781934208.0000\n",
      "Epoch 55: val_loss improved from 1879839290616184832.00000 to 1879816200872001536.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch55-val_Loss1879816200872001536.000-mae1038280640.000.h5\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 7819221471149948928.0000 - mae: 1038280640.0000 - val_loss: 1879816200872001536.0000 - val_mae: 927895104.0000\n",
      "Epoch 56/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20204542109128065024.0000 - mae: 1420121984.0000\n",
      "Epoch 56: val_loss improved from 1879816200872001536.00000 to 1879791599299330048.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch56-val_Loss1879791599299330048.000-mae1038267968.000.h5\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 7819195082870882304.0000 - mae: 1038267968.0000 - val_loss: 1879791599299330048.0000 - val_mae: 927882880.0000\n",
      "Epoch 57/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1354766138382221312.0000 - mae: 897262336.0000\n",
      "Epoch 57: val_loss improved from 1879791599299330048.00000 to 1879765623337123840.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch57-val_Loss1879765623337123840.000-mae1038254912.000.h5\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 7819158249231351808.0000 - mae: 1038254912.0000 - val_loss: 1879765623337123840.0000 - val_mae: 927869952.0000\n",
      "Epoch 58/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 874591474749014016.0000 - mae: 782707904.0000\n",
      "Epoch 58: val_loss improved from 1879765623337123840.00000 to 1879737723229569024.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch58-val_Loss1879737723229569024.000-mae1038241088.000.h5\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 7819123064859262976.0000 - mae: 1038241088.0000 - val_loss: 1879737723229569024.0000 - val_mae: 927856256.0000\n",
      "Epoch 59/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 806541256996945920.0000 - mae: 770462080.0000\n",
      "Epoch 59: val_loss improved from 1879737723229569024.00000 to 1879707761537712128.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch59-val_Loss1879707761537712128.000-mae1038225856.000.h5\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 7819086231219732480.0000 - mae: 1038225856.0000 - val_loss: 1879707761537712128.0000 - val_mae: 927841472.0000\n",
      "Epoch 60/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 954524526979317760.0000 - mae: 842965120.0000\n",
      "Epoch 60: val_loss improved from 1879707761537712128.00000 to 1879676150578413568.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch60-val_Loss1879676150578413568.000-mae1038209856.000.h5\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 7819046099045318656.0000 - mae: 1038209856.0000 - val_loss: 1879676150578413568.0000 - val_mae: 927825728.0000\n",
      "Epoch 61/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7963447709900210176.0000 - mae: 1062709888.0000\n",
      "Epoch 61: val_loss improved from 1879676150578413568.00000 to 1879642065717952512.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch61-val_Loss1879642065717952512.000-mae1038192768.000.h5\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 7819005966870904832.0000 - mae: 1038192768.0000 - val_loss: 1879642065717952512.0000 - val_mae: 927809024.0000\n",
      "Epoch 62/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11256094158705655808.0000 - mae: 1010357376.0000\n",
      "Epoch 62: val_loss improved from 1879642065717952512.00000 to 1879605919273189376.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch62-val_Loss1879605919273189376.000-mae1038174912.000.h5\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 7818959237626724352.0000 - mae: 1038174912.0000 - val_loss: 1879605919273189376.0000 - val_mae: 927791296.0000\n",
      "Epoch 63/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1162906444576587776.0000 - mae: 849321344.0000\n",
      "Epoch 63: val_loss improved from 1879605919273189376.00000 to 1879567573805170688.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch63-val_Loss1879567573805170688.000-mae1038155584.000.h5\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 7818910309359288320.0000 - mae: 1038155584.0000 - val_loss: 1879567573805170688.0000 - val_mae: 927772352.0000\n",
      "Epoch 64/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11450901230868496384.0000 - mae: 1070609664.0000\n",
      "Epoch 64: val_loss improved from 1879567573805170688.00000 to 1879526891874942976.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch64-val_Loss1879526891874942976.000-mae1038135040.000.h5\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 7818861381091852288.0000 - mae: 1038135040.0000 - val_loss: 1879526891874942976.0000 - val_mae: 927752320.0000\n",
      "Epoch 65/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9640980846735261696.0000 - mae: 1140095104.0000\n",
      "Epoch 65: val_loss improved from 1879526891874942976.00000 to 1879484148360413184.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch65-val_Loss1879484148360413184.000-mae1038113152.000.h5\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 7818807505022091264.0000 - mae: 1038113152.0000 - val_loss: 1879484148360413184.0000 - val_mae: 927731072.0000\n",
      "Epoch 66/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11367128340436615168.0000 - mae: 1073090944.0000\n",
      "Epoch 66: val_loss improved from 1879484148360413184.00000 to 1879438518627860480.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch66-val_Loss1879438518627860480.000-mae1038090496.000.h5\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 7818750880173260800.0000 - mae: 1038090496.0000 - val_loss: 1879438518627860480.0000 - val_mae: 927708736.0000\n",
      "Epoch 67/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 953929003993923584.0000 - mae: 831494528.0000\n",
      "Epoch 67: val_loss improved from 1879438518627860480.00000 to 1879390827311005696.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch67-val_Loss1879390827311005696.000-mae1038065984.000.h5\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 7818690956789547008.0000 - mae: 1038065984.0000 - val_loss: 1879390827311005696.0000 - val_mae: 927685184.0000\n",
      "Epoch 68/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11745786950903136256.0000 - mae: 1113676800.0000\n",
      "Epoch 68: val_loss improved from 1879390827311005696.00000 to 1879340249776128000.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch68-val_Loss1879340249776128000.000-mae1038040768.000.h5\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 7818629933894205440.0000 - mae: 1038040768.0000 - val_loss: 1879340249776128000.0000 - val_mae: 927660224.0000\n",
      "Epoch 69/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11709722969512083456.0000 - mae: 1140312704.0000\n",
      "Epoch 69: val_loss improved from 1879340249776128000.00000 to 1879287060901134336.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch69-val_Loss1879287060901134336.000-mae1038014080.000.h5\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 7818560114905841664.0000 - mae: 1038014080.0000 - val_loss: 1879287060901134336.0000 - val_mae: 927634240.0000\n",
      "Epoch 70/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2734160787470811136.0000 - mae: 907108352.0000\n",
      "Epoch 70: val_loss improved from 1879287060901134336.00000 to 1879231810441838592.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch70-val_Loss1879231810441838592.000-mae1037985728.000.h5\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 7818486447626780672.0000 - mae: 1037985728.0000 - val_loss: 1879231810441838592.0000 - val_mae: 927606848.0000\n",
      "Epoch 71/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1156959186181947392.0000 - mae: 847136384.0000\n",
      "Epoch 71: val_loss improved from 1879231810441838592.00000 to 1879173124008706048.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch71-val_Loss1879173124008706048.000-mae1037956096.000.h5\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 7818416628638416896.0000 - mae: 1037956096.0000 - val_loss: 1879173124008706048.0000 - val_mae: 927577792.0000\n",
      "Epoch 72/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7716545776813342720.0000 - mae: 1023134080.0000\n",
      "Epoch 72: val_loss improved from 1879173124008706048.00000 to 1879111413918597120.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch72-val_Loss1879111413918597120.000-mae1037924672.000.h5\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 7818338013557030912.0000 - mae: 1037924672.0000 - val_loss: 1879111413918597120.0000 - val_mae: 927547328.0000\n",
      "Epoch 73/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9553051802350387200.0000 - mae: 1102145664.0000\n",
      "Epoch 73: val_loss improved from 1879111413918597120.00000 to 1879045580659884032.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch73-val_Loss1879045580659884032.000-mae1037891648.000.h5\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 7818261597498900480.0000 - mae: 1037891648.0000 - val_loss: 1879045580659884032.0000 - val_mae: 927515008.0000\n",
      "Epoch 74/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18612167797131706368.0000 - mae: 1373186688.0000\n",
      "Epoch 74: val_loss improved from 1879045580659884032.00000 to 1878976998622101504.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch74-val_Loss1878976998622101504.000-mae1037857152.000.h5\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 7818175285836120064.0000 - mae: 1037857152.0000 - val_loss: 1878976998622101504.0000 - val_mae: 927481344.0000\n",
      "Epoch 75/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 842701857972289536.0000 - mae: 783552384.0000\n",
      "Epoch 75: val_loss improved from 1878976998622101504.00000 to 1878906492438970368.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch75-val_Loss1878906492438970368.000-mae1037820544.000.h5\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 7818078528812875776.0000 - mae: 1037820544.0000 - val_loss: 1878906492438970368.0000 - val_mae: 927446336.0000\n",
      "Epoch 76/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9807243497529409536.0000 - mae: 1191958528.0000\n",
      "Epoch 76: val_loss improved from 1878906492438970368.00000 to 1878831175892467712.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch76-val_Loss1878831175892467712.000-mae1037782208.000.h5\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 7817990018126839808.0000 - mae: 1037782208.0000 - val_loss: 1878831175892467712.0000 - val_mae: 927409344.0000\n",
      "Epoch 77/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8069233372875980800.0000 - mae: 1049525888.0000\n",
      "Epoch 77: val_loss improved from 1878831175892467712.00000 to 1878754072639569920.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch77-val_Loss1878754072639569920.000-mae1037743488.000.h5\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 7817885014766387200.0000 - mae: 1037743488.0000 - val_loss: 1878754072639569920.0000 - val_mae: 927371136.0000\n",
      "Epoch 78/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1088609351511310336.0000 - mae: 846733248.0000\n",
      "Epoch 78: val_loss improved from 1878754072639569920.00000 to 1878672571340161024.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch78-val_Loss1878672571340161024.000-mae1037701440.000.h5\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 7817780561161748480.0000 - mae: 1037701440.0000 - val_loss: 1878672571340161024.0000 - val_mae: 927330944.0000\n",
      "Epoch 79/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7709226877663051776.0000 - mae: 1028820352.0000\n",
      "Epoch 79: val_loss improved from 1878672571340161024.00000 to 1878587084311101440.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch79-val_Loss1878587084311101440.000-mae1037656768.000.h5\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 7817676107557109760.0000 - mae: 1037656768.0000 - val_loss: 1878587084311101440.0000 - val_mae: 927288768.0000\n",
      "Epoch 80/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 795325138882002944.0000 - mae: 743384704.0000\n",
      "Epoch 80: val_loss improved from 1878587084311101440.00000 to 1878498161308205056.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch80-val_Loss1878498161308205056.000-mae1037612736.000.h5\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 7817557360301309952.0000 - mae: 1037612736.0000 - val_loss: 1878498161308205056.0000 - val_mae: 927244864.0000\n",
      "Epoch 81/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1017758402765389824.0000 - mae: 804480768.0000\n",
      "Epoch 81: val_loss improved from 1878498161308205056.00000 to 1878404290502983680.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch81-val_Loss1878404290502983680.000-mae1037565120.000.h5\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 7817444110603649024.0000 - mae: 1037565120.0000 - val_loss: 1878404290502983680.0000 - val_mae: 927198720.0000\n",
      "Epoch 82/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 692885839545368576.0000 - mae: 747362176.0000\n",
      "Epoch 82: val_loss improved from 1878404290502983680.00000 to 1878306571407065088.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch82-val_Loss1878306571407065088.000-mae1037516096.000.h5\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 7817317666766454784.0000 - mae: 1037516096.0000 - val_loss: 1878306571407065088.0000 - val_mae: 927150528.0000\n",
      "Epoch 83/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7723505135661350912.0000 - mae: 1009412480.0000\n",
      "Epoch 83: val_loss improved from 1878306571407065088.00000 to 1878203767069868032.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch83-val_Loss1878203767069868032.000-mae1037463936.000.h5\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 7817195071219957760.0000 - mae: 1037463936.0000 - val_loss: 1878203767069868032.0000 - val_mae: 927099904.0000\n",
      "Epoch 84/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13213936240170106880.0000 - mae: 1146613760.0000\n",
      "Epoch 84: val_loss improved from 1878203767069868032.00000 to 1878096152369299456.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch84-val_Loss1878096152369299456.000-mae1037409856.000.h5\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 7817063129824624640.0000 - mae: 1037409856.0000 - val_loss: 1878096152369299456.0000 - val_mae: 927046912.0000\n",
      "Epoch 85/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1216300653366738944.0000 - mae: 837088512.0000\n",
      "Epoch 85: val_loss improved from 1878096152369299456.00000 to 1877986201206521856.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch85-val_Loss1877986201206521856.000-mae1037352384.000.h5\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 7816911397219991552.0000 - mae: 1037352384.0000 - val_loss: 1877986201206521856.0000 - val_mae: 926992640.0000\n",
      "Epoch 86/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11245461881265061888.0000 - mae: 1010459712.0000\n",
      "Epoch 86: val_loss improved from 1877986201206521856.00000 to 1877869790412931072.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch86-val_Loss1877869790412931072.000-mae1037293184.000.h5\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 7816768460708380672.0000 - mae: 1037293184.0000 - val_loss: 1877869790412931072.0000 - val_mae: 926935360.0000\n",
      "Epoch 87/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2970786410006577152.0000 - mae: 919402944.0000\n",
      "Epoch 87: val_loss improved from 1877869790412931072.00000 to 1877748156939108352.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch87-val_Loss1877748156939108352.000-mae1037232320.000.h5\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 7816618927127003136.0000 - mae: 1037232320.0000 - val_loss: 1877748156939108352.0000 - val_mae: 926875200.0000\n",
      "Epoch 88/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1299145418545823744.0000 - mae: 855288576.0000\n",
      "Epoch 88: val_loss improved from 1877748156939108352.00000 to 1877622400296681472.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch88-val_Loss1877622400296681472.000-mae1037167040.000.h5\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 7816454550138650624.0000 - mae: 1037167040.0000 - val_loss: 1877622400296681472.0000 - val_mae: 926813120.0000\n",
      "Epoch 89/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9846918275106078720.0000 - mae: 1133798144.0000\n",
      "Epoch 89: val_loss improved from 1877622400296681472.00000 to 1877490046584487936.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch89-val_Loss1877490046584487936.000-mae1037099648.000.h5\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 7816299518999134208.0000 - mae: 1037099648.0000 - val_loss: 1877490046584487936.0000 - val_mae: 926747840.0000\n",
      "Epoch 90/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18375706826462199808.0000 - mae: 1331475456.0000\n",
      "Epoch 90: val_loss improved from 1877490046584487936.00000 to 1877353432264736768.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch90-val_Loss1877353432264736768.000-mae1037028864.000.h5\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 7816125246406131712.0000 - mae: 1037028864.0000 - val_loss: 1877353432264736768.0000 - val_mae: 926680512.0000\n",
      "Epoch 91/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2718831396356358144.0000 - mae: 865276416.0000\n",
      "Epoch 91: val_loss improved from 1877353432264736768.00000 to 1877214069165916160.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch91-val_Loss1877214069165916160.000-mae1036958464.000.h5\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 7815928983580573696.0000 - mae: 1036958464.0000 - val_loss: 1877214069165916160.0000 - val_mae: 926611392.0000\n",
      "Epoch 92/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18567967429695111168.0000 - mae: 1337467392.0000\n",
      "Epoch 92: val_loss improved from 1877214069165916160.00000 to 1877065909974073344.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch92-val_Loss1877065909974073344.000-mae1036881344.000.h5\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 7815760758301523968.0000 - mae: 1036881344.0000 - val_loss: 1877065909974073344.0000 - val_mae: 926538432.0000\n",
      "Epoch 93/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11858589147331559424.0000 - mae: 1175226112.0000\n",
      "Epoch 93: val_loss improved from 1877065909974073344.00000 to 1876912802979905536.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch93-val_Loss1876912802979905536.000-mae1036805504.000.h5\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 7815563945720152064.0000 - mae: 1036805504.0000 - val_loss: 1876912802979905536.0000 - val_mae: 926463104.0000\n",
      "Epoch 94/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2682251194256064512.0000 - mae: 883165696.0000\n",
      "Epoch 94: val_loss improved from 1876912802979905536.00000 to 1876756122572947456.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch94-val_Loss1876756122572947456.000-mae1036723008.000.h5\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 7815350640464363520.0000 - mae: 1036723008.0000 - val_loss: 1876756122572947456.0000 - val_mae: 926385472.0000\n",
      "Epoch 95/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18803119981527564288.0000 - mae: 1404971008.0000\n",
      "Epoch 95: val_loss improved from 1876756122572947456.00000 to 1876589958878199808.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch95-val_Loss1876589958878199808.000-mae1036640256.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 7815162623976013824.0000 - mae: 1036640256.0000 - val_loss: 1876589958878199808.0000 - val_mae: 926303424.0000\n",
      "Epoch 96/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2744144078173110272.0000 - mae: 901275328.0000\n",
      "Epoch 96: val_loss improved from 1876589958878199808.00000 to 1876422833110777856.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch96-val_Loss1876422833110777856.000-mae1036551168.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 7814918532394647552.0000 - mae: 1036551168.0000 - val_loss: 1876422833110777856.0000 - val_mae: 926220608.0000\n",
      "Epoch 97/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9582181163905056768.0000 - mae: 1111707136.0000\n",
      "Epoch 97: val_loss improved from 1876422833110777856.00000 to 1876246498933473280.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch97-val_Loss1876246498933473280.000-mae1036461568.000.h5\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 7814708525673742336.0000 - mae: 1036461568.0000 - val_loss: 1876246498933473280.0000 - val_mae: 926133440.0000\n",
      "Epoch 98/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7588155254282125312.0000 - mae: 982453632.0000\n",
      "Epoch 98: val_loss improved from 1876246498933473280.00000 to 1876064117442215936.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch98-val_Loss1876064117442215936.000-mae1036368128.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 7814472130673770496.0000 - mae: 1036368128.0000 - val_loss: 1876064117442215936.0000 - val_mae: 926043392.0000\n",
      "Epoch 99/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1121364696255954944.0000 - mae: 846303360.0000\n",
      "Epoch 99: val_loss improved from 1876064117442215936.00000 to 1875875276320145408.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch99-val_Loss1875875276320145408.000-mae1036271488.000.h5\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 7814229138604032000.0000 - mae: 1036271488.0000 - val_loss: 1875875276320145408.0000 - val_mae: 925950144.0000\n",
      "Epoch 100/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9671921103940878336.0000 - mae: 1124613632.0000\n",
      "Epoch 100: val_loss improved from 1875875276320145408.00000 to 1875679563250401280.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch100-val_Loss1875679563250401280.000-mae1036169664.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 7813983947511037952.0000 - mae: 1036169664.0000 - val_loss: 1875679563250401280.0000 - val_mae: 925853120.0000\n",
      "Epoch 101/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1047260223883968512.0000 - mae: 828802048.0000\n",
      "Epoch 101: val_loss improved from 1875679563250401280.00000 to 1875475466404495360.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch101-val_Loss1875475466404495360.000-mae1036068288.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 7813731609592463360.0000 - mae: 1036068288.0000 - val_loss: 1875475466404495360.0000 - val_mae: 925752256.0000\n",
      "Epoch 102/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20252472020006076416.0000 - mae: 1443755520.0000\n",
      "Epoch 102: val_loss improved from 1875475466404495360.00000 to 1875264909927776256.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch102-val_Loss1875264909927776256.000-mae1035960064.000.h5\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 7813481470697144320.0000 - mae: 1035960064.0000 - val_loss: 1875264909927776256.0000 - val_mae: 925648000.0000\n",
      "Epoch 103/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9881201047660134400.0000 - mae: 1173893888.0000\n",
      "Epoch 103: val_loss improved from 1875264909927776256.00000 to 1875049405648732160.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch103-val_Loss1875049405648732160.000-mae1035848640.000.h5\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 7813197796697178112.0000 - mae: 1035848640.0000 - val_loss: 1875049405648732160.0000 - val_mae: 925541504.0000\n",
      "Epoch 104/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11852936558053163008.0000 - mae: 1156148480.0000\n",
      "Epoch 104: val_loss improved from 1875049405648732160.00000 to 1874828541250502656.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch104-val_Loss1874828541250502656.000-mae1035731648.000.h5\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 7812908625139073024.0000 - mae: 1035731648.0000 - val_loss: 1874828541250502656.0000 - val_mae: 925432256.0000\n",
      "Epoch 105/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2953844859967897600.0000 - mae: 917637696.0000\n",
      "Epoch 105: val_loss improved from 1874828541250502656.00000 to 1874600804904599552.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch105-val_Loss1874600804904599552.000-mae1035614144.000.h5\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 7812611756999573504.0000 - mae: 1035614144.0000 - val_loss: 1874600804904599552.0000 - val_mae: 925319552.0000\n",
      "Epoch 106/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2800528133957091328.0000 - mae: 929361280.0000\n",
      "Epoch 106: val_loss improved from 1874600804904599552.00000 to 1874364959660441600.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch106-val_Loss1874364959660441600.000-mae1035493376.000.h5\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 7812317087883329536.0000 - mae: 1035493376.0000 - val_loss: 1874364959660441600.0000 - val_mae: 925202688.0000\n",
      "Epoch 107/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 895088501794537472.0000 - mae: 795540160.0000\n",
      "Epoch 107: val_loss improved from 1874364959660441600.00000 to 1874124441491865600.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch107-val_Loss1874124441491865600.000-mae1035372096.000.h5\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 7811991632441507840.0000 - mae: 1035372096.0000 - val_loss: 1874124441491865600.0000 - val_mae: 925083328.0000\n",
      "Epoch 108/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3005145598618763264.0000 - mae: 958788672.0000\n",
      "Epoch 108: val_loss improved from 1874124441491865600.00000 to 1873873752840732672.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch108-val_Loss1873873752840732672.000-mae1035240960.000.h5\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 7811672774069452800.0000 - mae: 1035240960.0000 - val_loss: 1873873752840732672.0000 - val_mae: 924959168.0000\n",
      "Epoch 109/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9815800996528390144.0000 - mae: 1201943296.0000\n",
      "Epoch 109: val_loss improved from 1873873752840732672.00000 to 1873611244439601152.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch109-val_Loss1873611244439601152.000-mae1035105920.000.h5\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 7811359413255536640.0000 - mae: 1035105920.0000 - val_loss: 1873611244439601152.0000 - val_mae: 924829056.0000\n",
      "Epoch 110/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11449568622775631872.0000 - mae: 1111425024.0000\n",
      "Epoch 110: val_loss improved from 1873611244439601152.00000 to 1873340214823354368.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch110-val_Loss1873340214823354368.000-mae1034966336.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 7811016915383484416.0000 - mae: 1034966336.0000 - val_loss: 1873340214823354368.0000 - val_mae: 924695104.0000\n",
      "Epoch 111/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13477543053417775104.0000 - mae: 1201336064.0000\n",
      "Epoch 111: val_loss improved from 1873340214823354368.00000 to 1873061763503620096.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch111-val_Loss1873061763503620096.000-mae1034821760.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 7810659024348643328.0000 - mae: 1034821760.0000 - val_loss: 1873061763503620096.0000 - val_mae: 924557312.0000\n",
      "Epoch 112/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18549810094674018304.0000 - mae: 1339414400.0000\n",
      "Epoch 112: val_loss improved from 1873061763503620096.00000 to 1872775203285630976.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch112-val_Loss1872775203285630976.000-mae1034678016.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 7810297834778918912.0000 - mae: 1034678016.0000 - val_loss: 1872775203285630976.0000 - val_mae: 924415552.0000\n",
      "Epoch 113/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1190299264954138624.0000 - mae: 845622016.0000\n",
      "Epoch 113: val_loss improved from 1872775203285630976.00000 to 1872486581483339776.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch113-val_Loss1872486581483339776.000-mae1034524416.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 7809880570116177920.0000 - mae: 1034524416.0000 - val_loss: 1872486581483339776.0000 - val_mae: 924272192.0000\n",
      "Epoch 114/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9657663736663506944.0000 - mae: 1122628352.0000\n",
      "Epoch 114: val_loss improved from 1872486581483339776.00000 to 1872182566518259712.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch114-val_Loss1872182566518259712.000-mae1034369792.000.h5\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 7809528726395289600.0000 - mae: 1034369792.0000 - val_loss: 1872182566518259712.0000 - val_mae: 924121152.0000\n",
      "Epoch 115/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 830637191758610432.0000 - mae: 780085824.0000\n",
      "Epoch 115: val_loss improved from 1872182566518259712.00000 to 1871869480582250496.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch115-val_Loss1871869480582250496.000-mae1034208896.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 7809125205627895808.0000 - mae: 1034208896.0000 - val_loss: 1871869480582250496.0000 - val_mae: 923966016.0000\n",
      "Epoch 116/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2736719076150738944.0000 - mae: 880021248.0000\n",
      "Epoch 116: val_loss improved from 1871869480582250496.00000 to 1871550209893335040.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch116-val_Loss1871550209893335040.000-mae1034039168.000.h5\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 7808704642430271488.0000 - mae: 1034039168.0000 - val_loss: 1871550209893335040.0000 - val_mae: 923807616.0000\n",
      "Epoch 117/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7876839178980294656.0000 - mae: 1035794368.0000\n",
      "Epoch 117: val_loss improved from 1871550209893335040.00000 to 1871219944088141824.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch117-val_Loss1871219944088141824.000-mae1033872256.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 7808291226058227712.0000 - mae: 1033872256.0000 - val_loss: 1871219944088141824.0000 - val_mae: 923643904.0000\n",
      "Epoch 118/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1085371908242800640.0000 - mae: 821687360.0000\n",
      "Epoch 118: val_loss improved from 1871219944088141824.00000 to 1870882256579461120.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch118-val_Loss1870882256579461120.000-mae1033698880.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 7807834378976886784.0000 - mae: 1033698880.0000 - val_loss: 1870882256579461120.0000 - val_mae: 923476160.0000\n",
      "Epoch 119/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7721167024184885248.0000 - mae: 1041545984.0000\n",
      "Epoch 119: val_loss improved from 1870882256579461120.00000 to 1870528351274270720.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch119-val_Loss1870528351274270720.000-mae1033515840.000.h5\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 7807427559674609664.0000 - mae: 1033515840.0000 - val_loss: 1870528351274270720.0000 - val_mae: 923300800.0000\n",
      "Epoch 120/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2851637557584723968.0000 - mae: 924576832.0000\n",
      "Epoch 120: val_loss improved from 1870528351274270720.00000 to 1870168536094081024.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch120-val_Loss1870168536094081024.000-mae1033327040.000.h5\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 7806956418942107648.0000 - mae: 1033327040.0000 - val_loss: 1870168536094081024.0000 - val_mae: 923121856.0000\n",
      "Epoch 121/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7846949505135017984.0000 - mae: 1031032320.0000\n",
      "Epoch 121: val_loss improved from 1870168536094081024.00000 to 1869799237626101760.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch121-val_Loss1869799237626101760.000-mae1033134208.000.h5\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 7806487477232861184.0000 - mae: 1033134208.0000 - val_loss: 1869799237626101760.0000 - val_mae: 922938560.0000\n",
      "Epoch 122/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2789510752568868864.0000 - mae: 921413376.0000\n",
      "Epoch 122: val_loss improved from 1869799237626101760.00000 to 1869421692820914176.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch122-val_Loss1869421692820914176.000-mae1032940672.000.h5\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 7806003692116639744.0000 - mae: 1032940672.0000 - val_loss: 1869421692820914176.0000 - val_mae: 922750912.0000\n",
      "Epoch 123/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7579320678352945152.0000 - mae: 979016640.0000\n",
      "Epoch 123: val_loss improved from 1869421692820914176.00000 to 1869038100701773824.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch123-val_Loss1869038100701773824.000-mae1032743808.000.h5\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 7805491869453910016.0000 - mae: 1032743808.0000 - val_loss: 1869038100701773824.0000 - val_mae: 922560064.0000\n",
      "Epoch 124/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11573144933644632064.0000 - mae: 1138618624.0000\n",
      "Epoch 124: val_loss improved from 1869038100701773824.00000 to 1868639802614611968.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch124-val_Loss1868639802614611968.000-mae1032532224.000.h5\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 7804989942395830272.0000 - mae: 1032532224.0000 - val_loss: 1868639802614611968.0000 - val_mae: 922362496.0000\n",
      "Epoch 125/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8071127831410638848.0000 - mae: 1055367168.0000\n",
      "Epoch 125: val_loss improved from 1868639802614611968.00000 to 1868231196605939712.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch125-val_Loss1868231196605939712.000-mae1032318912.000.h5\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 7804464375837753344.0000 - mae: 1032318912.0000 - val_loss: 1868231196605939712.0000 - val_mae: 922159360.0000\n",
      "Epoch 126/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2794861800783347712.0000 - mae: 899961856.0000\n",
      "Epoch 126: val_loss improved from 1868231196605939712.00000 to 1867810358530408448.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch126-val_Loss1867810358530408448.000-mae1032104512.000.h5\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 7803921766849445888.0000 - mae: 1032104512.0000 - val_loss: 1867810358530408448.0000 - val_mae: 921949952.0000\n",
      "Epoch 127/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8143315167820644352.0000 - mae: 1107372160.0000\n",
      "Epoch 127: val_loss improved from 1867810358530408448.00000 to 1867375776559529984.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch127-val_Loss1867375776559529984.000-mae1031877568.000.h5\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 7803380257372766208.0000 - mae: 1031877568.0000 - val_loss: 1867375776559529984.0000 - val_mae: 921734336.0000\n",
      "Epoch 128/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 675149136441376768.0000 - mae: 733340352.0000\n",
      "Epoch 128: val_loss improved from 1867375776559529984.00000 to 1866934872396791808.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch128-val_Loss1866934872396791808.000-mae1031650944.000.h5\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 7802787070849581056.0000 - mae: 1031650944.0000 - val_loss: 1866934872396791808.0000 - val_mae: 921514944.0000\n",
      "Epoch 129/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18344720389768216576.0000 - mae: 1290650368.0000\n",
      "Epoch 129: val_loss improved from 1866934872396791808.00000 to 1866478987388125184.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch129-val_Loss1866478987388125184.000-mae1031407936.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 7802229068698484736.0000 - mae: 1031407936.0000 - val_loss: 1866478987388125184.0000 - val_mae: 921287936.0000\n",
      "Epoch 130/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11710929133767753728.0000 - mae: 1139624576.0000\n",
      "Epoch 130: val_loss improved from 1866478987388125184.00000 to 1866008808728297472.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch130-val_Loss1866008808728297472.000-mae1031166336.000.h5\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 7801635882175299584.0000 - mae: 1031166336.0000 - val_loss: 1866008808728297472.0000 - val_mae: 921053824.0000\n",
      "Epoch 131/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20245307602239488000.0000 - mae: 1406749312.0000\n",
      "Epoch 131: val_loss improved from 1866008808728297472.00000 to 1865527085196378112.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch131-val_Loss1865527085196378112.000-mae1030915200.000.h5\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 7801041046384672768.0000 - mae: 1030915200.0000 - val_loss: 1865527085196378112.0000 - val_mae: 920813440.0000\n",
      "Epoch 132/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20479615729141809152.0000 - mae: 1533169920.0000\n",
      "Epoch 132: val_loss improved from 1865527085196378112.00000 to 1865033679353413632.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch132-val_Loss1865033679353413632.000-mae1030660288.000.h5\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 7800432466698698752.0000 - mae: 1030660288.0000 - val_loss: 1865033679353413632.0000 - val_mae: 920567552.0000\n",
      "Epoch 133/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11633593883916500992.0000 - mae: 1107771648.0000\n",
      "Epoch 133: val_loss improved from 1865033679353413632.00000 to 1864540273510449152.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch133-val_Loss1864540273510449152.000-mae1030397312.000.h5\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 7799732077791805440.0000 - mae: 1030397312.0000 - val_loss: 1864540273510449152.0000 - val_mae: 920321856.0000\n",
      "Epoch 134/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1221047519941754880.0000 - mae: 817179072.0000\n",
      "Epoch 134: val_loss improved from 1864540273510449152.00000 to 1864036834623881216.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch134-val_Loss1864036834623881216.000-mae1030133184.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 7799060825943048192.0000 - mae: 1030133184.0000 - val_loss: 1864036834623881216.0000 - val_mae: 920070464.0000\n",
      "Epoch 135/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11746698446042562560.0000 - mae: 1137260416.0000\n",
      "Epoch 135: val_loss improved from 1864036834623881216.00000 to 1863516765623943168.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch135-val_Loss1863516765623943168.000-mae1029860352.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 7798398370187313152.0000 - mae: 1029860352.0000 - val_loss: 1863516765623943168.0000 - val_mae: 919810880.0000\n",
      "Epoch 136/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2747117157614616576.0000 - mae: 902415232.0000\n",
      "Epoch 136: val_loss improved from 1863516765623943168.00000 to 1862980203949588480.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch136-val_Loss1862980203949588480.000-mae1029583040.000.h5\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 7797719971512975360.0000 - mae: 1029583040.0000 - val_loss: 1862980203949588480.0000 - val_mae: 919542464.0000\n",
      "Epoch 137/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 964229435081359360.0000 - mae: 776371328.0000\n",
      "Epoch 137: val_loss improved from 1862980203949588480.00000 to 1862436495449653248.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch137-val_Loss1862436495449653248.000-mae1029295040.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 7796975602140971008.0000 - mae: 1029295040.0000 - val_loss: 1862436495449653248.0000 - val_mae: 919270464.0000\n",
      "Epoch 138/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9853118421175107584.0000 - mae: 1157262848.0000\n",
      "Epoch 138: val_loss improved from 1862436495449653248.00000 to 1861869559766581248.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch138-val_Loss1861869559766581248.000-mae1029003008.000.h5\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 7796307648827097088.0000 - mae: 1029003008.0000 - val_loss: 1861869559766581248.0000 - val_mae: 918986688.0000\n",
      "Epoch 139/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9947228919931338752.0000 - mae: 1184993024.0000\n",
      "Epoch 139: val_loss improved from 1861869559766581248.00000 to 1861284894458511360.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch139-val_Loss1861284894458511360.000-mae1028697984.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 7795587468710903808.0000 - mae: 1028697984.0000 - val_loss: 1861284894458511360.0000 - val_mae: 918694528.0000\n",
      "Epoch 140/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2844571271230914560.0000 - mae: 900419072.0000\n",
      "Epoch 140: val_loss improved from 1861284894458511360.00000 to 1860695281348116480.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch140-val_Loss1860695281348116480.000-mae1028391808.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 7794780976931930112.0000 - mae: 1028391808.0000 - val_loss: 1860695281348116480.0000 - val_mae: 918399744.0000\n",
      "Epoch 141/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8117648718137655296.0000 - mae: 1075319552.0000\n",
      "Epoch 141: val_loss improved from 1860695281348116480.00000 to 1860089725319118848.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch141-val_Loss1860089725319118848.000-mae1028067904.000.h5\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 7794028361222717440.0000 - mae: 1028067904.0000 - val_loss: 1860089725319118848.0000 - val_mae: 918097152.0000\n",
      "Epoch 142/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20130140356299718656.0000 - mae: 1405573376.0000\n",
      "Epoch 142: val_loss improved from 1860089725319118848.00000 to 1859469875638960128.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch142-val_Loss1859469875638960128.000-mae1027743872.000.h5\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 7793253755280949248.0000 - mae: 1027743872.0000 - val_loss: 1859469875638960128.0000 - val_mae: 917787264.0000\n",
      "Epoch 143/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2786709196941295616.0000 - mae: 925836032.0000\n",
      "Epoch 143: val_loss improved from 1859469875638960128.00000 to 1858842191938453504.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch143-val_Loss1858842191938453504.000-mae1027423424.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 7792398335234539520.0000 - mae: 1027423424.0000 - val_loss: 1858842191938453504.0000 - val_mae: 917473024.0000\n",
      "Epoch 144/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1155872044059983872.0000 - mae: 862190464.0000\n",
      "Epoch 144: val_loss improved from 1858842191938453504.00000 to 1858202138732134400.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch144-val_Loss1858202138732134400.000-mae1027081664.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 7791557758595104768.0000 - mae: 1027081664.0000 - val_loss: 1858202138732134400.0000 - val_mae: 917152768.0000\n",
      "Epoch 145/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9757540074395795456.0000 - mae: 1153740672.0000\n",
      "Epoch 145: val_loss improved from 1858202138732134400.00000 to 1857539408098492416.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch145-val_Loss1857539408098492416.000-mae1026730368.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 7790769958513803264.0000 - mae: 1026730368.0000 - val_loss: 1857539408098492416.0000 - val_mae: 916820864.0000\n",
      "Epoch 146/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13450038770048958464.0000 - mae: 1175504384.0000\n",
      "Epoch 146: val_loss improved from 1857539408098492416.00000 to 1856863483325317120.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch146-val_Loss1856863483325317120.000-mae1026378816.000.h5\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 7789905192618557440.0000 - mae: 1026378816.0000 - val_loss: 1856863483325317120.0000 - val_mae: 916482496.0000\n",
      "Epoch 147/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11552357566809899008.0000 - mae: 1119339392.0000\n",
      "Epoch 147: val_loss improved from 1856863483325317120.00000 to 1856179037337026560.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch147-val_Loss1856179037337026560.000-mae1026020416.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 7788996996014014464.0000 - mae: 1026020416.0000 - val_loss: 1856179037337026560.0000 - val_mae: 916139904.0000\n",
      "Epoch 148/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11589502368131055616.0000 - mae: 1123258880.0000\n",
      "Epoch 148: val_loss improved from 1856179037337026560.00000 to 1855481397209202688.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch148-val_Loss1855481397209202688.000-mae1025650432.000.h5\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 7788092097944354816.0000 - mae: 1025650432.0000 - val_loss: 1855481397209202688.0000 - val_mae: 915790400.0000\n",
      "Epoch 149/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9739135349258452992.0000 - mae: 1137249920.0000\n",
      "Epoch 149: val_loss improved from 1855481397209202688.00000 to 1854761491970916352.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch149-val_Loss1854761491970916352.000-mae1025286912.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 7787218535956086784.0000 - mae: 1025286912.0000 - val_loss: 1854761491970916352.0000 - val_mae: 915429248.0000\n",
      "Epoch 150/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18367509967277129728.0000 - mae: 1302063616.0000\n",
      "Epoch 150: val_loss improved from 1854761491970916352.00000 to 1854037601002979328.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch150-val_Loss1854037601002979328.000-mae1024888704.000.h5\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 7786241619874807808.0000 - mae: 1024888704.0000 - val_loss: 1854037601002979328.0000 - val_mae: 915065856.0000\n",
      "Epoch 151/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9491524231171670016.0000 - mae: 1085856896.0000\n",
      "Epoch 151: val_loss improved from 1854037601002979328.00000 to 1853302440040857600.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch151-val_Loss1853302440040857600.000-mae1024499008.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 7785252059409809408.0000 - mae: 1024499008.0000 - val_loss: 1853302440040857600.0000 - val_mae: 914695616.0000\n",
      "Epoch 152/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7783728136293711872.0000 - mae: 1052945792.0000\n",
      "Epoch 152: val_loss improved from 1853302440040857600.00000 to 1852540478482808832.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch152-val_Loss1852540478482808832.000-mae1024105984.000.h5\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 7784305929654108160.0000 - mae: 1024105984.0000 - val_loss: 1852540478482808832.0000 - val_mae: 914312896.0000\n",
      "Epoch 153/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13398158313892347904.0000 - mae: 1187605504.0000\n",
      "Epoch 153: val_loss improved from 1852540478482808832.00000 to 1851757488764878848.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch153-val_Loss1851757488764878848.000-mae1023694144.000.h5\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 7783322966258876416.0000 - mae: 1023694144.0000 - val_loss: 1851757488764878848.0000 - val_mae: 913919360.0000\n",
      "Epoch 154/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7979954677968011264.0000 - mae: 1016450048.0000\n",
      "Epoch 154: val_loss improved from 1851757488764878848.00000 to 1850968589171949568.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch154-val_Loss1850968589171949568.000-mae1023269376.000.h5\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 7782252591689236480.0000 - mae: 1023269376.0000 - val_loss: 1850968589171949568.0000 - val_mae: 913522304.0000\n",
      "Epoch 155/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11493609561026199552.0000 - mae: 1083556352.0000\n",
      "Epoch 155: val_loss improved from 1850968589171949568.00000 to 1850158798858092544.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch155-val_Loss1850158798858092544.000-mae1022848512.000.h5\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 7781203657596338176.0000 - mae: 1022848512.0000 - val_loss: 1850158798858092544.0000 - val_mae: 913115520.0000\n",
      "Epoch 156/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7914470514196742144.0000 - mae: 1034121344.0000\n",
      "Epoch 156: val_loss improved from 1850158798858092544.00000 to 1849331004041330688.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch156-val_Loss1849331004041330688.000-mae1022409728.000.h5\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 7780131084003442688.0000 - mae: 1022409728.0000 - val_loss: 1849331004041330688.0000 - val_mae: 912698560.0000\n",
      "Epoch 157/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2825669017082003456.0000 - mae: 941376448.0000\n",
      "Epoch 157: val_loss improved from 1849331004041330688.00000 to 1848475171678060544.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch157-val_Loss1848475171678060544.000-mae1021966400.000.h5\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 7779099742096588800.0000 - mae: 1021966400.0000 - val_loss: 1848475171678060544.0000 - val_mae: 912266368.0000\n",
      "Epoch 158/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 922347319192453120.0000 - mae: 783634624.0000\n",
      "Epoch 158: val_loss improved from 1848475171678060544.00000 to 1847615491024093184.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch158-val_Loss1847615491024093184.000-mae1021498816.000.h5\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 7777937008550215680.0000 - mae: 1021498816.0000 - val_loss: 1847615491024093184.0000 - val_mae: 911832512.0000\n",
      "Epoch 159/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11363287746320793600.0000 - mae: 1037496576.0000\n",
      "Epoch 159: val_loss improved from 1847615491024093184.00000 to 1846738767939895296.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch159-val_Loss1846738767939895296.000-mae1021030848.000.h5\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 7776802312550350848.0000 - mae: 1021030848.0000 - val_loss: 1846738767939895296.0000 - val_mae: 911390336.0000\n",
      "Epoch 160/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13378287939755180032.0000 - mae: 1190241664.0000\n",
      "Epoch 160: val_loss improved from 1846738767939895296.00000 to 1845833869870235648.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch160-val_Loss1845833869870235648.000-mae1020561152.000.h5\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 7775719843352805376.0000 - mae: 1020561152.0000 - val_loss: 1845833869870235648.0000 - val_mae: 910933056.0000\n",
      "Epoch 161/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9857024985988595712.0000 - mae: 1154723840.0000\n",
      "Epoch 161: val_loss improved from 1845833869870235648.00000 to 1844923749120344064.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch161-val_Loss1844923749120344064.000-mae1020081280.000.h5\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 7774497736178532352.0000 - mae: 1020081280.0000 - val_loss: 1844923749120344064.0000 - val_mae: 910472064.0000\n",
      "Epoch 162/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13170862872151982080.0000 - mae: 1132858752.0000\n",
      "Epoch 162: val_loss improved from 1844923749120344064.00000 to 1843995348989640704.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch162-val_Loss1843995348989640704.000-mae1019574400.000.h5\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 7773282775829839872.0000 - mae: 1019574400.0000 - val_loss: 1843995348989640704.0000 - val_mae: 910002624.0000\n",
      "Epoch 163/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2726848210512379904.0000 - mae: 874036288.0000\n",
      "Epoch 163: val_loss improved from 1843995348989640704.00000 to 1843056503498473472.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch163-val_Loss1843056503498473472.000-mae1019072640.000.h5\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 7772017237946269696.0000 - mae: 1019072640.0000 - val_loss: 1843056503498473472.0000 - val_mae: 909526464.0000\n",
      "Epoch 164/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1189582245933875200.0000 - mae: 832666368.0000\n",
      "Epoch 164: val_loss improved from 1843056503498473472.00000 to 1842093743629402112.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch164-val_Loss1842093743629402112.000-mae1018571520.000.h5\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 7770768742492930048.0000 - mae: 1018571520.0000 - val_loss: 1842093743629402112.0000 - val_mae: 909039488.0000\n",
      "Epoch 165/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1034049248078856192.0000 - mae: 810662272.0000\n",
      "Epoch 165: val_loss improved from 1842093743629402112.00000 to 1841107756577193984.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch165-val_Loss1841107756577193984.000-mae1018042560.000.h5\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 7769546635318657024.0000 - mae: 1018042560.0000 - val_loss: 1841107756577193984.0000 - val_mae: 908539328.0000\n",
      "Epoch 166/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11290458295120166912.0000 - mae: 1032120256.0000\n",
      "Epoch 166: val_loss improved from 1841107756577193984.00000 to 1840111049286615040.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch166-val_Loss1840111049286615040.000-mae1017510912.000.h5\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 7768229420388581376.0000 - mae: 1017510912.0000 - val_loss: 1840111049286615040.0000 - val_mae: 908034112.0000\n",
      "Epoch 167/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11365276762855440384.0000 - mae: 1039554240.0000\n",
      "Epoch 167: val_loss improved from 1840111049286615040.00000 to 1839088503472783360.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch167-val_Loss1839088503472783360.000-mae1016978560.000.h5\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 7766934195691061248.0000 - mae: 1016978560.0000 - val_loss: 1839088503472783360.0000 - val_mae: 907514880.0000\n",
      "Epoch 168/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 984832358840532992.0000 - mae: 806420224.0000\n",
      "Epoch 168: val_loss improved from 1839088503472783360.00000 to 1838059910344998912.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch168-val_Loss1838059910344998912.000-mae1016410624.000.h5\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 7765539465191227392.0000 - mae: 1016410624.0000 - val_loss: 1838059910344998912.0000 - val_mae: 906991360.0000\n",
      "Epoch 169/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13320027017622585344.0000 - mae: 1188797952.0000\n",
      "Epoch 169: val_loss improved from 1838059910344998912.00000 to 1836995445650358272.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch169-val_Loss1836995445650358272.000-mae1015853760.000.h5\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 7764253036586729472.0000 - mae: 1015853760.0000 - val_loss: 1836995445650358272.0000 - val_mae: 906449920.0000\n",
      "Epoch 170/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13382057065615196160.0000 - mae: 1180161920.0000\n",
      "Epoch 170: val_loss improved from 1836995445650358272.00000 to 1835912014380138496.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch170-val_Loss1835912014380138496.000-mae1015265472.000.h5\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 7762875898272940032.0000 - mae: 1015265472.0000 - val_loss: 1835912014380138496.0000 - val_mae: 905898688.0000\n",
      "Epoch 171/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7794744143292399616.0000 - mae: 1006521728.0000\n",
      "Epoch 171: val_loss improved from 1835912014380138496.00000 to 1834824047624454144.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch171-val_Loss1834824047624454144.000-mae1014689920.000.h5\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 7761396505377767424.0000 - mae: 1014689920.0000 - val_loss: 1834824047624454144.0000 - val_mae: 905345088.0000\n",
      "Epoch 172/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 837276249125552128.0000 - mae: 792833536.0000\n",
      "Epoch 172: val_loss improved from 1834824047624454144.00000 to 1833724810874585088.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch172-val_Loss1833724810874585088.000-mae1014084736.000.h5\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 7759896771517480960.0000 - mae: 1014084736.0000 - val_loss: 1833724810874585088.0000 - val_mae: 904784320.0000\n",
      "Epoch 173/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2902589751049584640.0000 - mae: 889445760.0000\n",
      "Epoch 173: val_loss improved from 1833724810874585088.00000 to 1832590939508441088.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch173-val_Loss1832590939508441088.000-mae1013487552.000.h5\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 7758503140529274880.0000 - mae: 1013487552.0000 - val_loss: 1832590939508441088.0000 - val_mae: 904204736.0000\n",
      "Epoch 174/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11433721361684496384.0000 - mae: 1058216192.0000\n",
      "Epoch 174: val_loss improved from 1832590939508441088.00000 to 1831430817302183936.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch174-val_Loss1831430817302183936.000-mae1012871168.000.h5\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 7757056183227121664.0000 - mae: 1012871168.0000 - val_loss: 1831430817302183936.0000 - val_mae: 903613120.0000\n",
      "Epoch 175/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13394211067148632064.0000 - mae: 1185031168.0000\n",
      "Epoch 175: val_loss improved from 1831430817302183936.00000 to 1830255164494184448.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch175-val_Loss1830255164494184448.000-mae1012236096.000.h5\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 7755531160599396352.0000 - mae: 1012236096.0000 - val_loss: 1830255164494184448.0000 - val_mae: 903012544.0000\n",
      "Epoch 176/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18583325408111886336.0000 - mae: 1318408064.0000\n",
      "Epoch 176: val_loss improved from 1830255164494184448.00000 to 1829059033282117632.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch176-val_Loss1829059033282117632.000-mae1011587584.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 7754027028692598784.0000 - mae: 1011587584.0000 - val_loss: 1829059033282117632.0000 - val_mae: 902401280.0000\n",
      "Epoch 177/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1028321685851340800.0000 - mae: 792117376.0000\n",
      "Epoch 177: val_loss improved from 1829059033282117632.00000 to 1827869224261910528.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch177-val_Loss1827869224261910528.000-mae1010939456.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 7752323335425359872.0000 - mae: 1010939456.0000 - val_loss: 1827869224261910528.0000 - val_mae: 901791744.0000\n",
      "Epoch 178/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13224959943750189056.0000 - mae: 1168596992.0000\n",
      "Epoch 178: val_loss improved from 1827869224261910528.00000 to 1826642993919033344.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch178-val_Loss1826642993919033344.000-mae1010282496.000.h5\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 7750803260599959552.0000 - mae: 1010282496.0000 - val_loss: 1826642993919033344.0000 - val_mae: 901163648.0000\n",
      "Epoch 179/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 725806454631563264.0000 - mae: 760133760.0000\n",
      "Epoch 179: val_loss improved from 1826642993919033344.00000 to 1825403019680808960.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch179-val_Loss1825403019680808960.000-mae1009615744.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 7749150144867598336.0000 - mae: 1009615744.0000 - val_loss: 1825403019680808960.0000 - val_mae: 900528576.0000\n",
      "Epoch 180/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13193904237823655936.0000 - mae: 1153287680.0000\n",
      "Epoch 180: val_loss improved from 1825403019680808960.00000 to 1824121676317589504.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch180-val_Loss1824121676317589504.000-mae1008938560.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 7747635017844523008.0000 - mae: 1008938560.0000 - val_loss: 1824121676317589504.0000 - val_mae: 899870400.0000\n",
      "Epoch 181/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9487369176730304512.0000 - mae: 1060454656.0000\n",
      "Epoch 181: val_loss improved from 1824121676317589504.00000 to 1822838958564835328.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch181-val_Loss1822838958564835328.000-mae1008225664.000.h5\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 7745905486054031360.0000 - mae: 1008225664.0000 - val_loss: 1822838958564835328.0000 - val_mae: 899210048.0000\n",
      "Epoch 182/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 877910762914316288.0000 - mae: 790484352.0000\n",
      "Epoch 182: val_loss improved from 1822838958564835328.00000 to 1821546620085338112.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch182-val_Loss1821546620085338112.000-mae1007532928.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 7744144068426334208.0000 - mae: 1007532928.0000 - val_loss: 1821546620085338112.0000 - val_mae: 898545280.0000\n",
      "Epoch 183/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13269274660396072960.0000 - mae: 1180867840.0000\n",
      "Epoch 183: val_loss improved from 1821546620085338112.00000 to 1820216760771543040.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch183-val_Loss1820216760771543040.000-mae1006816192.000.h5\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 7742504146833506304.0000 - mae: 1006816192.0000 - val_loss: 1820216760771543040.0000 - val_mae: 897859584.0000\n",
      "Epoch 184/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9704392980843986944.0000 - mae: 1127877632.0000\n",
      "Epoch 184: val_loss improved from 1820216760771543040.00000 to 1818870546222284800.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch184-val_Loss1818870546222284800.000-mae1006084096.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 7740752624810459136.0000 - mae: 1006084096.0000 - val_loss: 1818870546222284800.0000 - val_mae: 897163776.0000\n",
      "Epoch 185/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9909594835935821824.0000 - mae: 1141203200.0000\n",
      "Epoch 185: val_loss improved from 1818870546222284800.00000 to 1817491758641053696.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch185-val_Loss1817491758641053696.000-mae1005353856.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 7739054429101359104.0000 - mae: 1005353856.0000 - val_loss: 1817491758641053696.0000 - val_mae: 896451456.0000\n",
      "Epoch 186/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2770978758960611328.0000 - mae: 894981504.0000\n",
      "Epoch 186: val_loss improved from 1817491758641053696.00000 to 1816119980446449664.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch186-val_Loss1816119980446449664.000-mae1004583168.000.h5\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 7737108293520195584.0000 - mae: 1004583168.0000 - val_loss: 1816119980446449664.0000 - val_mae: 895741888.0000\n",
      "Epoch 187/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9530291911655424000.0000 - mae: 1096587776.0000\n",
      "Epoch 187: val_loss improved from 1816119980446449664.00000 to 1814709444466966528.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch187-val_Loss1814709444466966528.000-mae1003813184.000.h5\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 7735374913439006720.0000 - mae: 1003813184.0000 - val_loss: 1814709444466966528.0000 - val_mae: 895012608.0000\n",
      "Epoch 188/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13159225641083600896.0000 - mae: 1151278848.0000\n",
      "Epoch 188: val_loss improved from 1814709444466966528.00000 to 1813274581792718848.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch188-val_Loss1813274581792718848.000-mae1003037696.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 7733542027555504128.0000 - mae: 1003037696.0000 - val_loss: 1813274581792718848.0000 - val_mae: 894270336.0000\n",
      "Epoch 189/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9708557930890002432.0000 - mae: 1125859584.0000\n",
      "Epoch 189: val_loss improved from 1813274581792718848.00000 to 1811827761929519104.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch189-val_Loss1811827761929519104.000-mae1002244416.000.h5\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 7731654166090612736.0000 - mae: 1002244416.0000 - val_loss: 1811827761929519104.0000 - val_mae: 893519808.0000\n",
      "Epoch 190/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2890042674109218816.0000 - mae: 914142592.0000\n",
      "Epoch 190: val_loss improved from 1811827761929519104.00000 to 1810377368653529088.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch190-val_Loss1810377368653529088.000-mae1001435968.000.h5\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 7729644258835038208.0000 - mae: 1001435968.0000 - val_loss: 1810377368653529088.0000 - val_mae: 892766272.0000\n",
      "Epoch 191/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 902124963895443456.0000 - mae: 742384000.0000\n",
      "Epoch 191: val_loss improved from 1810377368653529088.00000 to 1808902511243821056.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch191-val_Loss1808902511243821056.000-mae1000629312.000.h5\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 7727718464218988544.0000 - mae: 1000629312.0000 - val_loss: 1808902511243821056.0000 - val_mae: 892000576.0000\n",
      "Epoch 192/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7769657136237248512.0000 - mae: 1027755008.0000\n",
      "Epoch 192: val_loss improved from 1808902511243821056.00000 to 1807385734953304064.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch192-val_Loss1807385734953304064.000-mae999838272.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 7725895473940135936.0000 - mae: 999838272.0000 - val_loss: 1807385734953304064.0000 - val_mae: 891211584.0000\n",
      "Epoch 193/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 816343540597522432.0000 - mae: 725946944.0000\n",
      "Epoch 193: val_loss improved from 1807385734953304064.00000 to 1805892048406970368.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch193-val_Loss1805892048406970368.000-mae998972288.000.h5\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 7723700298975281152.0000 - mae: 998972288.0000 - val_loss: 1805892048406970368.0000 - val_mae: 890432960.0000\n",
      "Epoch 194/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1114152793330941952.0000 - mae: 796768384.0000\n",
      "Epoch 194: val_loss improved from 1805892048406970368.00000 to 1804356717857734656.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch194-val_Loss1804356717857734656.000-mae998159616.000.h5\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 7721717879510401024.0000 - mae: 998159616.0000 - val_loss: 1804356717857734656.0000 - val_mae: 889632640.0000\n",
      "Epoch 195/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 913855790891139072.0000 - mae: 798601856.0000\n",
      "Epoch 195: val_loss improved from 1804356717857734656.00000 to 1802772321602109440.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch195-val_Loss1802772321602109440.000-mae997304000.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 7719787137092026368.0000 - mae: 997304000.0000 - val_loss: 1802772321602109440.0000 - val_mae: 888805952.0000\n",
      "Epoch 196/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13017473303495835648.0000 - mae: 1085963904.0000\n",
      "Epoch 196: val_loss improved from 1802772321602109440.00000 to 1801163186334859264.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch196-val_Loss1801163186334859264.000-mae996399232.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 7717773381545754624.0000 - mae: 996399232.0000 - val_loss: 1801163186334859264.0000 - val_mae: 887964480.0000\n",
      "Epoch 197/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 792261487170158592.0000 - mae: 739378816.0000\n",
      "Epoch 197: val_loss improved from 1801163186334859264.00000 to 1799569719108304896.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch197-val_Loss1799569719108304896.000-mae995520896.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 7715463857371611136.0000 - mae: 995520896.0000 - val_loss: 1799569719108304896.0000 - val_mae: 887128640.0000\n",
      "Epoch 198/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 820810787621699584.0000 - mae: 736030592.0000\n",
      "Epoch 198: val_loss improved from 1799569719108304896.00000 to 1797962920303263744.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch198-val_Loss1797962920303263744.000-mae994600000.000.h5\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 7713239545348620288.0000 - mae: 994600000.0000 - val_loss: 1797962920303263744.0000 - val_mae: 886286400.0000\n",
      "Epoch 199/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7798011342094336000.0000 - mae: 1014667264.0000\n",
      "Epoch 199: val_loss improved from 1797962920303263744.00000 to 1796269809835442176.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch199-val_Loss1796269809835442176.000-mae993714624.000.h5\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 7711383569720934400.0000 - mae: 993714624.0000 - val_loss: 1796269809835442176.0000 - val_mae: 885397888.0000\n",
      "Epoch 200/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7483292081562058752.0000 - mae: 921003456.0000\n",
      "Epoch 200: val_loss improved from 1796269809835442176.00000 to 1794580135341457408.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch200-val_Loss1794580135341457408.000-mae992772480.000.h5\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 7709076244570046464.0000 - mae: 992772480.0000 - val_loss: 1794580135341457408.0000 - val_mae: 884507584.0000\n",
      "Epoch 201/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 842319365364776960.0000 - mae: 756723712.0000\n",
      "Epoch 201: val_loss improved from 1794580135341457408.00000 to 1792883451460845568.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch201-val_Loss1792883451460845568.000-mae991803584.000.h5\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 7706763421861019648.0000 - mae: 991803584.0000 - val_loss: 1792883451460845568.0000 - val_mae: 883612864.0000\n",
      "Epoch 202/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11347396504764547072.0000 - mae: 1048991808.0000\n",
      "Epoch 202: val_loss improved from 1792883451460845568.00000 to 1791148284673261568.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch202-val_Loss1791148284673261568.000-mae990848192.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 7704556152268259328.0000 - mae: 990848192.0000 - val_loss: 1791148284673261568.0000 - val_mae: 882699776.0000\n",
      "Epoch 203/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19837226060613681152.0000 - mae: 1307843968.0000\n",
      "Epoch 203: val_loss improved from 1791148284673261568.00000 to 1789365426568822784.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch203-val_Loss1789365426568822784.000-mae989868096.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 7702462682128973824.0000 - mae: 989868096.0000 - val_loss: 1789365426568822784.0000 - val_mae: 881757056.0000\n",
      "Epoch 204/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 714728874981720064.0000 - mae: 726530880.0000\n",
      "Epoch 204: val_loss improved from 1789365426568822784.00000 to 1787601809917870080.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch204-val_Loss1787601809917870080.000-mae988887808.000.h5\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 7699948099036250112.0000 - mae: 988887808.0000 - val_loss: 1787601809917870080.0000 - val_mae: 880823424.0000\n",
      "Epoch 205/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 991592225047576576.0000 - mae: 777878144.0000\n",
      "Epoch 205: val_loss improved from 1787601809917870080.00000 to 1785835994243661824.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch205-val_Loss1785835994243661824.000-mae987881984.000.h5\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 7697529723210956800.0000 - mae: 987881984.0000 - val_loss: 1785835994243661824.0000 - val_mae: 879886208.0000\n",
      "Epoch 206/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11371722100017463296.0000 - mae: 1013793216.0000\n",
      "Epoch 206: val_loss improved from 1785835994243661824.00000 to 1784009705429925888.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch206-val_Loss1784009705429925888.000-mae986889472.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 7695369182862376960.0000 - mae: 986889472.0000 - val_loss: 1784009705429925888.0000 - val_mae: 878917760.0000\n",
      "Epoch 207/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 972064211343507456.0000 - mae: 792108992.0000\n",
      "Epoch 207: val_loss improved from 1784009705429925888.00000 to 1782200596485373952.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch207-val_Loss1782200596485373952.000-mae985832704.000.h5\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 7692788629071986688.0000 - mae: 985832704.0000 - val_loss: 1782200596485373952.0000 - val_mae: 877955008.0000\n",
      "Epoch 208/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2828737754035126272.0000 - mae: 896216896.0000\n",
      "Epoch 208: val_loss improved from 1782200596485373952.00000 to 1780342009517572096.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch208-val_Loss1780342009517572096.000-mae984819328.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 7690482403432726528.0000 - mae: 984819328.0000 - val_loss: 1780342009517572096.0000 - val_mae: 876964032.0000\n",
      "Epoch 209/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1011951881859104768.0000 - mae: 772910208.0000\n",
      "Epoch 209: val_loss improved from 1780342009517572096.00000 to 1778473664384073728.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch209-val_Loss1778473664384073728.000-mae983753856.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 7687926038898147328.0000 - mae: 983753856.0000 - val_loss: 1778473664384073728.0000 - val_mae: 875968768.0000\n",
      "Epoch 210/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11421683908383604736.0000 - mae: 1064533824.0000\n",
      "Epoch 210: val_loss improved from 1778473664384073728.00000 to 1776565736831975424.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch210-val_Loss1776565736831975424.000-mae982678912.000.h5\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 7685547245491453952.0000 - mae: 982678912.0000 - val_loss: 1776565736831975424.0000 - val_mae: 874952576.0000\n",
      "Epoch 211/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11255558696542928896.0000 - mae: 1022197376.0000\n",
      "Epoch 211: val_loss improved from 1776565736831975424.00000 to 1774634994413600768.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch211-val_Loss1774634994413600768.000-mae981584576.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 7683003525340594176.0000 - mae: 981584576.0000 - val_loss: 1774634994413600768.0000 - val_mae: 873920320.0000\n",
      "Epoch 212/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2476748622734032896.0000 - mae: 792078080.0000\n",
      "Epoch 212: val_loss improved from 1774634994413600768.00000 to 1772708512602783744.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch212-val_Loss1772708512602783744.000-mae980501440.000.h5\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 7680307522829287424.0000 - mae: 980501440.0000 - val_loss: 1772708512602783744.0000 - val_mae: 872885504.0000\n",
      "Epoch 213/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9862465369522831360.0000 - mae: 1151182848.0000\n",
      "Epoch 213: val_loss improved from 1772708512602783744.00000 to 1770701354126278656.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch213-val_Loss1770701354126278656.000-mae979377728.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 7678045827410952192.0000 - mae: 979377728.0000 - val_loss: 1770701354126278656.0000 - val_mae: 871807296.0000\n",
      "Epoch 214/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13387727247079636992.0000 - mae: 1221938432.0000\n",
      "Epoch 214: val_loss improved from 1770701354126278656.00000 to 1768692408943378432.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch214-val_Loss1768692408943378432.000-mae978237888.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 7675376213178712064.0000 - mae: 978237888.0000 - val_loss: 1768692408943378432.0000 - val_mae: 870726848.0000\n",
      "Epoch 215/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2790689429033844736.0000 - mae: 858550784.0000\n",
      "Epoch 215: val_loss improved from 1768692408943378432.00000 to 1766693359365128192.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch215-val_Loss1766693359365128192.000-mae977091392.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 7672573558039511040.0000 - mae: 977091392.0000 - val_loss: 1766693359365128192.0000 - val_mae: 869649152.0000\n",
      "Epoch 216/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 900918387322912768.0000 - mae: 749375040.0000\n",
      "Epoch 216: val_loss improved from 1766693359365128192.00000 to 1764701044295598080.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch216-val_Loss1764701044295598080.000-mae975945856.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 7669768154121240576.0000 - mae: 975945856.0000 - val_loss: 1764701044295598080.0000 - val_mae: 868571712.0000\n",
      "Epoch 217/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2588489790441652224.0000 - mae: 839752768.0000\n",
      "Epoch 217: val_loss improved from 1764701044295598080.00000 to 1762642071333634048.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch217-val_Loss1762642071333634048.000-mae974788352.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 7667253021272702976.0000 - mae: 974788352.0000 - val_loss: 1762642071333634048.0000 - val_mae: 867457536.0000\n",
      "Epoch 218/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11227259466267230208.0000 - mae: 996803776.0000\n",
      "Epoch 218: val_loss improved from 1762642071333634048.00000 to 1760572653011206144.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch218-val_Loss1760572653011206144.000-mae973603328.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 7664453664668385280.0000 - mae: 973603328.0000 - val_loss: 1760572653011206144.0000 - val_mae: 866339008.0000\n",
      "Epoch 219/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 866121318204964864.0000 - mae: 757221504.0000\n",
      "Epoch 219: val_loss improved from 1760572653011206144.00000 to 1758489490793431040.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch219-val_Loss1758489490793431040.000-mae972446272.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 7661637265633837056.0000 - mae: 972446272.0000 - val_loss: 1758489490793431040.0000 - val_mae: 865206400.0000\n",
      "Epoch 220/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 787030766759444480.0000 - mae: 735489344.0000\n",
      "Epoch 220: val_loss improved from 1758489490793431040.00000 to 1756362210671591424.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch220-val_Loss1756362210671591424.000-mae971231360.000.h5\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 7658924770448113664.0000 - mae: 971231360.0000 - val_loss: 1756362210671591424.0000 - val_mae: 864047552.0000\n",
      "Epoch 221/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9950509862628622336.0000 - mae: 1180075136.0000\n",
      "Epoch 221: val_loss improved from 1756362210671591424.00000 to 1754183528381153280.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch221-val_Loss1754183528381153280.000-mae970010432.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 7656280444983312384.0000 - mae: 970010432.0000 - val_loss: 1754183528381153280.0000 - val_mae: 862859712.0000\n",
      "Epoch 222/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2988698553934675968.0000 - mae: 901670528.0000\n",
      "Epoch 222: val_loss improved from 1754183528381153280.00000 to 1751997974143041536.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch222-val_Loss1751997974143041536.000-mae968751872.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 7653343649425522688.0000 - mae: 968751872.0000 - val_loss: 1751997974143041536.0000 - val_mae: 861667072.0000\n",
      "Epoch 223/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2894313177271500800.0000 - mae: 934361792.0000\n",
      "Epoch 223: val_loss improved from 1751997974143041536.00000 to 1749800050399117312.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch223-val_Loss1749800050399117312.000-mae967500416.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 7650528899658416128.0000 - mae: 967500416.0000 - val_loss: 1749800050399117312.0000 - val_mae: 860464128.0000\n",
      "Epoch 224/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 790294323429113856.0000 - mae: 729936128.0000\n",
      "Epoch 224: val_loss improved from 1749800050399117312.00000 to 1747648306143559680.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch224-val_Loss1747648306143559680.000-mae966233152.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 7647299634007638016.0000 - mae: 966233152.0000 - val_loss: 1747648306143559680.0000 - val_mae: 859285760.0000\n",
      "Epoch 225/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7522329142394617856.0000 - mae: 956413760.0000\n",
      "Epoch 225: val_loss improved from 1747648306143559680.00000 to 1745428529606033408.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch225-val_Loss1745428529606033408.000-mae964981568.000.h5\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 7644620124170747904.0000 - mae: 964981568.0000 - val_loss: 1745428529606033408.0000 - val_mae: 858065472.0000\n",
      "Epoch 226/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11182170693435392000.0000 - mae: 991446784.0000\n",
      "Epoch 226: val_loss improved from 1745428529606033408.00000 to 1743205042216763392.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch226-val_Loss1743205042216763392.000-mae963678016.000.h5\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 7641500259926933504.0000 - mae: 963678016.0000 - val_loss: 1743205042216763392.0000 - val_mae: 856844096.0000\n",
      "Epoch 227/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 870879660932595712.0000 - mae: 757508416.0000\n",
      "Epoch 227: val_loss improved from 1743205042216763392.00000 to 1740945958138544128.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch227-val_Loss1740945958138544128.000-mae962398976.000.h5\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 7638564563880771584.0000 - mae: 962398976.0000 - val_loss: 1740945958138544128.0000 - val_mae: 855595328.0000\n",
      "Epoch 228/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1074159638418554880.0000 - mae: 787003328.0000\n",
      "Epoch 228: val_loss improved from 1740945958138544128.00000 to 1738653064077770752.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch228-val_Loss1738653064077770752.000-mae961066368.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 7635568394695081984.0000 - mae: 961066368.0000 - val_loss: 1738653064077770752.0000 - val_mae: 854327424.0000\n",
      "Epoch 229/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 621874224662839296.0000 - mae: 695360384.0000\n",
      "Epoch 229: val_loss improved from 1738653064077770752.00000 to 1736334468932698112.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch229-val_Loss1736334468932698112.000-mae959741888.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 7632576073800089600.0000 - mae: 959741888.0000 - val_loss: 1736334468932698112.0000 - val_mae: 853044352.0000\n",
      "Epoch 230/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11401868509827825664.0000 - mae: 1014194944.0000\n",
      "Epoch 230: val_loss improved from 1736334468932698112.00000 to 1733981651488210944.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch230-val_Loss1733981651488210944.000-mae958371584.000.h5\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 7629628832881836032.0000 - mae: 958371584.0000 - val_loss: 1733981651488210944.0000 - val_mae: 851740352.0000\n",
      "Epoch 231/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2832392530685853696.0000 - mae: 868021248.0000\n",
      "Epoch 231: val_loss improved from 1733981651488210944.00000 to 1731656459273371648.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch231-val_Loss1731656459273371648.000-mae957006080.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 7626296762893860864.0000 - mae: 957006080.0000 - val_loss: 1731656459273371648.0000 - val_mae: 850444544.0000\n",
      "Epoch 232/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2912345717722841088.0000 - mae: 885365824.0000\n",
      "Epoch 232: val_loss improved from 1731656459273371648.00000 to 1729295670369583104.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch232-val_Loss1729295670369583104.000-mae955628288.000.h5\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 7623194490836090880.0000 - mae: 955628288.0000 - val_loss: 1729295670369583104.0000 - val_mae: 849126656.0000\n",
      "Epoch 233/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18373792576718241792.0000 - mae: 1258014464.0000\n",
      "Epoch 233: val_loss improved from 1729295670369583104.00000 to 1726868361012314112.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch233-val_Loss1726868361012314112.000-mae954263488.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 7620296178185273344.0000 - mae: 954263488.0000 - val_loss: 1726868361012314112.0000 - val_mae: 847773760.0000\n",
      "Epoch 234/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18133054505816686592.0000 - mae: 1210668032.0000\n",
      "Epoch 234: val_loss improved from 1726868361012314112.00000 to 1724428957027139584.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch234-val_Loss1724428957027139584.000-mae952820352.000.h5\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 7617105395441467392.0000 - mae: 952820352.0000 - val_loss: 1724428957027139584.0000 - val_mae: 846408384.0000\n",
      "Epoch 235/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11373347178203316224.0000 - mae: 1053137408.0000\n",
      "Epoch 235: val_loss improved from 1724428957027139584.00000 to 1722018002905333760.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch235-val_Loss1722018002905333760.000-mae951367232.000.h5\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 7613620493337231360.0000 - mae: 951367232.0000 - val_loss: 1722018002905333760.0000 - val_mae: 845056128.0000\n",
      "Epoch 236/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11236963755893981184.0000 - mae: 989061184.0000\n",
      "Epoch 236: val_loss improved from 1722018002905333760.00000 to 1719577636847484928.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch236-val_Loss1719577636847484928.000-mae949948096.000.h5\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 7610407170605056000.0000 - mae: 949948096.0000 - val_loss: 1719577636847484928.0000 - val_mae: 843682304.0000\n",
      "Epoch 237/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 744285809120641024.0000 - mae: 697536256.0000\n",
      "Epoch 237: val_loss improved from 1719577636847484928.00000 to 1717165033458237440.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch237-val_Loss1717165033458237440.000-mae948497664.000.h5\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 7606870041698500608.0000 - mae: 948497664.0000 - val_loss: 1717165033458237440.0000 - val_mae: 842316736.0000\n",
      "Epoch 238/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2726029349227593728.0000 - mae: 842676608.0000\n",
      "Epoch 238: val_loss improved from 1717165033458237440.00000 to 1714633270496329728.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch238-val_Loss1714633270496329728.000-mae947085760.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 7604001415861633024.0000 - mae: 947085760.0000 - val_loss: 1714633270496329728.0000 - val_mae: 840880384.0000\n",
      "Epoch 239/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11326765268580958208.0000 - mae: 1022924160.0000\n",
      "Epoch 239: val_loss improved from 1714633270496329728.00000 to 1712086664127447040.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch239-val_Loss1712086664127447040.000-mae945550272.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 7600738065350393856.0000 - mae: 945550272.0000 - val_loss: 1712086664127447040.0000 - val_mae: 839436096.0000\n",
      "Epoch 240/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13340374579806208000.0000 - mae: 1109245696.0000\n",
      "Epoch 240: val_loss improved from 1712086664127447040.00000 to 1709545692755656704.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch240-val_Loss1709545692755656704.000-mae944005376.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 7597345522222891008.0000 - mae: 944005376.0000 - val_loss: 1709545692755656704.0000 - val_mae: 837989824.0000\n",
      "Epoch 241/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19831618551312023552.0000 - mae: 1309740288.0000\n",
      "Epoch 241: val_loss improved from 1709545692755656704.00000 to 1706996062729797632.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch241-val_Loss1706996062729797632.000-mae942478464.000.h5\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 7593994760537243648.0000 - mae: 942478464.0000 - val_loss: 1706996062729797632.0000 - val_mae: 836534912.0000\n",
      "Epoch 242/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7422164732615852032.0000 - mae: 891406464.0000\n",
      "Epoch 242: val_loss improved from 1706996062729797632.00000 to 1704474607689400320.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch242-val_Loss1704474607689400320.000-mae940973440.000.h5\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 7590337784863260672.0000 - mae: 940973440.0000 - val_loss: 1704474607689400320.0000 - val_mae: 835093248.0000\n",
      "Epoch 243/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2893807401922723840.0000 - mae: 860152960.0000\n",
      "Epoch 243: val_loss improved from 1704474607689400320.00000 to 1701923465835053056.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch243-val_Loss1701923465835053056.000-mae939455104.000.h5\n",
      "3/3 [==============================] - 0s 154ms/step - loss: 7586887517375299584.0000 - mae: 939455104.0000 - val_loss: 1701923465835053056.0000 - val_mae: 833628608.0000\n",
      "Epoch 244/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 983975014648774656.0000 - mae: 764879616.0000\n",
      "Epoch 244: val_loss improved from 1701923465835053056.00000 to 1699320372056293376.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch244-val_Loss1699320372056293376.000-mae937921920.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 7583557646410579968.0000 - mae: 937921920.0000 - val_loss: 1699320372056293376.0000 - val_mae: 832134144.0000\n",
      "Epoch 245/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 660577514996891648.0000 - mae: 690447936.0000\n",
      "Epoch 245: val_loss improved from 1699320372056293376.00000 to 1696737894120554496.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch245-val_Loss1696737894120554496.000-mae936330496.000.h5\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 7579879230259855360.0000 - mae: 936330496.0000 - val_loss: 1696737894120554496.0000 - val_mae: 830646400.0000\n",
      "Epoch 246/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 645959095588749312.0000 - mae: 687864832.0000\n",
      "Epoch 246: val_loss improved from 1696737894120554496.00000 to 1694104838649937920.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch246-val_Loss1694104838649937920.000-mae934728768.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 7576558705143971840.0000 - mae: 934728768.0000 - val_loss: 1694104838649937920.0000 - val_mae: 829127040.0000\n",
      "Epoch 247/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9164211164454846464.0000 - mae: 961766720.0000\n",
      "Epoch 247: val_loss improved from 1694104838649937920.00000 to 1691428352470024192.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch247-val_Loss1691428352470024192.000-mae933162240.000.h5\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 7573210692237393920.0000 - mae: 933162240.0000 - val_loss: 1691428352470024192.0000 - val_mae: 827576640.0000\n",
      "Epoch 248/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 666929324951076864.0000 - mae: 707247488.0000\n",
      "Epoch 248: val_loss improved from 1691428352470024192.00000 to 1688771107743596544.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch248-val_Loss1688771107743596544.000-mae931516416.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 7569474001970397184.0000 - mae: 931516416.0000 - val_loss: 1688771107743596544.0000 - val_mae: 826035328.0000\n",
      "Epoch 249/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 764198858052861952.0000 - mae: 690876672.0000\n",
      "Epoch 249: val_loss improved from 1688771107743596544.00000 to 1686096820586938368.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch249-val_Loss1686096820586938368.000-mae929927040.000.h5\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 7565940171598725120.0000 - mae: 929927040.0000 - val_loss: 1686096820586938368.0000 - val_mae: 824478016.0000\n",
      "Epoch 250/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9432142906690371584.0000 - mae: 1067043968.0000\n",
      "Epoch 250: val_loss improved from 1686096820586938368.00000 to 1683381439183192064.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch250-val_Loss1683381439183192064.000-mae928229248.000.h5\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 7562531685552619520.0000 - mae: 928229248.0000 - val_loss: 1683381439183192064.0000 - val_mae: 822890816.0000\n",
      "Epoch 251/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18087873374008115200.0000 - mae: 1212073472.0000\n",
      "Epoch 251: val_loss improved from 1683381439183192064.00000 to 1680652726200958976.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch251-val_Loss1680652726200958976.000-mae926575232.000.h5\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 7558889553285611520.0000 - mae: 926575232.0000 - val_loss: 1680652726200958976.0000 - val_mae: 821296640.0000\n",
      "Epoch 252/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 966135850804969472.0000 - mae: 727757248.0000\n",
      "Epoch 252: val_loss improved from 1680652726200958976.00000 to 1677978164166393856.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch252-val_Loss1677978164166393856.000-mae924907072.000.h5\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 7554807616367493120.0000 - mae: 924907072.0000 - val_loss: 1677978164166393856.0000 - val_mae: 819723840.0000\n",
      "Epoch 253/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 520378306302836736.0000 - mae: 633014144.0000\n",
      "Epoch 253: val_loss improved from 1677978164166393856.00000 to 1675251512768462848.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch253-val_Loss1675251512768462848.000-mae923241536.000.h5\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 7551261141612101632.0000 - mae: 923241536.0000 - val_loss: 1675251512768462848.0000 - val_mae: 818115520.0000\n",
      "Epoch 254/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2557566850544173056.0000 - mae: 808296000.0000\n",
      "Epoch 254: val_loss improved from 1675251512768462848.00000 to 1672467549326934016.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch254-val_Loss1672467549326934016.000-mae921537792.000.h5\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 7547691027356712960.0000 - mae: 921537792.0000 - val_loss: 1672467549326934016.0000 - val_mae: 816469952.0000\n",
      "Epoch 255/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17903523756937052160.0000 - mae: 1184667008.0000\n",
      "Epoch 255: val_loss improved from 1672467549326934016.00000 to 1669629434937737216.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch255-val_Loss1669629434937737216.000-mae919778880.000.h5\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 7544196229647826944.0000 - mae: 919778880.0000 - val_loss: 1669629434937737216.0000 - val_mae: 814791424.0000\n",
      "Epoch 256/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 993849934736261120.0000 - mae: 736731776.0000\n",
      "Epoch 256: val_loss improved from 1669629434937737216.00000 to 1666861689292718080.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch256-val_Loss1666861689292718080.000-mae918024320.000.h5\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 7539904835764617216.0000 - mae: 918024320.0000 - val_loss: 1666861689292718080.0000 - val_mae: 813146496.0000\n",
      "Epoch 257/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 794259162358874112.0000 - mae: 720344320.0000\n",
      "Epoch 257: val_loss improved from 1666861689292718080.00000 to 1664027285755265024.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch257-val_Loss1664027285755265024.000-mae916306112.000.h5\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 7536277546904584192.0000 - mae: 916306112.0000 - val_loss: 1664027285755265024.0000 - val_mae: 811457216.0000\n",
      "Epoch 258/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 602074906464616448.0000 - mae: 650154688.0000\n",
      "Epoch 258: val_loss improved from 1664027285755265024.00000 to 1661167593450373120.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch258-val_Loss1661167593450373120.000-mae914477056.000.h5\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 7532472686916665344.0000 - mae: 914477056.0000 - val_loss: 1661167593450373120.0000 - val_mae: 809748992.0000\n",
      "Epoch 259/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1175010143453052928.0000 - mae: 791164544.0000\n",
      "Epoch 259: val_loss improved from 1661167593450373120.00000 to 1658281650305368064.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch259-val_Loss1658281650305368064.000-mae912703424.000.h5\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 7528683220091535360.0000 - mae: 912703424.0000 - val_loss: 1658281650305368064.0000 - val_mae: 808014016.0000\n",
      "Epoch 260/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9481074472661286912.0000 - mae: 1036519296.0000\n",
      "Epoch 260: val_loss improved from 1658281650305368064.00000 to 1655334546826067968.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch260-val_Loss1655334546826067968.000-mae910858624.000.h5\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 7525072973661732864.0000 - mae: 910858624.0000 - val_loss: 1655334546826067968.0000 - val_mae: 806240192.0000\n",
      "Epoch 261/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2610878321084334080.0000 - mae: 850607424.0000\n",
      "Epoch 261: val_loss improved from 1655334546826067968.00000 to 1652410807968858112.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch261-val_Loss1652410807968858112.000-mae909019904.000.h5\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 7521024571848261632.0000 - mae: 909019904.0000 - val_loss: 1652410807968858112.0000 - val_mae: 804474432.0000\n",
      "Epoch 262/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17869801735313162240.0000 - mae: 1161968896.0000\n",
      "Epoch 262: val_loss improved from 1652410807968858112.00000 to 1649488306062229504.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch262-val_Loss1649488306062229504.000-mae907172800.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 7517127902639423488.0000 - mae: 907172800.0000 - val_loss: 1649488306062229504.0000 - val_mae: 802706752.0000\n",
      "Epoch 263/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7409355971908075520.0000 - mae: 867929408.0000\n",
      "Epoch 263: val_loss improved from 1649488306062229504.00000 to 1646584908170133504.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch263-val_Loss1646584908170133504.000-mae905324928.000.h5\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 7513025624756191232.0000 - mae: 905324928.0000 - val_loss: 1646584908170133504.0000 - val_mae: 800942336.0000\n",
      "Epoch 264/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9575267434789601280.0000 - mae: 1026007104.0000\n",
      "Epoch 264: val_loss improved from 1646584908170133504.00000 to 1643650036757692416.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch264-val_Loss1643650036757692416.000-mae903511616.000.h5\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 7509155343826419712.0000 - mae: 903511616.0000 - val_loss: 1643650036757692416.0000 - val_mae: 799149696.0000\n",
      "Epoch 265/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7405641821629448192.0000 - mae: 851943552.0000\n",
      "Epoch 265: val_loss improved from 1643650036757692416.00000 to 1640709392909205504.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch265-val_Loss1640709392909205504.000-mae901615744.000.h5\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 7505114088838529024.0000 - mae: 901615744.0000 - val_loss: 1640709392909205504.0000 - val_mae: 797354304.0000\n",
      "Epoch 266/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11270123927076077568.0000 - mae: 939040000.0000\n",
      "Epoch 266: val_loss improved from 1640709392909205504.00000 to 1637744972121767936.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch266-val_Loss1637744972121767936.000-mae899767232.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 7501120662606446592.0000 - mae: 899767232.0000 - val_loss: 1637744972121767936.0000 - val_mae: 795541312.0000\n",
      "Epoch 267/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 561406685772513280.0000 - mae: 633215680.0000\n",
      "Epoch 267: val_loss improved from 1637744972121767936.00000 to 1634798418398281728.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch267-val_Loss1634798418398281728.000-mae897884096.000.h5\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 7496866652118581248.0000 - mae: 897884096.0000 - val_loss: 1634798418398281728.0000 - val_mae: 793727936.0000\n",
      "Epoch 268/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7595871626885857280.0000 - mae: 936692352.0000\n",
      "Epoch 268: val_loss improved from 1634798418398281728.00000 to 1631791528974221312.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch268-val_Loss1631791528974221312.000-mae895978304.000.h5\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 7493050247258570752.0000 - mae: 895978304.0000 - val_loss: 1631791528974221312.0000 - val_mae: 791866048.0000\n",
      "Epoch 269/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2624055418187415552.0000 - mae: 815534592.0000\n",
      "Epoch 269: val_loss improved from 1631791528974221312.00000 to 1628732550186795008.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch269-val_Loss1628732550186795008.000-mae894034880.000.h5\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 7489074962968346624.0000 - mae: 894034880.0000 - val_loss: 1628732550186795008.0000 - val_mae: 789967040.0000\n",
      "Epoch 270/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13002800320823164928.0000 - mae: 1073866496.0000\n",
      "Epoch 270: val_loss improved from 1628732550186795008.00000 to 1625661201893556224.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch270-val_Loss1625661201893556224.000-mae892044864.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 7484999623119994880.0000 - mae: 892044864.0000 - val_loss: 1625661201893556224.0000 - val_mae: 788057984.0000\n",
      "Epoch 271/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18335868221652992000.0000 - mae: 1285223168.0000\n",
      "Epoch 271: val_loss improved from 1625661201893556224.00000 to 1622580507751481344.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch271-val_Loss1622580507751481344.000-mae890101632.000.h5\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 7480966614469312512.0000 - mae: 890101632.0000 - val_loss: 1622580507751481344.0000 - val_mae: 786138048.0000\n",
      "Epoch 272/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7358919724274548736.0000 - mae: 839066688.0000\n",
      "Epoch 272: val_loss improved from 1622580507751481344.00000 to 1619566059184979968.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch272-val_Loss1619566059184979968.000-mae888179072.000.h5\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 7476349765144281088.0000 - mae: 888179072.0000 - val_loss: 1619566059184979968.0000 - val_mae: 784244736.0000\n",
      "Epoch 273/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 825161348974379008.0000 - mae: 702423040.0000\n",
      "Epoch 273: val_loss improved from 1619566059184979968.00000 to 1616491000039997440.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch273-val_Loss1616491000039997440.000-mae886241344.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 7472299714063368192.0000 - mae: 886241344.0000 - val_loss: 1616491000039997440.0000 - val_mae: 782306560.0000\n",
      "Epoch 274/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18058227241988390912.0000 - mae: 1173586304.0000\n",
      "Epoch 274: val_loss improved from 1616491000039997440.00000 to 1613341311543279616.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch274-val_Loss1613341311543279616.000-mae884262976.000.h5\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 7468488806761496576.0000 - mae: 884262976.0000 - val_loss: 1613341311543279616.0000 - val_mae: 780321280.0000\n",
      "Epoch 275/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11006115792084271104.0000 - mae: 897092928.0000\n",
      "Epoch 275: val_loss improved from 1613341311543279616.00000 to 1610246048872136704.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch275-val_Loss1610246048872136704.000-mae882333568.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 7463985756889939968.0000 - mae: 882333568.0000 - val_loss: 1610246048872136704.0000 - val_mae: 778361536.0000\n",
      "Epoch 276/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7394444395212177408.0000 - mae: 868955520.0000\n",
      "Epoch 276: val_loss improved from 1610246048872136704.00000 to 1607162331073085440.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch276-val_Loss1607162331073085440.000-mae880478784.000.h5\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 7459657529367199744.0000 - mae: 880478784.0000 - val_loss: 1607162331073085440.0000 - val_mae: 776401024.0000\n",
      "Epoch 277/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11400098296107106304.0000 - mae: 976560256.0000\n",
      "Epoch 277: val_loss improved from 1607162331073085440.00000 to 1604020614035668992.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch277-val_Loss1604020614035668992.000-mae878563136.000.h5\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 7455554151972339712.0000 - mae: 878563136.0000 - val_loss: 1604020614035668992.0000 - val_mae: 774395072.0000\n",
      "Epoch 278/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 860111662525448192.0000 - mae: 703232512.0000\n",
      "Epoch 278: val_loss improved from 1604020614035668992.00000 to 1600924801608712192.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch278-val_Loss1600924801608712192.000-mae876616064.000.h5\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 7450971387507769344.0000 - mae: 876616064.0000 - val_loss: 1600924801608712192.0000 - val_mae: 772405440.0000\n",
      "Epoch 279/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11299353344188874752.0000 - mae: 937613312.0000\n",
      "Epoch 279: val_loss improved from 1600924801608712192.00000 to 1597772501771878400.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch279-val_Loss1597772501771878400.000-mae874662208.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 7446822930136170496.0000 - mae: 874662208.0000 - val_loss: 1597772501771878400.0000 - val_mae: 770381376.0000\n",
      "Epoch 280/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9224924547273195520.0000 - mae: 964119232.0000\n",
      "Epoch 280: val_loss improved from 1597772501771878400.00000 to 1594591339754815488.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch280-val_Loss1594591339754815488.000-mae872738688.000.h5\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 7442631042055274496.0000 - mae: 872738688.0000 - val_loss: 1594591339754815488.0000 - val_mae: 768316864.0000\n",
      "Epoch 281/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2396800658377277440.0000 - mae: 734850944.0000\n",
      "Epoch 281: val_loss improved from 1594591339754815488.00000 to 1591402893473218560.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch281-val_Loss1591402893473218560.000-mae870790016.000.h5\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 7438125793160462336.0000 - mae: 870790016.0000 - val_loss: 1591402893473218560.0000 - val_mae: 766244608.0000\n",
      "Epoch 282/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 448855212354961408.0000 - mae: 574431040.0000\n",
      "Epoch 282: val_loss improved from 1591402893473218560.00000 to 1588258152778825728.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch282-val_Loss1588258152778825728.000-mae868908032.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 7433386348288933888.0000 - mae: 868908032.0000 - val_loss: 1588258152778825728.0000 - val_mae: 764235584.0000\n",
      "Epoch 283/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2473366524966993920.0000 - mae: 726170240.0000\n",
      "Epoch 283: val_loss improved from 1588258152778825728.00000 to 1584999475192004608.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch283-val_Loss1584999475192004608.000-mae867014720.000.h5\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 7429447347882426368.0000 - mae: 867014720.0000 - val_loss: 1584999475192004608.0000 - val_mae: 762191296.0000\n",
      "Epoch 284/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 839437545388376064.0000 - mae: 695693312.0000\n",
      "Epoch 284: val_loss improved from 1584999475192004608.00000 to 1581746570041229312.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch284-val_Loss1581746570041229312.000-mae865045248.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 7424977283359703040.0000 - mae: 865045248.0000 - val_loss: 1581746570041229312.0000 - val_mae: 760144192.0000\n",
      "Epoch 285/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11390245572410605568.0000 - mae: 973144192.0000\n",
      "Epoch 285: val_loss improved from 1581746570041229312.00000 to 1578459305152086016.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch285-val_Loss1578459305152086016.000-mae863195328.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 7420739765546254336.0000 - mae: 863195328.0000 - val_loss: 1578459305152086016.0000 - val_mae: 758100160.0000\n",
      "Epoch 286/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 754085618820055040.0000 - mae: 635671296.0000\n",
      "Epoch 286: val_loss improved from 1578459305152086016.00000 to 1575243371079794688.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch286-val_Loss1575243371079794688.000-mae861271104.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 7415990974825889792.0000 - mae: 861271104.0000 - val_loss: 1575243371079794688.0000 - val_mae: 756096192.0000\n",
      "Epoch 287/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9146798748561571840.0000 - mae: 956741248.0000\n",
      "Epoch 287: val_loss improved from 1575243371079794688.00000 to 1571970812158672896.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch287-val_Loss1571970812158672896.000-mae859305152.000.h5\n",
      "3/3 [==============================] - 0s 190ms/step - loss: 7411912886198468608.0000 - mae: 859305152.0000 - val_loss: 1571970812158672896.0000 - val_mae: 754041792.0000\n",
      "Epoch 288/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2549540690539315200.0000 - mae: 774598528.0000\n",
      "Epoch 288: val_loss improved from 1571970812158672896.00000 to 1568783602827657216.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch288-val_Loss1568783602827657216.000-mae857417152.000.h5\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 7406927700478132224.0000 - mae: 857417152.0000 - val_loss: 1568783602827657216.0000 - val_mae: 752104640.0000\n",
      "Epoch 289/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2670870424274862080.0000 - mae: 766344064.0000\n",
      "Epoch 289: val_loss improved from 1568783602827657216.00000 to 1565517640976302080.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch289-val_Loss1565517640976302080.000-mae855519936.000.h5\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 7402703376804216832.0000 - mae: 855519936.0000 - val_loss: 1565517640976302080.0000 - val_mae: 750117504.0000\n",
      "Epoch 290/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7454134132705067008.0000 - mae: 850817984.0000\n",
      "Epoch 290: val_loss improved from 1565517640976302080.00000 to 1562298270930173952.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch290-val_Loss1562298270930173952.000-mae853635904.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 7398024405072216064.0000 - mae: 853635904.0000 - val_loss: 1562298270930173952.0000 - val_mae: 748150592.0000\n",
      "Epoch 291/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2267474039133962240.0000 - mae: 686170240.0000\n",
      "Epoch 291: val_loss improved from 1562298270930173952.00000 to 1559045228340445184.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch291-val_Loss1559045228340445184.000-mae851697920.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 7393448237677412352.0000 - mae: 851697920.0000 - val_loss: 1559045228340445184.0000 - val_mae: 746150464.0000\n",
      "Epoch 292/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7413981617326129152.0000 - mae: 812818176.0000\n",
      "Epoch 292: val_loss improved from 1559045228340445184.00000 to 1555734873707118592.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch292-val_Loss1555734873707118592.000-mae849796672.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 7389209620352335872.0000 - mae: 849796672.0000 - val_loss: 1555734873707118592.0000 - val_mae: 744112448.0000\n",
      "Epoch 293/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2537518905279119360.0000 - mae: 750498368.0000\n",
      "Epoch 293: val_loss improved from 1555734873707118592.00000 to 1552439912236580864.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch293-val_Loss1552439912236580864.000-mae847822016.000.h5\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 7384519103748243456.0000 - mae: 847822016.0000 - val_loss: 1552439912236580864.0000 - val_mae: 742066816.0000\n",
      "Epoch 294/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7695552251548401664.0000 - mae: 913778048.0000\n",
      "Epoch 294: val_loss improved from 1552439912236580864.00000 to 1549144263571275776.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch294-val_Loss1549144263571275776.000-mae845893312.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 7379996262667386880.0000 - mae: 845893312.0000 - val_loss: 1549144263571275776.0000 - val_mae: 740012288.0000\n",
      "Epoch 295/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 585433316941889536.0000 - mae: 619745600.0000\n",
      "Epoch 295: val_loss improved from 1549144263571275776.00000 to 1545891770737360896.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch295-val_Loss1545891770737360896.000-mae843954944.000.h5\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 7374993484761006080.0000 - mae: 843954944.0000 - val_loss: 1545891770737360896.0000 - val_mae: 737979328.0000\n",
      "Epoch 296/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12958572465595875328.0000 - mae: 1002720000.0000\n",
      "Epoch 296: val_loss improved from 1545891770737360896.00000 to 1542482872374394880.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch296-val_Loss1542482872374394880.000-mae842043008.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 7371230406214942720.0000 - mae: 842043008.0000 - val_loss: 1542482872374394880.0000 - val_mae: 735836608.0000\n",
      "Epoch 297/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9134740954295566336.0000 - mae: 941744000.0000\n",
      "Epoch 297: val_loss improved from 1542482872374394880.00000 to 1539173617252696064.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch297-val_Loss1539173617252696064.000-mae840052608.000.h5\n",
      "3/3 [==============================] - 0s 184ms/step - loss: 7366222680506236928.0000 - mae: 840052608.0000 - val_loss: 1539173617252696064.0000 - val_mae: 733741312.0000\n",
      "Epoch 298/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7524262633592061952.0000 - mae: 869066048.0000\n",
      "Epoch 298: val_loss improved from 1539173617252696064.00000 to 1535900783453667328.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch298-val_Loss1535900783453667328.000-mae838123456.000.h5\n",
      "3/3 [==============================] - 0s 202ms/step - loss: 7361343597657980928.0000 - mae: 838123456.0000 - val_loss: 1535900783453667328.0000 - val_mae: 731665856.0000\n",
      "Epoch 299/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2627950987884625920.0000 - mae: 773808128.0000\n",
      "Epoch 299: val_loss improved from 1535900783453667328.00000 to 1532578059314528256.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch299-val_Loss1532578059314528256.000-mae836193472.000.h5\n",
      "3/3 [==============================] - 0s 176ms/step - loss: 7356802064879452160.0000 - mae: 836193472.0000 - val_loss: 1532578059314528256.0000 - val_mae: 729542848.0000\n",
      "Epoch 300/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11340913784207179776.0000 - mae: 979559680.0000\n",
      "Epoch 300: val_loss improved from 1532578059314528256.00000 to 1529203108373069824.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch300-val_Loss1529203108373069824.000-mae834280704.000.h5\n",
      "3/3 [==============================] - 0s 170ms/step - loss: 7352438103228809216.0000 - mae: 834280704.0000 - val_loss: 1529203108373069824.0000 - val_mae: 727384576.0000\n",
      "Epoch 301/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 409650407279689728.0000 - mae: 553225728.0000\n",
      "Epoch 301: val_loss improved from 1529203108373069824.00000 to 1525917492751368192.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch301-val_Loss1525917492751368192.000-mae832238208.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 7347441922392195072.0000 - mae: 832238208.0000 - val_loss: 1525917492751368192.0000 - val_mae: 725306624.0000\n",
      "Epoch 302/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9251669068107218944.0000 - mae: 935104320.0000\n",
      "Epoch 302: val_loss improved from 1525917492751368192.00000 to 1522571403990138880.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch302-val_Loss1522571403990138880.000-mae830280576.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 7343205504090374144.0000 - mae: 830280576.0000 - val_loss: 1522571403990138880.0000 - val_mae: 723207808.0000\n",
      "Epoch 303/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11335588849393860608.0000 - mae: 964381696.0000\n",
      "Epoch 303: val_loss improved from 1522571403990138880.00000 to 1519242357659140096.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch303-val_Loss1519242357659140096.000-mae828369792.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 7338435822649081856.0000 - mae: 828369792.0000 - val_loss: 1519242357659140096.0000 - val_mae: 721113088.0000\n",
      "Epoch 304/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10925277498186924032.0000 - mae: 845817920.0000\n",
      "Epoch 304: val_loss improved from 1519242357659140096.00000 to 1515952481429880832.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch304-val_Loss1515952481429880832.000-mae826358976.000.h5\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 7333695828021739520.0000 - mae: 826358976.0000 - val_loss: 1515952481429880832.0000 - val_mae: 719030592.0000\n",
      "Epoch 305/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2641685812260896768.0000 - mae: 762013120.0000\n",
      "Epoch 305: val_loss improved from 1515952481429880832.00000 to 1512652159840157696.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch305-val_Loss1512652159840157696.000-mae824479680.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 7329089424057171968.0000 - mae: 824479680.0000 - val_loss: 1512652159840157696.0000 - val_mae: 716921600.0000\n",
      "Epoch 306/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10941205023626887168.0000 - mae: 860581504.0000\n",
      "Epoch 306: val_loss improved from 1512652159840157696.00000 to 1509352800323108864.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch306-val_Loss1509352800323108864.000-mae822528192.000.h5\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 7324392860139126784.0000 - mae: 822528192.0000 - val_loss: 1509352800323108864.0000 - val_mae: 714843264.0000\n",
      "Epoch 307/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2879475267854663680.0000 - mae: 846821632.0000\n",
      "Epoch 307: val_loss improved from 1509352800323108864.00000 to 1506060725070594048.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch307-val_Loss1506060725070594048.000-mae820571648.000.h5\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 7319690248907128832.0000 - mae: 820571648.0000 - val_loss: 1506060725070594048.0000 - val_mae: 712799424.0000\n",
      "Epoch 308/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2251837471958499328.0000 - mae: 658716352.0000\n",
      "Epoch 308: val_loss improved from 1506060725070594048.00000 to 1502805895774470144.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch308-val_Loss1502805895774470144.000-mae818588864.000.h5\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 7314731451465859072.0000 - mae: 818588864.0000 - val_loss: 1502805895774470144.0000 - val_mae: 710824704.0000\n",
      "Epoch 309/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 692614947368075264.0000 - mae: 641168896.0000\n",
      "Epoch 309: val_loss improved from 1502805895774470144.00000 to 1499497602725445632.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch309-val_Loss1499497602725445632.000-mae816685056.000.h5\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 7310258638164066304.0000 - mae: 816685056.0000 - val_loss: 1499497602725445632.0000 - val_mae: 708866944.0000\n",
      "Epoch 310/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 737276010176708608.0000 - mae: 620895616.0000\n",
      "Epoch 310: val_loss improved from 1499497602725445632.00000 to 1496185186507816960.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch310-val_Loss1496185186507816960.000-mae814707968.000.h5\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 7305488406966960128.0000 - mae: 814707968.0000 - val_loss: 1496185186507816960.0000 - val_mae: 706924608.0000\n",
      "Epoch 311/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 650245679108587520.0000 - mae: 574716416.0000\n",
      "Epoch 311: val_loss improved from 1496185186507816960.00000 to 1492843358354145280.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch311-val_Loss1492843358354145280.000-mae812770432.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 7300901244455878656.0000 - mae: 812770432.0000 - val_loss: 1492843358354145280.0000 - val_mae: 704955776.0000\n",
      "Epoch 312/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7250743173264179200.0000 - mae: 816286592.0000\n",
      "Epoch 312: val_loss improved from 1492843358354145280.00000 to 1489503866662682624.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch312-val_Loss1489503866662682624.000-mae810801664.000.h5\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 7296284395130847232.0000 - mae: 810801664.0000 - val_loss: 1489503866662682624.0000 - val_mae: 702977088.0000\n",
      "Epoch 313/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8997254721823571968.0000 - mae: 896516928.0000\n",
      "Epoch 313: val_loss improved from 1489503866662682624.00000 to 1486141285227036672.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch313-val_Loss1486141285227036672.000-mae808888640.000.h5\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 7291773648677896192.0000 - mae: 808888640.0000 - val_loss: 1486141285227036672.0000 - val_mae: 700972032.0000\n",
      "Epoch 314/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19631894463149768704.0000 - mae: 1220874624.0000\n",
      "Epoch 314: val_loss improved from 1486141285227036672.00000 to 1482754377096626176.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch314-val_Loss1482754377096626176.000-mae807010688.000.h5\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 7287340417794703360.0000 - mae: 807010688.0000 - val_loss: 1482754377096626176.0000 - val_mae: 698941120.0000\n",
      "Epoch 315/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12958936403944669184.0000 - mae: 1016714560.0000\n",
      "Epoch 315: val_loss improved from 1482754377096626176.00000 to 1479460927454576640.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch315-val_Loss1479460927454576640.000-mae805121664.000.h5\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 7282151822423228416.0000 - mae: 805121664.0000 - val_loss: 1479460927454576640.0000 - val_mae: 696967808.0000\n",
      "Epoch 316/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 867805770018717696.0000 - mae: 643098240.0000\n",
      "Epoch 316: val_loss improved from 1479460927454576640.00000 to 1476209396693336064.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch316-val_Loss1476209396693336064.000-mae803344064.000.h5\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 7277260095191252992.0000 - mae: 803344064.0000 - val_loss: 1476209396693336064.0000 - val_mae: 695065216.0000\n",
      "Epoch 317/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10824972350919802880.0000 - mae: 825019520.0000\n",
      "Epoch 317: val_loss improved from 1476209396693336064.00000 to 1472946458498957312.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch317-val_Loss1472946458498957312.000-mae801429440.000.h5\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 7272716363389468672.0000 - mae: 801429440.0000 - val_loss: 1472946458498957312.0000 - val_mae: 693146432.0000\n",
      "Epoch 318/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 800757550956937216.0000 - mae: 592771520.0000\n",
      "Epoch 318: val_loss improved from 1472946458498957312.00000 to 1469730386987712512.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch318-val_Loss1469730386987712512.000-mae799502720.000.h5\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 7267799347390054400.0000 - mae: 799502720.0000 - val_loss: 1469730386987712512.0000 - val_mae: 691242752.0000\n",
      "Epoch 319/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7268020349227237376.0000 - mae: 801308480.0000\n",
      "Epoch 319: val_loss improved from 1469730386987712512.00000 to 1466488064636354560.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch319-val_Loss1466488064636354560.000-mae797741504.000.h5\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 7263246819495247872.0000 - mae: 797741504.0000 - val_loss: 1466488064636354560.0000 - val_mae: 689313152.0000\n",
      "Epoch 320/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17796349960531214336.0000 - mae: 1100389632.0000\n",
      "Epoch 320: val_loss improved from 1466488064636354560.00000 to 1463164790741401600.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch320-val_Loss1463164790741401600.000-mae795836928.000.h5\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 7258944430495760384.0000 - mae: 795836928.0000 - val_loss: 1463164790741401600.0000 - val_mae: 687325632.0000\n",
      "Epoch 321/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 714652871240450048.0000 - mae: 582504704.0000\n",
      "Epoch 321: val_loss improved from 1463164790741401600.00000 to 1459955178860969984.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch321-val_Loss1459955178860969984.000-mae793994304.000.h5\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 7253559022542913536.0000 - mae: 793994304.0000 - val_loss: 1459955178860969984.0000 - val_mae: 685392384.0000\n",
      "Epoch 322/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10734816795488681984.0000 - mae: 789462656.0000\n",
      "Epoch 322: val_loss improved from 1459955178860969984.00000 to 1456666402143338496.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch322-val_Loss1456666402143338496.000-mae792137536.000.h5\n",
      "3/3 [==============================] - 0s 154ms/step - loss: 7249335248624812032.0000 - mae: 792137536.0000 - val_loss: 1456666402143338496.0000 - val_mae: 683401344.0000\n",
      "Epoch 323/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 427704113329864704.0000 - mae: 541733696.0000\n",
      "Epoch 323: val_loss improved from 1456666402143338496.00000 to 1453478505617555456.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch323-val_Loss1453478505617555456.000-mae790285504.000.h5\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 7244104871811481600.0000 - mae: 790285504.0000 - val_loss: 1453478505617555456.0000 - val_mae: 681457216.0000\n",
      "Epoch 324/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9020317527971987456.0000 - mae: 886125120.0000\n",
      "Epoch 324: val_loss improved from 1453478505617555456.00000 to 1450150283920277504.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch324-val_Loss1450150283920277504.000-mae788496448.000.h5\n",
      "3/3 [==============================] - 0s 199ms/step - loss: 7240274723056123904.0000 - mae: 788496448.0000 - val_loss: 1450150283920277504.0000 - val_mae: 679413376.0000\n",
      "Epoch 325/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 840611205331550208.0000 - mae: 632518144.0000\n",
      "Epoch 325: val_loss improved from 1450150283920277504.00000 to 1446971595804377088.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch325-val_Loss1446971595804377088.000-mae786599424.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 7234739781521899520.0000 - mae: 786599424.0000 - val_loss: 1446971595804377088.0000 - val_mae: 677447296.0000\n",
      "Epoch 326/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2217632489852108800.0000 - mae: 640580480.0000\n",
      "Epoch 326: val_loss improved from 1446971595804377088.00000 to 1443714155168137216.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch326-val_Loss1443714155168137216.000-mae784791232.000.h5\n",
      "3/3 [==============================] - 0s 174ms/step - loss: 7230563286603792384.0000 - mae: 784791232.0000 - val_loss: 1443714155168137216.0000 - val_mae: 675423168.0000\n",
      "Epoch 327/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10831607903593431040.0000 - mae: 820822976.0000\n",
      "Epoch 327: val_loss improved from 1443714155168137216.00000 to 1440521310840029184.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch327-val_Loss1440521310840029184.000-mae782850880.000.h5\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 7225518727255556096.0000 - mae: 782850880.0000 - val_loss: 1440521310840029184.0000 - val_mae: 673427520.0000\n",
      "Epoch 328/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 554324147262455808.0000 - mae: 538492416.0000\n",
      "Epoch 328: val_loss improved from 1440521310840029184.00000 to 1437343584796803072.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch328-val_Loss1437343584796803072.000-mae781038656.000.h5\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 7220679776581713920.0000 - mae: 781038656.0000 - val_loss: 1437343584796803072.0000 - val_mae: 671424064.0000\n",
      "Epoch 329/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12845780164772102144.0000 - mae: 1005890176.0000\n",
      "Epoch 329: val_loss improved from 1437343584796803072.00000 to 1434063054416379904.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch329-val_Loss1434063054416379904.000-mae779155136.000.h5\n",
      "3/3 [==============================] - 0s 171ms/step - loss: 7216561006024065024.0000 - mae: 779155136.0000 - val_loss: 1434063054416379904.0000 - val_mae: 669347328.0000\n",
      "Epoch 330/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17639045030968557568.0000 - mae: 1042145152.0000\n",
      "Epoch 330: val_loss improved from 1434063054416379904.00000 to 1430799291588280320.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch330-val_Loss1430799291588280320.000-mae777319616.000.h5\n",
      "3/3 [==============================] - 0s 170ms/step - loss: 7211995833745539072.0000 - mae: 777319616.0000 - val_loss: 1430799291588280320.0000 - val_mae: 667264640.0000\n",
      "Epoch 331/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17699480787100893184.0000 - mae: 1095205504.0000\n",
      "Epoch 331: val_loss improved from 1430799291588280320.00000 to 1427591466414243840.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch331-val_Loss1427591466414243840.000-mae775563584.000.h5\n",
      "3/3 [==============================] - 0s 181ms/step - loss: 7207186020129832960.0000 - mae: 775563584.0000 - val_loss: 1427591466414243840.0000 - val_mae: 665280448.0000\n",
      "Epoch 332/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9048911977119744000.0000 - mae: 894762816.0000\n",
      "Epoch 332: val_loss improved from 1427591466414243840.00000 to 1424417176344854528.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch332-val_Loss1424417176344854528.000-mae773734976.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 7202426784049004544.0000 - mae: 773734976.0000 - val_loss: 1424417176344854528.0000 - val_mae: 663386432.0000\n",
      "Epoch 333/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7275670751133302784.0000 - mae: 790776448.0000\n",
      "Epoch 333: val_loss improved from 1424417176344854528.00000 to 1421334008301617152.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch333-val_Loss1421334008301617152.000-mae771883520.000.h5\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 7197332196921704448.0000 - mae: 771883520.0000 - val_loss: 1421334008301617152.0000 - val_mae: 661550400.0000\n",
      "Epoch 334/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12478007818932191232.0000 - mae: 885392064.0000\n",
      "Epoch 334: val_loss improved from 1421334008301617152.00000 to 1418171125665366016.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch334-val_Loss1418171125665366016.000-mae770117248.000.h5\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 7193102375689650176.0000 - mae: 770117248.0000 - val_loss: 1418171125665366016.0000 - val_mae: 659704512.0000\n",
      "Epoch 335/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19289385595981266944.0000 - mae: 1120114432.0000\n",
      "Epoch 335: val_loss improved from 1418171125665366016.00000 to 1415012915953532928.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch335-val_Loss1415012915953532928.000-mae768293504.000.h5\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 7188699381376221184.0000 - mae: 768293504.0000 - val_loss: 1415012915953532928.0000 - val_mae: 657954112.0000\n",
      "Epoch 336/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2356938688945979392.0000 - mae: 698227136.0000\n",
      "Epoch 336: val_loss improved from 1415012915953532928.00000 to 1411973316058546176.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch336-val_Loss1411973316058546176.000-mae766523072.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 7183367849493135360.0000 - mae: 766523072.0000 - val_loss: 1411973316058546176.0000 - val_mae: 656277184.0000\n",
      "Epoch 337/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17677011167475662848.0000 - mae: 1068190720.0000\n",
      "Epoch 337: val_loss improved from 1411973316058546176.00000 to 1408841906942640128.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch337-val_Loss1408841906942640128.000-mae764718464.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 7179340338400591872.0000 - mae: 764718464.0000 - val_loss: 1408841906942640128.0000 - val_mae: 654533760.0000\n",
      "Epoch 338/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 454233748360134656.0000 - mae: 546394624.0000\n",
      "Epoch 338: val_loss improved from 1408841906942640128.00000 to 1405835842152300544.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch338-val_Loss1405835842152300544.000-mae762954048.000.h5\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 7174062682587267072.0000 - mae: 762954048.0000 - val_loss: 1405835842152300544.0000 - val_mae: 652860096.0000\n",
      "Epoch 339/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2389320405895610368.0000 - mae: 663538048.0000\n",
      "Epoch 339: val_loss improved from 1405835842152300544.00000 to 1402720375954997248.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch339-val_Loss1402720375954997248.000-mae761262912.000.h5\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 7170136876320292864.0000 - mae: 761262912.0000 - val_loss: 1402720375954997248.0000 - val_mae: 651113792.0000\n",
      "Epoch 340/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9128234594238201856.0000 - mae: 889862976.0000\n",
      "Epoch 340: val_loss improved from 1402720375954997248.00000 to 1399652051318734848.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch340-val_Loss1399652051318734848.000-mae759419264.000.h5\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 7165493088960380928.0000 - mae: 759419264.0000 - val_loss: 1399652051318734848.0000 - val_mae: 649381248.0000\n",
      "Epoch 341/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8829103110052904960.0000 - mae: 846646144.0000\n",
      "Epoch 341: val_loss improved from 1399652051318734848.00000 to 1396641038726070272.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch341-val_Loss1396641038726070272.000-mae757495488.000.h5\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 7160665133402816512.0000 - mae: 757495488.0000 - val_loss: 1396641038726070272.0000 - val_mae: 647671616.0000\n",
      "Epoch 342/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8746564421423202304.0000 - mae: 804544640.0000\n",
      "Epoch 342: val_loss improved from 1396641038726070272.00000 to 1393616144799105024.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch342-val_Loss1393616144799105024.000-mae755731200.000.h5\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 7156113705019637760.0000 - mae: 755731200.0000 - val_loss: 1393616144799105024.0000 - val_mae: 645940096.0000\n",
      "Epoch 343/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7072909262099316736.0000 - mae: 745076096.0000\n",
      "Epoch 343: val_loss improved from 1393616144799105024.00000 to 1390605269645393920.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch343-val_Loss1390605269645393920.000-mae753855360.000.h5\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 7151444628892286976.0000 - mae: 753855360.0000 - val_loss: 1390605269645393920.0000 - val_mae: 644197888.0000\n",
      "Epoch 344/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2321560802810658816.0000 - mae: 654558336.0000\n",
      "Epoch 344: val_loss improved from 1390605269645393920.00000 to 1387545603663200256.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch344-val_Loss1387545603663200256.000-mae752137728.000.h5\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 7147205461811396608.0000 - mae: 752137728.0000 - val_loss: 1387545603663200256.0000 - val_mae: 642417856.0000\n",
      "Epoch 345/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 675363266330886144.0000 - mae: 571805824.0000\n",
      "Epoch 345: val_loss improved from 1387545603663200256.00000 to 1384552458134487040.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch345-val_Loss1384552458134487040.000-mae750238848.000.h5\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 7142522641788698624.0000 - mae: 750238848.0000 - val_loss: 1384552458134487040.0000 - val_mae: 640659008.0000\n",
      "Epoch 346/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12633150009123012608.0000 - mae: 948936384.0000\n",
      "Epoch 346: val_loss improved from 1384552458134487040.00000 to 1381560274678448128.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch346-val_Loss1381560274678448128.000-mae748447488.000.h5\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 7138176822079913984.0000 - mae: 748447488.0000 - val_loss: 1381560274678448128.0000 - val_mae: 638906432.0000\n",
      "Epoch 347/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12461217176864423936.0000 - mae: 900478208.0000\n",
      "Epoch 347: val_loss improved from 1381560274678448128.00000 to 1378591318405545984.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch347-val_Loss1378591318405545984.000-mae746604736.000.h5\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 7133773827766484992.0000 - mae: 746604736.0000 - val_loss: 1378591318405545984.0000 - val_mae: 637202304.0000\n",
      "Epoch 348/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12427274153403351040.0000 - mae: 903122176.0000\n",
      "Epoch 348: val_loss improved from 1378591318405545984.00000 to 1375673214545428480.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch348-val_Loss1375673214545428480.000-mae744925440.000.h5\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 7129083311162392576.0000 - mae: 744925440.0000 - val_loss: 1375673214545428480.0000 - val_mae: 635511296.0000\n",
      "Epoch 349/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10705140976655007744.0000 - mae: 775464704.0000\n",
      "Epoch 349: val_loss improved from 1375673214545428480.00000 to 1372800602978910208.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch349-val_Loss1372800602978910208.000-mae743122752.000.h5\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 7124526385221074944.0000 - mae: 743122752.0000 - val_loss: 1372800602978910208.0000 - val_mae: 633833152.0000\n",
      "Epoch 350/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7071932346018037760.0000 - mae: 724617216.0000\n",
      "Epoch 350: val_loss improved from 1372800602978910208.00000 to 1369955341764132864.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch350-val_Loss1369955341764132864.000-mae741501312.000.h5\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 7120046974849515520.0000 - mae: 741501312.0000 - val_loss: 1369955341764132864.0000 - val_mae: 632155776.0000\n",
      "Epoch 351/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12391623588384342016.0000 - mae: 865015616.0000\n",
      "Epoch 351: val_loss improved from 1369955341764132864.00000 to 1367050569482502144.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch351-val_Loss1367050569482502144.000-mae739729408.000.h5\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 7115977682315116544.0000 - mae: 739729408.0000 - val_loss: 1367050569482502144.0000 - val_mae: 630589824.0000\n",
      "Epoch 352/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2368030287369076736.0000 - mae: 653665536.0000\n",
      "Epoch 352: val_loss improved from 1367050569482502144.00000 to 1364221800942141440.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch352-val_Loss1364221800942141440.000-mae738141952.000.h5\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 7111194806734290944.0000 - mae: 738141952.0000 - val_loss: 1364221800942141440.0000 - val_mae: 629053056.0000\n",
      "Epoch 353/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 364106780231335936.0000 - mae: 479262944.0000\n",
      "Epoch 353: val_loss improved from 1364221800942141440.00000 to 1361500784541302784.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch353-val_Loss1361500784541302784.000-mae736475584.000.h5\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 7106216767839535104.0000 - mae: 736475584.0000 - val_loss: 1361500784541302784.0000 - val_mae: 627732800.0000\n",
      "Epoch 354/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8962122576537059328.0000 - mae: 878610624.0000\n",
      "Epoch 354: val_loss improved from 1361500784541302784.00000 to 1358572372759674880.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch354-val_Loss1358572372759674880.000-mae734835648.000.h5\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 7103025435339915264.0000 - mae: 734835648.0000 - val_loss: 1358572372759674880.0000 - val_mae: 626304640.0000\n",
      "Epoch 355/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12685144814488911872.0000 - mae: 921923136.0000\n",
      "Epoch 355: val_loss improved from 1358572372759674880.00000 to 1355693851318157312.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch355-val_Loss1355693851318157312.000-mae733057600.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 7098541626921844736.0000 - mae: 733057600.0000 - val_loss: 1355693851318157312.0000 - val_mae: 624888960.0000\n",
      "Epoch 356/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 500284250268893184.0000 - mae: 511914272.0000\n",
      "Epoch 356: val_loss improved from 1355693851318157312.00000 to 1352990976859176960.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch356-val_Loss1352990976859176960.000-mae731413696.000.h5\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 7093258473550381056.0000 - mae: 731413696.0000 - val_loss: 1352990976859176960.0000 - val_mae: 623544256.0000\n",
      "Epoch 357/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 277822433442922496.0000 - mae: 427035072.0000\n",
      "Epoch 357: val_loss improved from 1352990976859176960.00000 to 1350247695347875840.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch357-val_Loss1350247695347875840.000-mae729788288.000.h5\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 7089313425829920768.0000 - mae: 729788288.0000 - val_loss: 1350247695347875840.0000 - val_mae: 622225664.0000\n",
      "Epoch 358/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7166530478181187584.0000 - mae: 717568384.0000\n",
      "Epoch 358: val_loss improved from 1350247695347875840.00000 to 1347552242592382976.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch358-val_Loss1347552242592382976.000-mae728326912.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 7084876896411844608.0000 - mae: 728326912.0000 - val_loss: 1347552242592382976.0000 - val_mae: 620969216.0000\n",
      "Epoch 359/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 341144510677385216.0000 - mae: 450887232.0000\n",
      "Epoch 359: val_loss improved from 1347552242592382976.00000 to 1344865998246772736.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch359-val_Loss1344865998246772736.000-mae726760640.000.h5\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 7080373296784474112.0000 - mae: 726760640.0000 - val_loss: 1344865998246772736.0000 - val_mae: 619706880.0000\n",
      "Epoch 360/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19261556956682256384.0000 - mae: 1163778816.0000\n",
      "Epoch 360: val_loss improved from 1344865998246772736.00000 to 1342027746418622464.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch360-val_Loss1342027746418622464.000-mae725147456.000.h5\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 7077181964284854272.0000 - mae: 725147456.0000 - val_loss: 1342027746418622464.0000 - val_mae: 618366912.0000\n",
      "Epoch 361/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2056370692818993152.0000 - mae: 560934144.0000\n",
      "Epoch 361: val_loss improved from 1342027746418622464.00000 to 1339325971471269888.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch361-val_Loss1339325971471269888.000-mae723547840.000.h5\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 7072261099994742784.0000 - mae: 723547840.0000 - val_loss: 1339325971471269888.0000 - val_mae: 617079744.0000\n",
      "Epoch 362/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7253057095484833792.0000 - mae: 782177792.0000\n",
      "Epoch 362: val_loss improved from 1339325971471269888.00000 to 1336691816489025536.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch362-val_Loss1336691816489025536.000-mae722003712.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 7067827869111549952.0000 - mae: 722003712.0000 - val_loss: 1336691816489025536.0000 - val_mae: 615808448.0000\n",
      "Epoch 363/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8651620492853116928.0000 - mae: 792641920.0000\n",
      "Epoch 363: val_loss improved from 1336691816489025536.00000 to 1334034434323644416.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch363-val_Loss1334034434323644416.000-mae720422784.000.h5\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 7063839390681792512.0000 - mae: 720422784.0000 - val_loss: 1334034434323644416.0000 - val_mae: 614517248.0000\n",
      "Epoch 364/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17242112537248399360.0000 - mae: 942899520.0000\n",
      "Epoch 364: val_loss improved from 1334034434323644416.00000 to 1331367293992566784.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch364-val_Loss1331367293992566784.000-mae719012800.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 7059840466891571200.0000 - mae: 719012800.0000 - val_loss: 1331367293992566784.0000 - val_mae: 613258048.0000\n",
      "Epoch 365/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 557481051304230912.0000 - mae: 505551872.0000\n",
      "Epoch 365: val_loss improved from 1331367293992566784.00000 to 1328851336510308352.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch365-val_Loss1328851336510308352.000-mae717642112.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 7054827793380540416.0000 - mae: 717642112.0000 - val_loss: 1328851336510308352.0000 - val_mae: 612060096.0000\n",
      "Epoch 366/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7010352548037001216.0000 - mae: 697368256.0000\n",
      "Epoch 366: val_loss improved from 1328851336510308352.00000 to 1326292772952473600.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch366-val_Loss1326292772952473600.000-mae716142656.000.h5\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 7051079008485638144.0000 - mae: 716142656.0000 - val_loss: 1326292772952473600.0000 - val_mae: 610833344.0000\n",
      "Epoch 367/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17543019182956740608.0000 - mae: 1013326336.0000\n",
      "Epoch 367: val_loss improved from 1326292772952473600.00000 to 1323691328441155584.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch367-val_Loss1323691328441155584.000-mae714826880.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 7047298887509344256.0000 - mae: 714826880.0000 - val_loss: 1323691328441155584.0000 - val_mae: 609607488.0000\n",
      "Epoch 368/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 474542896356261888.0000 - mae: 531734112.0000\n",
      "Epoch 368: val_loss improved from 1323691328441155584.00000 to 1321198460703080448.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch368-val_Loss1321198460703080448.000-mae713603648.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 7042585281161068544.0000 - mae: 713603648.0000 - val_loss: 1321198460703080448.0000 - val_mae: 608454592.0000\n",
      "Epoch 369/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 289135033703202816.0000 - mae: 440547968.0000\n",
      "Epoch 369: val_loss improved from 1321198460703080448.00000 to 1318746274895233024.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch369-val_Loss1318746274895233024.000-mae712202112.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 7038228466336006144.0000 - mae: 712202112.0000 - val_loss: 1318746274895233024.0000 - val_mae: 607306624.0000\n",
      "Epoch 370/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 486251526960971776.0000 - mae: 509521472.0000\n",
      "Epoch 370: val_loss improved from 1318746274895233024.00000 to 1316233341069950976.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch370-val_Loss1316233341069950976.000-mae710975360.000.h5\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 7034579736999231488.0000 - mae: 710975360.0000 - val_loss: 1316233341069950976.0000 - val_mae: 606118528.0000\n",
      "Epoch 371/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 657087733809807360.0000 - mae: 511242464.0000\n",
      "Epoch 371: val_loss improved from 1316233341069950976.00000 to 1313804107567333376.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch371-val_Loss1313804107567333376.000-mae709615872.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 7030030507639308288.0000 - mae: 709615872.0000 - val_loss: 1313804107567333376.0000 - val_mae: 605008512.0000\n",
      "Epoch 372/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 497210118836846592.0000 - mae: 507926080.0000\n",
      "Epoch 372: val_loss improved from 1313804107567333376.00000 to 1311272207166472192.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch372-val_Loss1311272207166472192.000-mae708356352.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 7026697887895519232.0000 - mae: 708356352.0000 - val_loss: 1311272207166472192.0000 - val_mae: 603908800.0000\n",
      "Epoch 373/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10871812645774688256.0000 - mae: 782589632.0000\n",
      "Epoch 373: val_loss improved from 1311272207166472192.00000 to 1308772742358630400.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch373-val_Loss1308772742358630400.000-mae707104000.000.h5\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 7022703911907622912.0000 - mae: 707104000.0000 - val_loss: 1308772742358630400.0000 - val_mae: 602805376.0000\n",
      "Epoch 374/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7019957881617252352.0000 - mae: 718551680.0000\n",
      "Epoch 374: val_loss improved from 1308772742358630400.00000 to 1306342409344385024.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch374-val_Loss1306342409344385024.000-mae705826176.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 7018659358384848896.0000 - mae: 705826176.0000 - val_loss: 1306342409344385024.0000 - val_mae: 601728384.0000\n",
      "Epoch 375/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 335755082635149312.0000 - mae: 459715072.0000\n",
      "Epoch 375: val_loss improved from 1306342409344385024.00000 to 1304043605408612352.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch375-val_Loss1304043605408612352.000-mae704520192.000.h5\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 7013993580792381440.0000 - mae: 704520192.0000 - val_loss: 1304043605408612352.0000 - val_mae: 600698880.0000\n",
      "Epoch 376/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17204941347648176128.0000 - mae: 953813760.0000\n",
      "Epoch 376: val_loss improved from 1304043605408612352.00000 to 1301680067725754368.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch376-val_Loss1301680067725754368.000-mae703435008.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 7010674704943939584.0000 - mae: 703435008.0000 - val_loss: 1301680067725754368.0000 - val_mae: 599630144.0000\n",
      "Epoch 377/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 347853249593737216.0000 - mae: 461832320.0000\n",
      "Epoch 377: val_loss improved from 1301680067725754368.00000 to 1299414524016721920.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch377-val_Loss1299414524016721920.000-mae702236224.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 7006228279921213440.0000 - mae: 702236224.0000 - val_loss: 1299414524016721920.0000 - val_mae: 598600832.0000\n",
      "Epoch 378/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10523452177723162624.0000 - mae: 705907072.0000\n",
      "Epoch 378: val_loss improved from 1299414524016721920.00000 to 1297088232290254848.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch378-val_Loss1297088232290254848.000-mae701244928.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 7002704345154191360.0000 - mae: 701244928.0000 - val_loss: 1297088232290254848.0000 - val_mae: 597529920.0000\n",
      "Epoch 379/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10840100531406372864.0000 - mae: 775872512.0000\n",
      "Epoch 379: val_loss improved from 1297088232290254848.00000 to 1294796437741109248.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch379-val_Loss1294796437741109248.000-mae699951360.000.h5\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 6998799979363958784.0000 - mae: 699951360.0000 - val_loss: 1294796437741109248.0000 - val_mae: 596474880.0000\n",
      "Epoch 380/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2234532945643700224.0000 - mae: 618449088.0000\n",
      "Epoch 380: val_loss improved from 1294796437741109248.00000 to 1292590954854744064.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch380-val_Loss1292590954854744064.000-mae698976256.000.h5\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 6994549267410976768.0000 - mae: 698976256.0000 - val_loss: 1292590954854744064.0000 - val_mae: 595491072.0000\n",
      "Epoch 381/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 489450178084864000.0000 - mae: 468437568.0000\n",
      "Epoch 381: val_loss improved from 1292590954854744064.00000 to 1290364168930590720.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch381-val_Loss1290364168930590720.000-mae697971136.000.h5\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 6990831268841652224.0000 - mae: 697971136.0000 - val_loss: 1290364168930590720.0000 - val_mae: 594479808.0000\n",
      "Epoch 382/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10591226074459275264.0000 - mae: 740548480.0000\n",
      "Epoch 382: val_loss improved from 1290364168930590720.00000 to 1288117454358183936.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch382-val_Loss1288117454358183936.000-mae697064896.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 6987455218388566016.0000 - mae: 697064896.0000 - val_loss: 1288117454358183936.0000 - val_mae: 593471040.0000\n",
      "Epoch 383/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 623626227722223616.0000 - mae: 537684608.0000\n",
      "Epoch 383: val_loss improved from 1288117454358183936.00000 to 1285936573044490240.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch383-val_Loss1285936573044490240.000-mae696110272.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 6983505222865780736.0000 - mae: 696110272.0000 - val_loss: 1285936573044490240.0000 - val_mae: 592533248.0000\n",
      "Epoch 384/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 483450967765811200.0000 - mae: 479082784.0000\n",
      "Epoch 384: val_loss improved from 1285936573044490240.00000 to 1283878974472060928.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch384-val_Loss1283878974472060928.000-mae695200640.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 6979162151936065536.0000 - mae: 695200640.0000 - val_loss: 1283878974472060928.0000 - val_mae: 591654720.0000\n",
      "Epoch 385/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10925266503070646272.0000 - mae: 798892352.0000\n",
      "Epoch 385: val_loss improved from 1283878974472060928.00000 to 1281683662068252672.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch385-val_Loss1281683662068252672.000-mae694334848.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 6976359496796864512.0000 - mae: 694334848.0000 - val_loss: 1281683662068252672.0000 - val_mae: 590819008.0000\n",
      "Epoch 386/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 723428623297544192.0000 - mae: 543744000.0000\n",
      "Epoch 386: val_loss improved from 1281683662068252672.00000 to 1279641044341751808.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch386-val_Loss1279641044341751808.000-mae693386560.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 6971886133739257856.0000 - mae: 693386560.0000 - val_loss: 1279641044341751808.0000 - val_mae: 590053120.0000\n",
      "Epoch 387/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12474045179025686528.0000 - mae: 888544960.0000\n",
      "Epoch 387: val_loss improved from 1279641044341751808.00000 to 1277489574964101120.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch387-val_Loss1277489574964101120.000-mae692508736.000.h5\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 6969038398623318016.0000 - mae: 692508736.0000 - val_loss: 1277489574964101120.0000 - val_mae: 589305408.0000\n",
      "Epoch 388/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10544505626371817472.0000 - mae: 722877184.0000\n",
      "Epoch 388: val_loss improved from 1277489574964101120.00000 to 1275464686862598144.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch388-val_Loss1275464686862598144.000-mae691657728.000.h5\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 6964730512065691648.0000 - mae: 691657728.0000 - val_loss: 1275464686862598144.0000 - val_mae: 588593024.0000\n",
      "Epoch 389/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10744977382440960000.0000 - mae: 783147136.0000\n",
      "Epoch 389: val_loss improved from 1275464686862598144.00000 to 1273448869732024320.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch389-val_Loss1273448869732024320.000-mae690773888.000.h5\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 6961351712833536000.0000 - mae: 690773888.0000 - val_loss: 1273448869732024320.0000 - val_mae: 587881472.0000\n",
      "Epoch 390/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 472524845842694144.0000 - mae: 476644288.0000\n",
      "Epoch 390: val_loss improved from 1273448869732024320.00000 to 1271510018415394816.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch390-val_Loss1271510018415394816.000-mae690110976.000.h5\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 6957396769508425728.0000 - mae: 690110976.0000 - val_loss: 1271510018415394816.0000 - val_mae: 587185088.0000\n",
      "Epoch 391/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 483525150440947712.0000 - mae: 472715968.0000\n",
      "Epoch 391: val_loss improved from 1271510018415394816.00000 to 1269604564764459008.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch391-val_Loss1269604564764459008.000-mae689315392.000.h5\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 6953633141206548480.0000 - mae: 689315392.0000 - val_loss: 1269604564764459008.0000 - val_mae: 586501248.0000\n",
      "Epoch 392/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 468122023327956992.0000 - mae: 468849152.0000\n",
      "Epoch 392: val_loss improved from 1269604564764459008.00000 to 1267672722834456576.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch392-val_Loss1267672722834456576.000-mae688676864.000.h5\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 6950383534590656512.0000 - mae: 688676864.0000 - val_loss: 1267672722834456576.0000 - val_mae: 585804544.0000\n",
      "Epoch 393/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7091773033341255680.0000 - mae: 716256960.0000\n",
      "Epoch 393: val_loss improved from 1267672722834456576.00000 to 1265730985299804160.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch393-val_Loss1265730985299804160.000-mae687839232.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 6946864547625959424.0000 - mae: 687839232.0000 - val_loss: 1265730985299804160.0000 - val_mae: 585078400.0000\n",
      "Epoch 394/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1908040801795964928.0000 - mae: 527341824.0000\n",
      "Epoch 394: val_loss improved from 1265730985299804160.00000 to 1263770830945386496.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch394-val_Loss1263770830945386496.000-mae687071232.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 6943575908347281408.0000 - mae: 687071232.0000 - val_loss: 1263770830945386496.0000 - val_mae: 584350208.0000\n",
      "Epoch 395/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17414553343879020544.0000 - mae: 1035503424.0000\n",
      "Epoch 395: val_loss improved from 1263770830945386496.00000 to 1261800231230504960.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch395-val_Loss1261800231230504960.000-mae686359552.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 6940547853324386304.0000 - mae: 686359552.0000 - val_loss: 1261800231230504960.0000 - val_mae: 583658944.0000\n",
      "Epoch 396/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 316930687653380096.0000 - mae: 462423360.0000\n",
      "Epoch 396: val_loss improved from 1261800231230504960.00000 to 1260035515067924480.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch396-val_Loss1260035515067924480.000-mae685497728.000.h5\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 6935802361138905088.0000 - mae: 685497728.0000 - val_loss: 1260035515067924480.0000 - val_mae: 583216832.0000\n",
      "Epoch 397/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6886473321714548736.0000 - mae: 683070528.0000\n",
      "Epoch 397: val_loss improved from 1260035515067924480.00000 to 1258206889791979520.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch397-val_Loss1258206889791979520.000-mae684814592.000.h5\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 6932906247511343104.0000 - mae: 684814592.0000 - val_loss: 1258206889791979520.0000 - val_mae: 582831040.0000\n",
      "Epoch 398/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7001102906468335616.0000 - mae: 697427840.0000\n",
      "Epoch 398: val_loss improved from 1258206889791979520.00000 to 1256367544277663744.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch398-val_Loss1256367544277663744.000-mae684043712.000.h5\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 6929581324348948480.0000 - mae: 684043712.0000 - val_loss: 1256367544277663744.0000 - val_mae: 582483136.0000\n",
      "Epoch 399/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6909849488676880384.0000 - mae: 681784192.0000\n",
      "Epoch 399: val_loss improved from 1256367544277663744.00000 to 1254572866423226368.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch399-val_Loss1254572866423226368.000-mae683375040.000.h5\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 6926012309605187584.0000 - mae: 683375040.0000 - val_loss: 1254572866423226368.0000 - val_mae: 582157248.0000\n",
      "Epoch 400/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18829558838129065984.0000 - mae: 1075604096.0000\n",
      "Epoch 400: val_loss improved from 1254572866423226368.00000 to 1252684730080428032.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch400-val_Loss1252684730080428032.000-mae682664064.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 6923543356245016576.0000 - mae: 682664064.0000 - val_loss: 1252684730080428032.0000 - val_mae: 581814848.0000\n",
      "Epoch 401/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12262837791422939136.0000 - mae: 845537792.0000\n",
      "Epoch 401: val_loss improved from 1252684730080428032.00000 to 1250913966603894784.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch401-val_Loss1250913966603894784.000-mae681788032.000.h5\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 6919533987094331392.0000 - mae: 681788032.0000 - val_loss: 1250913966603894784.0000 - val_mae: 581484096.0000\n",
      "Epoch 402/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6885610754842558464.0000 - mae: 682446976.0000\n",
      "Epoch 402: val_loss improved from 1250913966603894784.00000 to 1249235562104094720.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch402-val_Loss1249235562104094720.000-mae681133504.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 6915845675338956800.0000 - mae: 681133504.0000 - val_loss: 1249235562104094720.0000 - val_mae: 581278592.0000\n",
      "Epoch 403/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10484064372681342976.0000 - mae: 703387904.0000\n",
      "Epoch 403: val_loss improved from 1249235562104094720.00000 to 1247544925537435648.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch403-val_Loss1247544925537435648.000-mae680450240.000.h5\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 6912609262862598144.0000 - mae: 680450240.0000 - val_loss: 1247544925537435648.0000 - val_mae: 581110720.0000\n",
      "Epoch 404/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 346029194162995200.0000 - mae: 456404544.0000\n",
      "Epoch 404: val_loss improved from 1247544925537435648.00000 to 1245898681752748032.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch404-val_Loss1245898681752748032.000-mae679761216.000.h5\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 6909379447456006144.0000 - mae: 679761216.0000 - val_loss: 1245898681752748032.0000 - val_mae: 580947200.0000\n",
      "Epoch 405/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 487248234251550720.0000 - mae: 477154560.0000\n",
      "Epoch 405: val_loss improved from 1245898681752748032.00000 to 1244268518325616640.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch405-val_Loss1244268518325616640.000-mae679084608.000.h5\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 6906137537421508608.0000 - mae: 679084608.0000 - val_loss: 1244268518325616640.0000 - val_mae: 580776128.0000\n",
      "Epoch 406/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6886158311633190912.0000 - mae: 644450752.0000\n",
      "Epoch 406: val_loss improved from 1244268518325616640.00000 to 1242678487072899072.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch406-val_Loss1242678487072899072.000-mae678374848.000.h5\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 6902944555654447104.0000 - mae: 678374848.0000 - val_loss: 1242678487072899072.0000 - val_mae: 580616000.0000\n",
      "Epoch 407/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17133121248121847808.0000 - mae: 978842944.0000\n",
      "Epoch 407: val_loss improved from 1242678487072899072.00000 to 1241013826468446208.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch407-val_Loss1241013826468446208.000-mae677804480.000.h5\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 6900358504305917952.0000 - mae: 677804480.0000 - val_loss: 1241013826468446208.0000 - val_mae: 580434944.0000\n",
      "Epoch 408/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10789842954412359680.0000 - mae: 778235904.0000\n",
      "Epoch 408: val_loss improved from 1241013826468446208.00000 to 1239424482410496000.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch408-val_Loss1239424482410496000.000-mae677139904.000.h5\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 6896823024666804224.0000 - mae: 677139904.0000 - val_loss: 1239424482410496000.0000 - val_mae: 580256000.0000\n",
      "Epoch 409/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8518602675636404224.0000 - mae: 794173824.0000\n",
      "Epoch 409: val_loss improved from 1239424482410496000.00000 to 1237875545404866560.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch409-val_Loss1237875545404866560.000-mae676530496.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 6893757036492750848.0000 - mae: 676530496.0000 - val_loss: 1237875545404866560.0000 - val_mae: 580087168.0000\n",
      "Epoch 410/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10478441470216896512.0000 - mae: 707255168.0000\n",
      "Epoch 410: val_loss improved from 1237875545404866560.00000 to 1236380896785858560.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch410-val_Loss1236380896785858560.000-mae675837632.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 6890171529074573312.0000 - mae: 675837632.0000 - val_loss: 1236380896785858560.0000 - val_mae: 579907712.0000\n",
      "Epoch 411/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2240577648256352256.0000 - mae: 619047808.0000\n",
      "Epoch 411: val_loss improved from 1236380896785858560.00000 to 1234875665367433216.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch411-val_Loss1234875665367433216.000-mae675321664.000.h5\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 6887233084249341952.0000 - mae: 675321664.0000 - val_loss: 1234875665367433216.0000 - val_mae: 579735744.0000\n",
      "Epoch 412/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 390819483947630592.0000 - mae: 449216640.0000\n",
      "Epoch 412: val_loss improved from 1234875665367433216.00000 to 1233411253318189056.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch412-val_Loss1233411253318189056.000-mae674773632.000.h5\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 6883957639110197248.0000 - mae: 674773632.0000 - val_loss: 1233411253318189056.0000 - val_mae: 579554496.0000\n",
      "Epoch 413/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12172406258573246464.0000 - mae: 853330304.0000\n",
      "Epoch 413: val_loss improved from 1233411253318189056.00000 to 1231884443984068608.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch413-val_Loss1231884443984068608.000-mae674154816.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 6881441406750031872.0000 - mae: 674154816.0000 - val_loss: 1231884443984068608.0000 - val_mae: 579373184.0000\n",
      "Epoch 414/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 426667411303825408.0000 - mae: 473826688.0000\n",
      "Epoch 414: val_loss improved from 1231884443984068608.00000 to 1230443533995868160.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch414-val_Loss1230443533995868160.000-mae673670080.000.h5\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 6878012579738812416.0000 - mae: 673670080.0000 - val_loss: 1230443533995868160.0000 - val_mae: 579187776.0000\n",
      "Epoch 415/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 465461617505599488.0000 - mae: 474103008.0000\n",
      "Epoch 415: val_loss improved from 1230443533995868160.00000 to 1229049628129755136.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch415-val_Loss1229049628129755136.000-mae673011840.000.h5\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 6874721741436878848.0000 - mae: 673011840.0000 - val_loss: 1229049628129755136.0000 - val_mae: 579008192.0000\n",
      "Epoch 416/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 565324967256784896.0000 - mae: 469449792.0000\n",
      "Epoch 416: val_loss improved from 1229049628129755136.00000 to 1227622187158994944.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch416-val_Loss1227622187158994944.000-mae672485184.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 6871947123844186112.0000 - mae: 672485184.0000 - val_loss: 1227622187158994944.0000 - val_mae: 578872064.0000\n",
      "Epoch 417/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12226960727008608256.0000 - mae: 858743296.0000\n",
      "Epoch 417: val_loss improved from 1227622187158994944.00000 to 1226144855848124416.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch417-val_Loss1226144855848124416.000-mae671808256.000.h5\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 6869505108518895616.0000 - mae: 671808256.0000 - val_loss: 1226144855848124416.0000 - val_mae: 578746752.0000\n",
      "Epoch 418/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8503510229277736960.0000 - mae: 773307904.0000\n",
      "Epoch 418: val_loss improved from 1226144855848124416.00000 to 1224767717534334976.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch418-val_Loss1224767717534334976.000-mae671297728.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 6866121361484414976.0000 - mae: 671297728.0000 - val_loss: 1224767717534334976.0000 - val_mae: 578632512.0000\n",
      "Epoch 419/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 446596849831247872.0000 - mae: 481430784.0000\n",
      "Epoch 419: val_loss improved from 1224767717534334976.00000 to 1223447204069376000.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch419-val_Loss1223447204069376000.000-mae670742976.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6862543550647631872.0000 - mae: 670742976.0000 - val_loss: 1223447204069376000.0000 - val_mae: 578504064.0000\n",
      "Epoch 420/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6886444734412226560.0000 - mae: 679812224.0000\n",
      "Epoch 420: val_loss improved from 1223447204069376000.00000 to 1222056184421285888.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch420-val_Loss1222056184421285888.000-mae670306112.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6860206538682793984.0000 - mae: 670306112.0000 - val_loss: 1222056184421285888.0000 - val_mae: 578368960.0000\n",
      "Epoch 421/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10629088856873369600.0000 - mae: 750821760.0000\n",
      "Epoch 421: val_loss improved from 1222056184421285888.00000 to 1220687979639472128.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch421-val_Loss1220687979639472128.000-mae669598528.000.h5\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 6857006410090151936.0000 - mae: 669598528.0000 - val_loss: 1220687979639472128.0000 - val_mae: 578223616.0000\n",
      "Epoch 422/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1962460854934831104.0000 - mae: 565650304.0000\n",
      "Epoch 422: val_loss improved from 1220687979639472128.00000 to 1219350973500096512.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch422-val_Loss1219350973500096512.000-mae669066432.000.h5\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 6854187812032348160.0000 - mae: 669066432.0000 - val_loss: 1219350973500096512.0000 - val_mae: 578150848.0000\n",
      "Epoch 423/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 492423017008201728.0000 - mae: 509691072.0000\n",
      "Epoch 423: val_loss improved from 1219350973500096512.00000 to 1218051075878158336.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch423-val_Loss1218051075878158336.000-mae668541952.000.h5\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 6851020119032725504.0000 - mae: 668541952.0000 - val_loss: 1218051075878158336.0000 - val_mae: 578085312.0000\n",
      "Epoch 424/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 298953088423690240.0000 - mae: 383194496.0000\n",
      "Epoch 424: val_loss improved from 1218051075878158336.00000 to 1216824845535281152.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch424-val_Loss1216824845535281152.000-mae667932928.000.h5\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 6847787554847064064.0000 - mae: 667932928.0000 - val_loss: 1216824845535281152.0000 - val_mae: 578020544.0000\n",
      "Epoch 425/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 351768782298939392.0000 - mae: 425525952.0000\n",
      "Epoch 425: val_loss improved from 1216824845535281152.00000 to 1215541990343573504.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch425-val_Loss1215541990343573504.000-mae667433408.000.h5\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 6845437898498506752.0000 - mae: 667433408.0000 - val_loss: 1215541990343573504.0000 - val_mae: 577959040.0000\n",
      "Epoch 426/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 395264672019775488.0000 - mae: 432412960.0000\n",
      "Epoch 426: val_loss improved from 1215541990343573504.00000 to 1214267381489074176.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch426-val_Loss1214267381489074176.000-mae666889216.000.h5\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 6842552779987222528.0000 - mae: 666889216.0000 - val_loss: 1214267381489074176.0000 - val_mae: 577885632.0000\n",
      "Epoch 427/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12275584429723746304.0000 - mae: 863082880.0000\n",
      "Epoch 427: val_loss improved from 1214267381489074176.00000 to 1212961986308997120.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch427-val_Loss1212961986308997120.000-mae666365184.000.h5\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 6840333415766556672.0000 - mae: 666365184.0000 - val_loss: 1212961986308997120.0000 - val_mae: 577837440.0000\n",
      "Epoch 428/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8580741025524350976.0000 - mae: 805080320.0000\n",
      "Epoch 428: val_loss improved from 1212961986308997120.00000 to 1211729571213213696.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch428-val_Loss1211729571213213696.000-mae665906112.000.h5\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 6837350440720400384.0000 - mae: 665906112.0000 - val_loss: 1211729571213213696.0000 - val_mae: 577816128.0000\n",
      "Epoch 429/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 393001808370335744.0000 - mae: 440799584.0000\n",
      "Epoch 429: val_loss improved from 1211729571213213696.00000 to 1210574396809281536.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch429-val_Loss1210574396809281536.000-mae665498240.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 6834148113104502784.0000 - mae: 665498240.0000 - val_loss: 1210574396809281536.0000 - val_mae: 577777600.0000\n",
      "Epoch 430/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6820332749501497344.0000 - mae: 665577792.0000\n",
      "Epoch 430: val_loss improved from 1210574396809281536.00000 to 1209443136783253504.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch430-val_Loss1209443136783253504.000-mae665147392.000.h5\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 6831478498872262656.0000 - mae: 665147392.0000 - val_loss: 1209443136783253504.0000 - val_mae: 577743744.0000\n",
      "Epoch 431/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16929635730680971264.0000 - mae: 940758528.0000\n",
      "Epoch 431: val_loss improved from 1209443136783253504.00000 to 1208273256411299840.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch431-val_Loss1208273256411299840.000-mae664743616.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 6829163477139980288.0000 - mae: 664743616.0000 - val_loss: 1208273256411299840.0000 - val_mae: 577685248.0000\n",
      "Epoch 432/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12029545613264683008.0000 - mae: 821395072.0000\n",
      "Epoch 432: val_loss improved from 1208273256411299840.00000 to 1207135124437598208.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch432-val_Loss1207135124437598208.000-mae664328256.000.h5\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 6826492213640298496.0000 - mae: 664328256.0000 - val_loss: 1207135124437598208.0000 - val_mae: 577643456.0000\n",
      "Epoch 433/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8596680095836405760.0000 - mae: 771940352.0000\n",
      "Epoch 433: val_loss improved from 1207135124437598208.00000 to 1206060076943540224.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch433-val_Loss1206060076943540224.000-mae663970944.000.h5\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 6823686809722028032.0000 - mae: 663970944.0000 - val_loss: 1206060076943540224.0000 - val_mae: 577611264.0000\n",
      "Epoch 434/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16895020905615327232.0000 - mae: 928671872.0000\n",
      "Epoch 434: val_loss improved from 1206060076943540224.00000 to 1204970735798321152.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch434-val_Loss1204970735798321152.000-mae663541568.000.h5\n",
      "3/3 [==============================] - 0s 172ms/step - loss: 6821141990059540480.0000 - mae: 663541568.0000 - val_loss: 1204970735798321152.0000 - val_mae: 577535552.0000\n",
      "Epoch 435/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18639631398570295296.0000 - mae: 1060719488.0000\n",
      "Epoch 435: val_loss improved from 1204970735798321152.00000 to 1203885242943799296.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch435-val_Loss1203885242943799296.000-mae663083840.000.h5\n",
      "3/3 [==============================] - 0s 175ms/step - loss: 6818837413687721984.0000 - mae: 663083840.0000 - val_loss: 1203885242943799296.0000 - val_mae: 577480960.0000\n",
      "Epoch 436/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12039612741728600064.0000 - mae: 831229760.0000\n",
      "Epoch 436: val_loss improved from 1203885242943799296.00000 to 1202840157141598208.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch436-val_Loss1202840157141598208.000-mae662740160.000.h5\n",
      "3/3 [==============================] - 0s 174ms/step - loss: 6816001223443873792.0000 - mae: 662740160.0000 - val_loss: 1202840157141598208.0000 - val_mae: 577425024.0000\n",
      "Epoch 437/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10433574798733869056.0000 - mae: 710102080.0000\n",
      "Epoch 437: val_loss improved from 1202840157141598208.00000 to 1201825582787067904.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch437-val_Loss1201825582787067904.000-mae662379136.000.h5\n",
      "3/3 [==============================] - 0s 219ms/step - loss: 6813270586316292096.0000 - mae: 662379136.0000 - val_loss: 1201825582787067904.0000 - val_mae: 577357184.0000\n",
      "Epoch 438/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12079881255584268288.0000 - mae: 813278656.0000\n",
      "Epoch 438: val_loss improved from 1201825582787067904.00000 to 1200812107944165376.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch438-val_Loss1200812107944165376.000-mae661950720.000.h5\n",
      "3/3 [==============================] - 1s 324ms/step - loss: 6810989649444470784.0000 - mae: 661950720.0000 - val_loss: 1200812107944165376.0000 - val_mae: 577295232.0000\n",
      "Epoch 439/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6764811260589506560.0000 - mae: 646430592.0000\n",
      "Epoch 439: val_loss improved from 1200812107944165376.00000 to 1199841651493699584.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch439-val_Loss1199841651493699584.000-mae661600768.000.h5\n",
      "3/3 [==============================] - 0s 204ms/step - loss: 6808425038572683264.0000 - mae: 661600768.0000 - val_loss: 1199841651493699584.0000 - val_mae: 577246592.0000\n",
      "Epoch 440/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10561505175648862208.0000 - mae: 732568960.0000\n",
      "Epoch 440: val_loss improved from 1199841651493699584.00000 to 1198879441380442112.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch440-val_Loss1198879441380442112.000-mae661253824.000.h5\n",
      "3/3 [==============================] - 0s 155ms/step - loss: 6805841736003223552.0000 - mae: 661253824.0000 - val_loss: 1198879441380442112.0000 - val_mae: 577225152.0000\n",
      "Epoch 441/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7032822167662231552.0000 - mae: 708106112.0000\n",
      "Epoch 441: val_loss improved from 1198879441380442112.00000 to 1197956951124738048.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch441-val_Loss1197956951124738048.000-mae660856384.000.h5\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 6803307361701199872.0000 - mae: 660856384.0000 - val_loss: 1197956951124738048.0000 - val_mae: 577201664.0000\n",
      "Epoch 442/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10653075802544930816.0000 - mae: 754819840.0000\n",
      "Epoch 442: val_loss improved from 1197956951124738048.00000 to 1196987044430086144.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch442-val_Loss1196987044430086144.000-mae660528064.000.h5\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 6800976946806128640.0000 - mae: 660528064.0000 - val_loss: 1196987044430086144.0000 - val_mae: 577167424.0000\n",
      "Epoch 443/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 326159129263472640.0000 - mae: 425409152.0000\n",
      "Epoch 443: val_loss improved from 1196987044430086144.00000 to 1196080222215077888.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch443-val_Loss1196080222215077888.000-mae660073280.000.h5\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 6798155599969255424.0000 - mae: 660073280.0000 - val_loss: 1196080222215077888.0000 - val_mae: 577127232.0000\n",
      "Epoch 444/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6995728493631766528.0000 - mae: 717391104.0000\n",
      "Epoch 444: val_loss improved from 1196080222215077888.00000 to 1195110040642519040.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch444-val_Loss1195110040642519040.000-mae659728000.000.h5\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 6796538218364796928.0000 - mae: 659728000.0000 - val_loss: 1195110040642519040.0000 - val_mae: 577115840.0000\n",
      "Epoch 445/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1918951805434200064.0000 - mae: 542189440.0000\n",
      "Epoch 445: val_loss improved from 1195110040642519040.00000 to 1194198408064139264.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch445-val_Loss1194198408064139264.000-mae659379392.000.h5\n",
      "3/3 [==============================] - 0s 161ms/step - loss: 6793554693562826752.0000 - mae: 659379392.0000 - val_loss: 1194198408064139264.0000 - val_mae: 577101824.0000\n",
      "Epoch 446/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 507510515564544000.0000 - mae: 481146176.0000\n",
      "Epoch 446: val_loss improved from 1194198408064139264.00000 to 1193281140488667136.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch446-val_Loss1193281140488667136.000-mae659092544.000.h5\n",
      "3/3 [==============================] - 0s 169ms/step - loss: 6791389205411921920.0000 - mae: 659092544.0000 - val_loss: 1193281140488667136.0000 - val_mae: 577094656.0000\n",
      "Epoch 447/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6895162212353048576.0000 - mae: 628588416.0000\n",
      "Epoch 447: val_loss improved from 1193281140488667136.00000 to 1192401668625399808.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch447-val_Loss1192401668625399808.000-mae658749056.000.h5\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 6788930697412214784.0000 - mae: 658749056.0000 - val_loss: 1192401668625399808.0000 - val_mae: 577084608.0000\n",
      "Epoch 448/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6812578443746607104.0000 - mae: 638522880.0000\n",
      "Epoch 448: val_loss improved from 1192401668625399808.00000 to 1191506391282483200.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch448-val_Loss1191506391282483200.000-mae658337216.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6786568946435751936.0000 - mae: 658337216.0000 - val_loss: 1191506391282483200.0000 - val_mae: 577042176.0000\n",
      "Epoch 449/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6847746872916836352.0000 - mae: 695594176.0000\n",
      "Epoch 449: val_loss improved from 1191506391282483200.00000 to 1190643274654679040.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch449-val_Loss1190643274654679040.000-mae657974784.000.h5\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 6784067007726747648.0000 - mae: 657974784.0000 - val_loss: 1190643274654679040.0000 - val_mae: 577008768.0000\n",
      "Epoch 450/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 474373915162968064.0000 - mae: 455416384.0000\n",
      "Epoch 450: val_loss improved from 1190643274654679040.00000 to 1189764764864086016.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch450-val_Loss1189764764864086016.000-mae657675456.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 6781679967982845952.0000 - mae: 657675456.0000 - val_loss: 1189764764864086016.0000 - val_mae: 576966208.0000\n",
      "Epoch 451/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10418813855130976256.0000 - mae: 696156160.0000\n",
      "Epoch 451: val_loss improved from 1189764764864086016.00000 to 1188885293000818688.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch451-val_Loss1188885293000818688.000-mae657335040.000.h5\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 6779401779890094080.0000 - mae: 657335040.0000 - val_loss: 1188885293000818688.0000 - val_mae: 576925696.0000\n",
      "Epoch 452/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2022239377991663616.0000 - mae: 574981248.0000\n",
      "Epoch 452: val_loss improved from 1188885293000818688.00000 to 1188045403556151296.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch452-val_Loss1188045403556151296.000-mae656975360.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 6776961963588059136.0000 - mae: 656975360.0000 - val_loss: 1188045403556151296.0000 - val_mae: 576893824.0000\n",
      "Epoch 453/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 240338175086559232.0000 - mae: 397293824.0000\n",
      "Epoch 453: val_loss improved from 1188045403556151296.00000 to 1187210187035901952.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch453-val_Loss1187210187035901952.000-mae656740992.000.h5\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 6774701917437165568.0000 - mae: 656740992.0000 - val_loss: 1187210187035901952.0000 - val_mae: 576864000.0000\n",
      "Epoch 454/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2203115362952675328.0000 - mae: 624013824.0000\n",
      "Epoch 454: val_loss improved from 1187210187035901952.00000 to 1186366724178444288.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch454-val_Loss1186366724178444288.000-mae656515264.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 6772783269646696448.0000 - mae: 656515264.0000 - val_loss: 1186366724178444288.0000 - val_mae: 576878208.0000\n",
      "Epoch 455/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 259834972968321024.0000 - mae: 390764992.0000\n",
      "Epoch 455: val_loss improved from 1186366724178444288.00000 to 1185604212864581632.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch455-val_Loss1185604212864581632.000-mae656199488.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 6769718930740084736.0000 - mae: 656199488.0000 - val_loss: 1185604212864581632.0000 - val_mae: 576851840.0000\n",
      "Epoch 456/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10526689139955335168.0000 - mae: 741337728.0000\n",
      "Epoch 456: val_loss improved from 1185604212864581632.00000 to 1184746731233869824.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch456-val_Loss1184746731233869824.000-mae655892416.000.h5\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 6768214798833287168.0000 - mae: 655892416.0000 - val_loss: 1184746731233869824.0000 - val_mae: 576820416.0000\n",
      "Epoch 457/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1918271757492420608.0000 - mae: 539476864.0000\n",
      "Epoch 457: val_loss improved from 1184746731233869824.00000 to 1183971987853148160.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch457-val_Loss1183971987853148160.000-mae655603264.000.h5\n",
      "3/3 [==============================] - 0s 179ms/step - loss: 6765436332949897216.0000 - mae: 655603264.0000 - val_loss: 1183971987853148160.0000 - val_mae: 576793152.0000\n",
      "Epoch 458/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 356880583655161856.0000 - mae: 448157184.0000\n",
      "Epoch 458: val_loss improved from 1183971987853148160.00000 to 1183165908391034880.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch458-val_Loss1183165908391034880.000-mae655374848.000.h5\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 6763424226671067136.0000 - mae: 655374848.0000 - val_loss: 1183165908391034880.0000 - val_mae: 576805312.0000\n",
      "Epoch 459/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10471423287496802304.0000 - mae: 723382400.0000\n",
      "Epoch 459: val_loss improved from 1183165908391034880.00000 to 1182367800388222976.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch459-val_Loss1182367800388222976.000-mae655059392.000.h5\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 6761031139613212672.0000 - mae: 655059392.0000 - val_loss: 1182367800388222976.0000 - val_mae: 576822272.0000\n",
      "Epoch 460/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 433349418343727104.0000 - mae: 427838720.0000\n",
      "Epoch 460: val_loss improved from 1182367800388222976.00000 to 1181594019080175616.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch460-val_Loss1181594019080175616.000-mae654810880.000.h5\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 6758998692369268736.0000 - mae: 654810880.0000 - val_loss: 1181594019080175616.0000 - val_mae: 576866112.0000\n",
      "Epoch 461/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 565321015886872576.0000 - mae: 486556064.0000\n",
      "Epoch 461: val_loss improved from 1181594019080175616.00000 to 1180842777760497664.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch461-val_Loss1180842777760497664.000-mae654455104.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 6756586363857928192.0000 - mae: 654455104.0000 - val_loss: 1180842777760497664.0000 - val_mae: 576880960.0000\n",
      "Epoch 462/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1975180555200757760.0000 - mae: 508785664.0000\n",
      "Epoch 462: val_loss improved from 1180842777760497664.00000 to 1180125483862327296.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch462-val_Loss1180125483862327296.000-mae654236288.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 6754411529858187264.0000 - mae: 654236288.0000 - val_loss: 1180125483862327296.0000 - val_mae: 576915200.0000\n",
      "Epoch 463/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2138023449954615296.0000 - mae: 604924352.0000\n",
      "Epoch 463: val_loss improved from 1180125483862327296.00000 to 1179377403638579200.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch463-val_Loss1179377403638579200.000-mae654002624.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6752279027056115712.0000 - mae: 654002624.0000 - val_loss: 1179377403638579200.0000 - val_mae: 576940288.0000\n",
      "Epoch 464/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 351679584418136064.0000 - mae: 458591072.0000\n",
      "Epoch 464: val_loss improved from 1179377403638579200.00000 to 1178643067310178304.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch464-val_Loss1178643067310178304.000-mae653597504.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6750105842323816448.0000 - mae: 653597504.0000 - val_loss: 1178643067310178304.0000 - val_mae: 576920768.0000\n",
      "Epoch 465/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 439880448693239808.0000 - mae: 457633152.0000\n",
      "Epoch 465: val_loss improved from 1178643067310178304.00000 to 1177951199618400256.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch465-val_Loss1177951199618400256.000-mae653402432.000.h5\n",
      "3/3 [==============================] - 0s 152ms/step - loss: 6747519790975287296.0000 - mae: 653402432.0000 - val_loss: 1177951199618400256.0000 - val_mae: 576895616.0000\n",
      "Epoch 466/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1918729704085389312.0000 - mae: 541102528.0000\n",
      "Epoch 466: val_loss improved from 1177951199618400256.00000 to 1177213564755116032.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch466-val_Loss1177213564755116032.000-mae653114496.000.h5\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 6746073383428947968.0000 - mae: 653114496.0000 - val_loss: 1177213564755116032.0000 - val_mae: 576906304.0000\n",
      "Epoch 467/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 347908019016695808.0000 - mae: 446320256.0000\n",
      "Epoch 467: val_loss improved from 1177213564755116032.00000 to 1176510564508106752.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch467-val_Loss1176510564508106752.000-mae652843968.000.h5\n",
      "3/3 [==============================] - 0s 182ms/step - loss: 6743470289650188288.0000 - mae: 652843968.0000 - val_loss: 1176510564508106752.0000 - val_mae: 576901632.0000\n",
      "Epoch 468/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 608750934910042112.0000 - mae: 522511552.0000\n",
      "Epoch 468: val_loss improved from 1176510564508106752.00000 to 1175795194755284992.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch468-val_Loss1175795194755284992.000-mae652671296.000.h5\n",
      "3/3 [==============================] - 0s 158ms/step - loss: 6741497765789958144.0000 - mae: 652671296.0000 - val_loss: 1175795194755284992.0000 - val_mae: 576938304.0000\n",
      "Epoch 469/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10575721860996005888.0000 - mae: 735041408.0000\n",
      "Epoch 469: val_loss improved from 1175795194755284992.00000 to 1175045877580955648.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch469-val_Loss1175045877580955648.000-mae652370688.000.h5\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 6739425736127414272.0000 - mae: 652370688.0000 - val_loss: 1175045877580955648.0000 - val_mae: 576940736.0000\n",
      "Epoch 470/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6717589435199782912.0000 - mae: 653721472.0000\n",
      "Epoch 470: val_loss improved from 1175045877580955648.00000 to 1174330645267087360.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch470-val_Loss1174330645267087360.000-mae652108096.000.h5\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 6737449913732300800.0000 - mae: 652108096.0000 - val_loss: 1174330645267087360.0000 - val_mae: 576944192.0000\n",
      "Epoch 471/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 556393737383575552.0000 - mae: 450033504.0000\n",
      "Epoch 471: val_loss improved from 1174330645267087360.00000 to 1173670525973561344.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch471-val_Loss1173670525973561344.000-mae651851200.000.h5\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 6734847919465168896.0000 - mae: 651851200.0000 - val_loss: 1173670525973561344.0000 - val_mae: 576942016.0000\n",
      "Epoch 472/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 311065171076317184.0000 - mae: 429998752.0000\n",
      "Epoch 472: val_loss improved from 1173670525973561344.00000 to 1173014942165499904.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch472-val_Loss1173014942165499904.000-mae651611968.000.h5\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 6732612612325900288.0000 - mae: 651611968.0000 - val_loss: 1173014942165499904.0000 - val_mae: 576938752.0000\n",
      "Epoch 473/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2127516654278541312.0000 - mae: 591443968.0000\n",
      "Epoch 473: val_loss improved from 1173014942165499904.00000 to 1172319913377792000.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch473-val_Loss1172319913377792000.000-mae651410176.000.h5\n",
      "3/3 [==============================] - 0s 161ms/step - loss: 6730930359535403008.0000 - mae: 651410176.0000 - val_loss: 1172319913377792000.0000 - val_mae: 576969920.0000\n",
      "Epoch 474/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1787602084795777024.0000 - mae: 494426688.0000\n",
      "Epoch 474: val_loss improved from 1172319913377792000.00000 to 1171641377264500736.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch474-val_Loss1171641377264500736.000-mae651216640.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 6728994119558889472.0000 - mae: 651216640.0000 - val_loss: 1171641377264500736.0000 - val_mae: 576971136.0000\n",
      "Epoch 475/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2063144096762953728.0000 - mae: 558711808.0000\n",
      "Epoch 475: val_loss improved from 1171641377264500736.00000 to 1170997750645391360.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch475-val_Loss1170997750645391360.000-mae650968448.000.h5\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 6726568596908015616.0000 - mae: 650968448.0000 - val_loss: 1170997750645391360.0000 - val_mae: 576972032.0000\n",
      "Epoch 476/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6770231303158628352.0000 - mae: 654554112.0000\n",
      "Epoch 476: val_loss improved from 1170997750645391360.00000 to 1170340929886748672.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch476-val_Loss1170340929886748672.000-mae650739520.000.h5\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 6724530102350118912.0000 - mae: 650739520.0000 - val_loss: 1170340929886748672.0000 - val_mae: 576917696.0000\n",
      "Epoch 477/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 304499368671576064.0000 - mae: 425264512.0000\n",
      "Epoch 477: val_loss improved from 1170340929886748672.00000 to 1169692630343221248.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch477-val_Loss1169692630343221248.000-mae650544960.000.h5\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 6722134816269008896.0000 - mae: 650544960.0000 - val_loss: 1169692630343221248.0000 - val_mae: 576883264.0000\n",
      "Epoch 478/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1861319666513739776.0000 - mae: 512602720.0000\n",
      "Epoch 478: val_loss improved from 1169692630343221248.00000 to 1169018492276441088.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch478-val_Loss1169018492276441088.000-mae650339712.000.h5\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 6720568012199428096.0000 - mae: 650339712.0000 - val_loss: 1169018492276441088.0000 - val_mae: 576844160.0000\n",
      "Epoch 479/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 377776183665754112.0000 - mae: 441387904.0000\n",
      "Epoch 479: val_loss improved from 1169018492276441088.00000 to 1168364145418960896.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch479-val_Loss1168364145418960896.000-mae650113216.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 6718295321664815104.0000 - mae: 650113216.0000 - val_loss: 1168364145418960896.0000 - val_mae: 576796288.0000\n",
      "Epoch 480/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8519402570345611264.0000 - mae: 805704256.0000\n",
      "Epoch 480: val_loss improved from 1168364145418960896.00000 to 1167724229651595264.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch480-val_Loss1167724229651595264.000-mae649988864.000.h5\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 6716802734630109184.0000 - mae: 649988864.0000 - val_loss: 1167724229651595264.0000 - val_mae: 576902400.0000\n",
      "Epoch 481/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 261269234347147264.0000 - mae: 404825344.0000\n",
      "Epoch 481: val_loss improved from 1167724229651595264.00000 to 1167118673622597632.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch481-val_Loss1167118673622597632.000-mae649807104.000.h5\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 6713953350246727680.0000 - mae: 649807104.0000 - val_loss: 1167118673622597632.0000 - val_mae: 576929216.0000\n",
      "Epoch 482/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6829037583058599936.0000 - mae: 656351232.0000\n",
      "Epoch 482: val_loss improved from 1167118673622597632.00000 to 1166484392852324352.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch482-val_Loss1166484392852324352.000-mae649655680.000.h5\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 6712549273898057728.0000 - mae: 649655680.0000 - val_loss: 1166484392852324352.0000 - val_mae: 576997184.0000\n",
      "Epoch 483/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 289268590006239232.0000 - mae: 408278464.0000\n",
      "Epoch 483: val_loss improved from 1166484392852324352.00000 to 1165897803398905856.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch483-val_Loss1165897803398905856.000-mae649509376.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 6710008852282081280.0000 - mae: 649509376.0000 - val_loss: 1165897803398905856.0000 - val_mae: 577012928.0000\n",
      "Epoch 484/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 316202536077885440.0000 - mae: 427629632.0000\n",
      "Epoch 484: val_loss improved from 1165897803398905856.00000 to 1165287024689676288.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch484-val_Loss1165287024689676288.000-mae649260096.000.h5\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 6708051171828826112.0000 - mae: 649260096.0000 - val_loss: 1165287024689676288.0000 - val_mae: 577032256.0000\n",
      "Epoch 485/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10347110303837192192.0000 - mae: 706306432.0000\n",
      "Epoch 485: val_loss improved from 1165287024689676288.00000 to 1164669099154866176.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch485-val_Loss1164669099154866176.000-mae649115008.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 6706683929119686656.0000 - mae: 649115008.0000 - val_loss: 1164669099154866176.0000 - val_mae: 577085952.0000\n",
      "Epoch 486/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10262757970777473024.0000 - mae: 689755776.0000\n",
      "Epoch 486: val_loss improved from 1164669099154866176.00000 to 1164070277634588672.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch486-val_Loss1164070277634588672.000-mae648969536.000.h5\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 6704641036515278848.0000 - mae: 648969536.0000 - val_loss: 1164070277634588672.0000 - val_mae: 577137152.0000\n",
      "Epoch 487/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2101940227110076416.0000 - mae: 588937216.0000\n",
      "Epoch 487: val_loss improved from 1164070277634588672.00000 to 1163509251826515968.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch487-val_Loss1163509251826515968.000-mae648775296.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 6702261693352771584.0000 - mae: 648775296.0000 - val_loss: 1163509251826515968.0000 - val_mae: 577162624.0000\n",
      "Epoch 488/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 469370072104697856.0000 - mae: 465148768.0000\n",
      "Epoch 488: val_loss improved from 1163509251826515968.00000 to 1162945889556234240.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch488-val_Loss1162945889556234240.000-mae648633280.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 6700321055329746944.0000 - mae: 648633280.0000 - val_loss: 1162945889556234240.0000 - val_mae: 577172032.0000\n",
      "Epoch 489/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 340327436098994176.0000 - mae: 422276768.0000\n",
      "Epoch 489: val_loss improved from 1162945889556234240.00000 to 1162397370692927488.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch489-val_Loss1162397370692927488.000-mae648488448.000.h5\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 6698154467667214336.0000 - mae: 648488448.0000 - val_loss: 1162397370692927488.0000 - val_mae: 577156096.0000\n",
      "Epoch 490/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 312083525002067968.0000 - mae: 405933408.0000\n",
      "Epoch 490: val_loss improved from 1162397370692927488.00000 to 1161823563062181888.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch490-val_Loss1161823563062181888.000-mae648360704.000.h5\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 6696705861097619456.0000 - mae: 648360704.0000 - val_loss: 1161823563062181888.0000 - val_mae: 577172480.0000\n",
      "Epoch 491/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 309592787567771648.0000 - mae: 420772928.0000\n",
      "Epoch 491: val_loss improved from 1161823563062181888.00000 to 1161273669809340416.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch491-val_Loss1161273669809340416.000-mae648216192.000.h5\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 6694491444679278592.0000 - mae: 648216192.0000 - val_loss: 1161273669809340416.0000 - val_mae: 577184640.0000\n",
      "Epoch 492/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1887666713894322176.0000 - mae: 519086528.0000\n",
      "Epoch 492: val_loss improved from 1161273669809340416.00000 to 1160725700701847552.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch492-val_Loss1160725700701847552.000-mae648122048.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6692968621074808832.0000 - mae: 648122048.0000 - val_loss: 1160725700701847552.0000 - val_mae: 577252480.0000\n",
      "Epoch 493/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6659415374485782528.0000 - mae: 643287424.0000\n",
      "Epoch 493: val_loss improved from 1160725700701847552.00000 to 1160191338050748416.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch493-val_Loss1160191338050748416.000-mae647948480.000.h5\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 6690810829505298432.0000 - mae: 647948480.0000 - val_loss: 1160191338050748416.0000 - val_mae: 577289408.0000\n",
      "Epoch 494/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6763468756891992064.0000 - mae: 662355648.0000\n",
      "Epoch 494: val_loss improved from 1160191338050748416.00000 to 1159625914196164608.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch494-val_Loss1159625914196164608.000-mae647855104.000.h5\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 6689169258645028864.0000 - mae: 647855104.0000 - val_loss: 1159625914196164608.0000 - val_mae: 577315776.0000\n",
      "Epoch 495/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1813010286685192192.0000 - mae: 526175232.0000\n",
      "Epoch 495: val_loss improved from 1159625914196164608.00000 to 1159105432879366144.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch495-val_Loss1159105432879366144.000-mae647745472.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 6686623339470913536.0000 - mae: 647745472.0000 - val_loss: 1159105432879366144.0000 - val_mae: 577315328.0000\n",
      "Epoch 496/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16661377983247810560.0000 - mae: 939387776.0000\n",
      "Epoch 496: val_loss improved from 1159105432879366144.00000 to 1158508123187576832.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch496-val_Loss1158508123187576832.000-mae647578048.000.h5\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 6685491392250118144.0000 - mae: 647578048.0000 - val_loss: 1158508123187576832.0000 - val_mae: 577359488.0000\n",
      "Epoch 497/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10217261279131729920.0000 - mae: 675842240.0000\n",
      "Epoch 497: val_loss improved from 1158508123187576832.00000 to 1157946410184736768.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch497-val_Loss1157946410184736768.000-mae647468864.000.h5\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 6683246739262013440.0000 - mae: 647468864.0000 - val_loss: 1157946410184736768.0000 - val_mae: 577403392.0000\n",
      "Epoch 498/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10288176480588398592.0000 - mae: 705028224.0000\n",
      "Epoch 498: val_loss improved from 1157946410184736768.00000 to 1157408611559800832.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch498-val_Loss1157408611559800832.000-mae647335168.000.h5\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 6681263770041319424.0000 - mae: 647335168.0000 - val_loss: 1157408611559800832.0000 - val_mae: 577449408.0000\n",
      "Epoch 499/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 379725342903894016.0000 - mae: 448400128.0000\n",
      "Epoch 499: val_loss improved from 1157408611559800832.00000 to 1156889504632537088.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch499-val_Loss1156889504632537088.000-mae647267008.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 6679218128657842176.0000 - mae: 647267008.0000 - val_loss: 1156889504632537088.0000 - val_mae: 577478208.0000\n",
      "Epoch 500/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16928698946774106112.0000 - mae: 1007663424.0000\n",
      "Epoch 500: val_loss improved from 1156889504632537088.00000 to 1156356516370972672.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch500-val_Loss1156356516370972672.000-mae647143872.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6678135659460296704.0000 - mae: 647143872.0000 - val_loss: 1156356516370972672.0000 - val_mae: 577574464.0000\n",
      "Epoch 501/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 368652779775852544.0000 - mae: 457725920.0000\n",
      "Epoch 501: val_loss improved from 1156356516370972672.00000 to 1155857338091962368.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch501-val_Loss1155857338091962368.000-mae647017472.000.h5\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 6675434159390851072.0000 - mae: 647017472.0000 - val_loss: 1155857338091962368.0000 - val_mae: 577609856.0000\n",
      "Epoch 502/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6627768681059319808.0000 - mae: 639832768.0000\n",
      "Epoch 502: val_loss improved from 1155857338091962368.00000 to 1155370529318764544.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch502-val_Loss1155370529318764544.000-mae646930432.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 6673902539693359104.0000 - mae: 646930432.0000 - val_loss: 1155370529318764544.0000 - val_mae: 577678272.0000\n",
      "Epoch 503/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11900974221070696448.0000 - mae: 806723584.0000\n",
      "Epoch 503: val_loss improved from 1155370529318764544.00000 to 1154841801664757760.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch503-val_Loss1154841801664757760.000-mae646877952.000.h5\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 6672457231658647552.0000 - mae: 646877952.0000 - val_loss: 1154841801664757760.0000 - val_mae: 577758848.0000\n",
      "Epoch 504/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 451702741312471040.0000 - mae: 465782272.0000\n",
      "Epoch 504: val_loss improved from 1154841801664757760.00000 to 1154314310961332224.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch504-val_Loss1154314310961332224.000-mae646712064.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 6669894819810115584.0000 - mae: 646712064.0000 - val_loss: 1154314310961332224.0000 - val_mae: 577718912.0000\n",
      "Epoch 505/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 437303193437732864.0000 - mae: 444879680.0000\n",
      "Epoch 505: val_loss improved from 1154314310961332224.00000 to 1153821042557321216.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch505-val_Loss1153821042557321216.000-mae646567552.000.h5\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 6667966826170810368.0000 - mae: 646567552.0000 - val_loss: 1153821042557321216.0000 - val_mae: 577697024.0000\n",
      "Epoch 506/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1813561279449661440.0000 - mae: 529272064.0000\n",
      "Epoch 506: val_loss improved from 1153821042557321216.00000 to 1153322551473078272.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch506-val_Loss1153322551473078272.000-mae646472320.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 6666817836519784448.0000 - mae: 646472320.0000 - val_loss: 1153322551473078272.0000 - val_mae: 577774208.0000\n",
      "Epoch 507/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6712515189037596672.0000 - mae: 696977792.0000\n",
      "Epoch 507: val_loss improved from 1153322551473078272.00000 to 1152803925582151680.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch507-val_Loss1152803925582151680.000-mae646329344.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6664860705822343168.0000 - mae: 646329344.0000 - val_loss: 1152803925582151680.0000 - val_mae: 577759616.0000\n",
      "Epoch 508/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 324517902000586752.0000 - mae: 447692832.0000\n",
      "Epoch 508: val_loss improved from 1152803925582151680.00000 to 1152308801752268800.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch508-val_Loss1152308801752268800.000-mae646228992.000.h5\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 6662969545822568448.0000 - mae: 646228992.0000 - val_loss: 1152308801752268800.0000 - val_mae: 577761472.0000\n",
      "Epoch 509/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11748413684181893120.0000 - mae: 756158976.0000\n",
      "Epoch 509: val_loss improved from 1152308801752268800.00000 to 1151815052311920640.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch509-val_Loss1151815052311920640.000-mae646108160.000.h5\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 6661271899869282304.0000 - mae: 646108160.0000 - val_loss: 1151815052311920640.0000 - val_mae: 577793728.0000\n",
      "Epoch 510/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 427342683241971712.0000 - mae: 420059520.0000\n",
      "Epoch 510: val_loss improved from 1151815052311920640.00000 to 1151310651352678400.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch510-val_Loss1151310651352678400.000-mae646030848.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 6659331811602071552.0000 - mae: 646030848.0000 - val_loss: 1151310651352678400.0000 - val_mae: 577787776.0000\n",
      "Epoch 511/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 512347851330682880.0000 - mae: 504261600.0000\n",
      "Epoch 511: val_loss improved from 1151310651352678400.00000 to 1150815458803318784.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch511-val_Loss1150815458803318784.000-mae645912192.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6657631416869715968.0000 - mae: 645912192.0000 - val_loss: 1150815458803318784.0000 - val_mae: 577759488.0000\n",
      "Epoch 512/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11764743630877622272.0000 - mae: 805381248.0000\n",
      "Epoch 512: val_loss improved from 1150815458803318784.00000 to 1150329612102795264.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch512-val_Loss1150329612102795264.000-mae645737344.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 6656178962009423872.0000 - mae: 645737344.0000 - val_loss: 1150329612102795264.0000 - val_mae: 577779264.0000\n",
      "Epoch 513/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 254402681152471040.0000 - mae: 380420288.0000\n",
      "Epoch 513: val_loss improved from 1150329612102795264.00000 to 1149841291501109248.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch513-val_Loss1149841291501109248.000-mae645630144.000.h5\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 6653812812986449920.0000 - mae: 645630144.0000 - val_loss: 1149841291501109248.0000 - val_mae: 577694336.0000\n",
      "Epoch 514/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16478584175130050560.0000 - mae: 862034624.0000\n",
      "Epoch 514: val_loss improved from 1149841291501109248.00000 to 1149321428659601408.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch514-val_Loss1149321428659601408.000-mae645542848.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6652868332498190336.0000 - mae: 645542848.0000 - val_loss: 1149321428659601408.0000 - val_mae: 577696384.0000\n",
      "Epoch 515/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 450385285864226816.0000 - mae: 423562496.0000\n",
      "Epoch 515: val_loss improved from 1149321428659601408.00000 to 1148848844818087936.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch515-val_Loss1148848844818087936.000-mae645450816.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6650448307405455360.0000 - mae: 645450816.0000 - val_loss: 1148848844818087936.0000 - val_mae: 577676992.0000\n",
      "Epoch 516/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8134640021077491712.0000 - mae: 736139648.0000\n",
      "Epoch 516: val_loss improved from 1148848844818087936.00000 to 1148361142691692544.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch516-val_Loss1148361142691692544.000-mae645319296.000.h5\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 6649021141312602112.0000 - mae: 645319296.0000 - val_loss: 1148361142691692544.0000 - val_mae: 577648576.0000\n",
      "Epoch 517/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 361377105176428544.0000 - mae: 420784800.0000\n",
      "Epoch 517: val_loss improved from 1148361142691692544.00000 to 1147823137908326400.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch517-val_Loss1147823137908326400.000-mae645167232.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6646927121417502720.0000 - mae: 645167232.0000 - val_loss: 1147823137908326400.0000 - val_mae: 577520576.0000\n",
      "Epoch 518/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 553788685099728896.0000 - mae: 481971264.0000\n",
      "Epoch 518: val_loss improved from 1147823137908326400.00000 to 1147277573982519296.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch518-val_Loss1147277573982519296.000-mae645039488.000.h5\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 6645476865580466176.0000 - mae: 645039488.0000 - val_loss: 1147277573982519296.0000 - val_mae: 577458432.0000\n",
      "Epoch 519/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 352957388728303616.0000 - mae: 414957248.0000\n",
      "Epoch 519: val_loss improved from 1147277573982519296.00000 to 1146782999908450304.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch519-val_Loss1146782999908450304.000-mae644904000.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6643523033417908224.0000 - mae: 644904000.0000 - val_loss: 1146782999908450304.0000 - val_mae: 577415232.0000\n",
      "Epoch 520/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 500625889147486208.0000 - mae: 469193216.0000\n",
      "Epoch 520: val_loss improved from 1146782999908450304.00000 to 1146253104023339008.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch520-val_Loss1146253104023339008.000-mae644788032.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 6642046389301805056.0000 - mae: 644788032.0000 - val_loss: 1146253104023339008.0000 - val_mae: 577360064.0000\n",
      "Epoch 521/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 334052042202939392.0000 - mae: 410692032.0000\n",
      "Epoch 521: val_loss improved from 1146253104023339008.00000 to 1145727468745785344.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch521-val_Loss1145727468745785344.000-mae644570112.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 6640189863918305280.0000 - mae: 644570112.0000 - val_loss: 1145727468745785344.0000 - val_mae: 577269248.0000\n",
      "Epoch 522/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16520984642031976448.0000 - mae: 905106752.0000\n",
      "Epoch 522: val_loss improved from 1145727468745785344.00000 to 1145230420770553856.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch522-val_Loss1145230420770553856.000-mae644468096.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6639038675244023808.0000 - mae: 644468096.0000 - val_loss: 1145230420770553856.0000 - val_mae: 577257344.0000\n",
      "Epoch 523/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 334592795765374976.0000 - mae: 434446528.0000\n",
      "Epoch 523: val_loss improved from 1145230420770553856.00000 to 1144725195177590784.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch523-val_Loss1144725195177590784.000-mae644381696.000.h5\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 6636729700825694208.0000 - mae: 644381696.0000 - val_loss: 1144725195177590784.0000 - val_mae: 577149120.0000\n",
      "Epoch 524/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1809554659078045696.0000 - mae: 501253536.0000\n",
      "Epoch 524: val_loss improved from 1144725195177590784.00000 to 1144275426202353664.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch524-val_Loss1144275426202353664.000-mae644236416.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 6634968283197997056.0000 - mae: 644236416.0000 - val_loss: 1144275426202353664.0000 - val_mae: 577093440.0000\n",
      "Epoch 525/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 276087730511806464.0000 - mae: 383365088.0000\n",
      "Epoch 525: val_loss improved from 1144275426202353664.00000 to 1143775285850669056.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch525-val_Loss1143775285850669056.000-mae644123584.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 6633215111907508224.0000 - mae: 644123584.0000 - val_loss: 1143775285850669056.0000 - val_mae: 577010112.0000\n",
      "Epoch 526/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10173704125997383680.0000 - mae: 650389312.0000\n",
      "Epoch 526: val_loss improved from 1143775285850669056.00000 to 1143264012943753216.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch526-val_Loss1143264012943753216.000-mae643970752.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6631935830128590848.0000 - mae: 643970752.0000 - val_loss: 1143264012943753216.0000 - val_mae: 576973184.0000\n",
      "Epoch 527/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1881918054787448832.0000 - mae: 526745632.0000\n",
      "Epoch 527: val_loss improved from 1143264012943753216.00000 to 1142791154224332800.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch527-val_Loss1142791154224332800.000-mae643811968.000.h5\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 6629693925919555584.0000 - mae: 643811968.0000 - val_loss: 1142791154224332800.0000 - val_mae: 576889280.0000\n",
      "Epoch 528/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6739750092057608192.0000 - mae: 690009216.0000\n",
      "Epoch 528: val_loss improved from 1142791154224332800.00000 to 1142279881317416960.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch528-val_Loss1142279881317416960.000-mae643660672.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6628286551036002304.0000 - mae: 643660672.0000 - val_loss: 1142279881317416960.0000 - val_mae: 576793792.0000\n",
      "Epoch 529/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 280659894997090304.0000 - mae: 395547744.0000\n",
      "Epoch 529: val_loss improved from 1142279881317416960.00000 to 1141771632067477504.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch529-val_Loss1141771632067477504.000-mae643533184.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 6626414082733899776.0000 - mae: 643533184.0000 - val_loss: 1141771632067477504.0000 - val_mae: 576721344.0000\n",
      "Epoch 530/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18003484757064679424.0000 - mae: 1019204096.0000\n",
      "Epoch 530: val_loss improved from 1141771632067477504.00000 to 1141277332871315456.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch530-val_Loss1141277332871315456.000-mae643374144.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 6625460256396804096.0000 - mae: 643374144.0000 - val_loss: 1141277332871315456.0000 - val_mae: 576689792.0000\n",
      "Epoch 531/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10251829924709007360.0000 - mae: 677257856.0000\n",
      "Epoch 531: val_loss improved from 1141277332871315456.00000 to 1140744207170797568.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch531-val_Loss1140744207170797568.000-mae643200576.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 6622943474280824832.0000 - mae: 643200576.0000 - val_loss: 1140744207170797568.0000 - val_mae: 576556544.0000\n",
      "Epoch 532/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10308500953027837952.0000 - mae: 743268096.0000\n",
      "Epoch 532: val_loss improved from 1140744207170797568.00000 to 1140242417551671296.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch532-val_Loss1140242417551671296.000-mae643049792.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6621293107327533056.0000 - mae: 643049792.0000 - val_loss: 1140242417551671296.0000 - val_mae: 576452096.0000\n",
      "Epoch 533/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6659161387299766272.0000 - mae: 632447744.0000\n",
      "Epoch 533: val_loss improved from 1140242417551671296.00000 to 1139792167540097024.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch533-val_Loss1139792167540097024.000-mae642861120.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 6619511348734722048.0000 - mae: 642861120.0000 - val_loss: 1139792167540097024.0000 - val_mae: 576405696.0000\n",
      "Epoch 534/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1910129461571878912.0000 - mae: 515282944.0000\n",
      "Epoch 534: val_loss improved from 1139792167540097024.00000 to 1139307763948584960.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch534-val_Loss1139307763948584960.000-mae642733056.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 6617880772990730240.0000 - mae: 642733056.0000 - val_loss: 1139307763948584960.0000 - val_mae: 576368960.0000\n",
      "Epoch 535/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1999166263921737728.0000 - mae: 582992896.0000\n",
      "Epoch 535: val_loss improved from 1139307763948584960.00000 to 1138813602191376384.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch535-val_Loss1138813602191376384.000-mae642589760.000.h5\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 6615908249130500096.0000 - mae: 642589760.0000 - val_loss: 1138813602191376384.0000 - val_mae: 576274240.0000\n",
      "Epoch 536/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6593018066307645440.0000 - mae: 598490624.0000\n",
      "Epoch 536: val_loss improved from 1138813602191376384.00000 to 1138302604162367488.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch536-val_Loss1138302604162367488.000-mae642476992.000.h5\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 6614704833653899264.0000 - mae: 642476992.0000 - val_loss: 1138302604162367488.0000 - val_mae: 576205248.0000\n",
      "Epoch 537/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 337866351118909440.0000 - mae: 441932416.0000\n",
      "Epoch 537: val_loss improved from 1138302604162367488.00000 to 1137757864870281216.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch537-val_Loss1137757864870281216.000-mae642313600.000.h5\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 6612535497212297216.0000 - mae: 642313600.0000 - val_loss: 1137757864870281216.0000 - val_mae: 576076416.0000\n",
      "Epoch 538/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11699440336769122304.0000 - mae: 797081088.0000\n",
      "Epoch 538: val_loss improved from 1137757864870281216.00000 to 1137264321588363264.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch538-val_Loss1137264321588363264.000-mae642141120.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6611167704747343872.0000 - mae: 642141120.0000 - val_loss: 1137264321588363264.0000 - val_mae: 576025856.0000\n",
      "Epoch 539/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16490163132082159616.0000 - mae: 927863232.0000\n",
      "Epoch 539: val_loss improved from 1137264321588363264.00000 to 1136761157579702272.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch539-val_Loss1136761157579702272.000-mae642015744.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 6609857636642848768.0000 - mae: 642015744.0000 - val_loss: 1136761157579702272.0000 - val_mae: 575958912.0000\n",
      "Epoch 540/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6656449441869856768.0000 - mae: 649901696.0000\n",
      "Epoch 540: val_loss improved from 1136761157579702272.00000 to 1136283419777433600.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch540-val_Loss1136283419777433600.000-mae641883328.000.h5\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 6607532719305916416.0000 - mae: 641883328.0000 - val_loss: 1136283419777433600.0000 - val_mae: 575875456.0000\n",
      "Epoch 541/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10270994412381143040.0000 - mae: 703128000.0000\n",
      "Epoch 541: val_loss improved from 1136283419777433600.00000 to 1135744452921393152.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch541-val_Loss1135744452921393152.000-mae641717824.000.h5\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 6606141837096779776.0000 - mae: 641717824.0000 - val_loss: 1135744452921393152.0000 - val_mae: 575790528.0000\n",
      "Epoch 542/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10458612877521584128.0000 - mae: 787431040.0000\n",
      "Epoch 542: val_loss improved from 1135744452921393152.00000 to 1135233729770291200.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch542-val_Loss1135233729770291200.000-mae641558016.000.h5\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 6604342486317924352.0000 - mae: 641558016.0000 - val_loss: 1135233729770291200.0000 - val_mae: 575686720.0000\n",
      "Epoch 543/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10185873520693608448.0000 - mae: 667605376.0000\n",
      "Epoch 543: val_loss improved from 1135233729770291200.00000 to 1134723693813956608.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch543-val_Loss1134723693813956608.000-mae641413696.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 6602628347690221568.0000 - mae: 641413696.0000 - val_loss: 1134723693813956608.0000 - val_mae: 575584704.0000\n",
      "Epoch 544/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8252538453900656640.0000 - mae: 794274368.0000\n",
      "Epoch 544: val_loss improved from 1134723693813956608.00000 to 1134278529043660800.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch544-val_Loss1134278529043660800.000-mae641254336.000.h5\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 6601196233795043328.0000 - mae: 641254336.0000 - val_loss: 1134278529043660800.0000 - val_mae: 575531648.0000\n",
      "Epoch 545/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 327314715984265216.0000 - mae: 407693312.0000\n",
      "Epoch 545: val_loss improved from 1134278529043660800.00000 to 1133771722902732800.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch545-val_Loss1133771722902732800.000-mae641119168.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 6599217112865046528.0000 - mae: 641119168.0000 - val_loss: 1133771722902732800.0000 - val_mae: 575374272.0000\n",
      "Epoch 546/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6657001946462814208.0000 - mae: 659032448.0000\n",
      "Epoch 546: val_loss improved from 1133771722902732800.00000 to 1133316525088833536.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch546-val_Loss1133316525088833536.000-mae640965312.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 6597660754155929600.0000 - mae: 640965312.0000 - val_loss: 1133316525088833536.0000 - val_mae: 575263744.0000\n",
      "Epoch 547/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16540424007611056128.0000 - mae: 913544768.0000\n",
      "Epoch 547: val_loss improved from 1133316525088833536.00000 to 1132817484248776704.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch547-val_Loss1132817484248776704.000-mae640753984.000.h5\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 6596484826470023168.0000 - mae: 640753984.0000 - val_loss: 1132817484248776704.0000 - val_mae: 575184768.0000\n",
      "Epoch 548/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 256140235941871616.0000 - mae: 392301120.0000\n",
      "Epoch 548: val_loss improved from 1132817484248776704.00000 to 1132310952985755648.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch548-val_Loss1132310952985755648.000-mae640578368.000.h5\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 6594270410051682304.0000 - mae: 640578368.0000 - val_loss: 1132310952985755648.0000 - val_mae: 575031552.0000\n",
      "Epoch 549/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8238922101902278656.0000 - mae: 785068928.0000\n",
      "Epoch 549: val_loss improved from 1132310952985755648.00000 to 1131831153599184896.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch549-val_Loss1131831153599184896.000-mae640441088.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 6592699207935590400.0000 - mae: 640441088.0000 - val_loss: 1131831153599184896.0000 - val_mae: 574896512.0000\n",
      "Epoch 550/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2008575059798523904.0000 - mae: 540830848.0000\n",
      "Epoch 550: val_loss improved from 1131831153599184896.00000 to 1131336098488778752.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch550-val_Loss1131336098488778752.000-mae640284928.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6590921847389290496.0000 - mae: 640284928.0000 - val_loss: 1131336098488778752.0000 - val_mae: 574762112.0000\n",
      "Epoch 551/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11926541164951371776.0000 - mae: 841811072.0000\n",
      "Epoch 551: val_loss improved from 1131336098488778752.00000 to 1130798506022273024.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch551-val_Loss1130798506022273024.000-mae640077440.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 6589751967017336832.0000 - mae: 640077440.0000 - val_loss: 1130798506022273024.0000 - val_mae: 574627328.0000\n",
      "Epoch 552/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16390060294954549248.0000 - mae: 909529984.0000\n",
      "Epoch 552: val_loss improved from 1130798506022273024.00000 to 1130229852352282624.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch552-val_Loss1130229852352282624.000-mae639937088.000.h5\n",
      "3/3 [==============================] - 0s 155ms/step - loss: 6588125239564042240.0000 - mae: 639937088.0000 - val_loss: 1130229852352282624.0000 - val_mae: 574446848.0000\n",
      "Epoch 553/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11731159048207204352.0000 - mae: 815496832.0000\n",
      "Epoch 553: val_loss improved from 1130229852352282624.00000 to 1129719472798564352.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch553-val_Loss1129719472798564352.000-mae639766400.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6586333035610767360.0000 - mae: 639766400.0000 - val_loss: 1129719472798564352.0000 - val_mae: 574299264.0000\n",
      "Epoch 554/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17930659703910563840.0000 - mae: 999520448.0000\n",
      "Epoch 554: val_loss improved from 1129719472798564352.00000 to 1129239261095133184.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch554-val_Loss1129239261095133184.000-mae639645376.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6585012522145808384.0000 - mae: 639645376.0000 - val_loss: 1129239261095133184.0000 - val_mae: 574174528.0000\n",
      "Epoch 555/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10206743350900424704.0000 - mae: 709504256.0000\n",
      "Epoch 555: val_loss improved from 1129239261095133184.00000 to 1128667446329212928.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch555-val_Loss1128667446329212928.000-mae639458880.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 6582790958901886976.0000 - mae: 639458880.0000 - val_loss: 1128667446329212928.0000 - val_mae: 573933824.0000\n",
      "Epoch 556/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10109470656702709760.0000 - mae: 665010048.0000\n",
      "Epoch 556: val_loss improved from 1128667446329212928.00000 to 1128152393851076608.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch556-val_Loss1128152393851076608.000-mae639310528.000.h5\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 6581409972297400320.0000 - mae: 639310528.0000 - val_loss: 1128152393851076608.0000 - val_mae: 573759616.0000\n",
      "Epoch 557/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16297607859733004288.0000 - mae: 848629120.0000\n",
      "Epoch 557: val_loss improved from 1128152393851076608.00000 to 1127684689092411392.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch557-val_Loss1127684689092411392.000-mae639183936.000.h5\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 6580087259809185792.0000 - mae: 639183936.0000 - val_loss: 1127684689092411392.0000 - val_mae: 573626880.0000\n",
      "Epoch 558/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1891149004658442240.0000 - mae: 496840992.0000\n",
      "Epoch 558: val_loss improved from 1127684689092411392.00000 to 1127226055304675328.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch558-val_Loss1127226055304675328.000-mae639031424.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 6577779384902483968.0000 - mae: 639031424.0000 - val_loss: 1127226055304675328.0000 - val_mae: 573462976.0000\n",
      "Epoch 559/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1777461563930705920.0000 - mae: 500000896.0000\n",
      "Epoch 559: val_loss improved from 1127226055304675328.00000 to 1126736154155024384.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch559-val_Loss1126736154155024384.000-mae638938944.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 6576593561611927552.0000 - mae: 638938944.0000 - val_loss: 1126736154155024384.0000 - val_mae: 573302016.0000\n",
      "Epoch 560/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16593946034627936256.0000 - mae: 954211584.0000\n",
      "Epoch 560: val_loss improved from 1126736154155024384.00000 to 1126169768227766272.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch560-val_Loss1126169768227766272.000-mae638744576.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6575242811577204736.0000 - mae: 638744576.0000 - val_loss: 1126169768227766272.0000 - val_mae: 573088384.0000\n",
      "Epoch 561/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 454441349899354112.0000 - mae: 464923776.0000\n",
      "Epoch 561: val_loss improved from 1126169768227766272.00000 to 1125669834034511872.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch561-val_Loss1125669834034511872.000-mae638572736.000.h5\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 6573049285879791616.0000 - mae: 638572736.0000 - val_loss: 1125669834034511872.0000 - val_mae: 572874560.0000\n",
      "Epoch 562/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 519111840706330624.0000 - mae: 461654976.0000\n",
      "Epoch 562: val_loss improved from 1125669834034511872.00000 to 1125152857411026944.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch562-val_Loss1125152857411026944.000-mae638387456.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6571344493100924928.0000 - mae: 638387456.0000 - val_loss: 1125152857411026944.0000 - val_mae: 572680640.0000\n",
      "Epoch 563/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17815159305947578368.0000 - mae: 989158144.0000\n",
      "Epoch 563: val_loss improved from 1125152857411026944.00000 to 1124660551079690240.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch563-val_Loss1124660551079690240.000-mae638314432.000.h5\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 6570752406089367552.0000 - mae: 638314432.0000 - val_loss: 1124660551079690240.0000 - val_mae: 572553984.0000\n",
      "Epoch 564/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11810087490407104512.0000 - mae: 820746048.0000\n",
      "Epoch 564: val_loss improved from 1124660551079690240.00000 to 1124150515123355648.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch564-val_Loss1124150515123355648.000-mae638215744.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6568758991508209664.0000 - mae: 638215744.0000 - val_loss: 1124150515123355648.0000 - val_mae: 572388608.0000\n",
      "Epoch 565/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 322294242812624896.0000 - mae: 439951040.0000\n",
      "Epoch 565: val_loss improved from 1124150515123355648.00000 to 1123626597832720384.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch565-val_Loss1123626597832720384.000-mae637972800.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6566524234124754944.0000 - mae: 637972800.0000 - val_loss: 1123626597832720384.0000 - val_mae: 572148864.0000\n",
      "Epoch 566/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11735603274206674944.0000 - mae: 832302272.0000\n",
      "Epoch 566: val_loss improved from 1123626597832720384.00000 to 1123095396277551104.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch566-val_Loss1123095396277551104.000-mae637823104.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 6565304325973737472.0000 - mae: 637823104.0000 - val_loss: 1123095396277551104.0000 - val_mae: 571992192.0000\n",
      "Epoch 567/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 423452439304208384.0000 - mae: 411522560.0000\n",
      "Epoch 567: val_loss improved from 1123095396277551104.00000 to 1122556841738371072.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch567-val_Loss1122556841738371072.000-mae637598656.000.h5\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 6563349944055365632.0000 - mae: 637598656.0000 - val_loss: 1122556841738371072.0000 - val_mae: 571811776.0000\n",
      "Epoch 568/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16525127601845436416.0000 - mae: 990334400.0000\n",
      "Epoch 568: val_loss improved from 1122556841738371072.00000 to 1122027014572736512.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch568-val_Loss1122027014572736512.000-mae637385856.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 6562314753857814528.0000 - mae: 637385856.0000 - val_loss: 1122027014572736512.0000 - val_mae: 571673792.0000\n",
      "Epoch 569/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 310604200826372096.0000 - mae: 397059456.0000\n",
      "Epoch 569: val_loss improved from 1122027014572736512.00000 to 1121526255745761280.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch569-val_Loss1121526255745761280.000-mae637266944.000.h5\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 6560054157951107072.0000 - mae: 637266944.0000 - val_loss: 1121526255745761280.0000 - val_mae: 571518208.0000\n",
      "Epoch 570/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 444099068650586112.0000 - mae: 433393920.0000\n",
      "Epoch 570: val_loss improved from 1121526255745761280.00000 to 1121024947162972160.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch570-val_Loss1121024947162972160.000-mae637065600.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 6558452169509437440.0000 - mae: 637065600.0000 - val_loss: 1121024947162972160.0000 - val_mae: 571364480.0000\n",
      "Epoch 571/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8065380134376439808.0000 - mae: 748825344.0000\n",
      "Epoch 571: val_loss improved from 1121024947162972160.00000 to 1120601154149941248.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch571-val_Loss1120601154149941248.000-mae636955008.000.h5\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 6557293834009575424.0000 - mae: 636955008.0000 - val_loss: 1120601154149941248.0000 - val_mae: 571285248.0000\n",
      "Epoch 572/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6527469031350337536.0000 - mae: 615070848.0000\n",
      "Epoch 572: val_loss improved from 1120601154149941248.00000 to 1120132281160171520.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch572-val_Loss1120132281160171520.000-mae636838976.000.h5\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 6555576396846989312.0000 - mae: 636838976.0000 - val_loss: 1120132281160171520.0000 - val_mae: 571173312.0000\n",
      "Epoch 573/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 417202780852191232.0000 - mae: 429875520.0000\n",
      "Epoch 573: val_loss improved from 1120132281160171520.00000 to 1119647671410229248.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch573-val_Loss1119647671410229248.000-mae636646784.000.h5\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 6553517561323978752.0000 - mae: 636646784.0000 - val_loss: 1119647671410229248.0000 - val_mae: 571012992.0000\n",
      "Epoch 574/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11543739594671390720.0000 - mae: 768867904.0000\n",
      "Epoch 574: val_loss improved from 1119647671410229248.00000 to 1119183471344877568.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch574-val_Loss1119183471344877568.000-mae636488768.000.h5\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 6552199246882275328.0000 - mae: 636488768.0000 - val_loss: 1119183471344877568.0000 - val_mae: 570903616.0000\n",
      "Epoch 575/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17735419424166379520.0000 - mae: 953737920.0000\n",
      "Epoch 575: val_loss improved from 1119183471344877568.00000 to 1118739887122546688.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch575-val_Loss1118739887122546688.000-mae636406208.000.h5\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 6551033764556832768.0000 - mae: 636406208.0000 - val_loss: 1118739887122546688.0000 - val_mae: 570806464.0000\n",
      "Epoch 576/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11543106275973791744.0000 - mae: 774087936.0000\n",
      "Epoch 576: val_loss improved from 1118739887122546688.00000 to 1118230332202549248.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch576-val_Loss1118230332202549248.000-mae636190464.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 6548781964743147520.0000 - mae: 636190464.0000 - val_loss: 1118230332202549248.0000 - val_mae: 570611776.0000\n",
      "Epoch 577/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17760139744093667328.0000 - mae: 996600640.0000\n",
      "Epoch 577: val_loss improved from 1118230332202549248.00000 to 1117748883548536832.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch577-val_Loss1117748883548536832.000-mae636032832.000.h5\n",
      "3/3 [==============================] - 0s 151ms/step - loss: 6547640121917702144.0000 - mae: 636032832.0000 - val_loss: 1117748883548536832.0000 - val_mae: 570460544.0000\n",
      "Epoch 578/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16256279416668160000.0000 - mae: 879421184.0000\n",
      "Epoch 578: val_loss improved from 1117748883548536832.00000 to 1117246681612550144.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch578-val_Loss1117246681612550144.000-mae635843392.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6545864960394657792.0000 - mae: 635843392.0000 - val_loss: 1117246681612550144.0000 - val_mae: 570280320.0000\n",
      "Epoch 579/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 398102511531065344.0000 - mae: 451175648.0000\n",
      "Epoch 579: val_loss improved from 1117246681612550144.00000 to 1116657137221632000.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch579-val_Loss1116657137221632000.000-mae635582848.000.h5\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 6543580724987953152.0000 - mae: 635582848.0000 - val_loss: 1116657137221632000.0000 - val_mae: 569940672.0000\n",
      "Epoch 580/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6618285393269751808.0000 - mae: 663110912.0000\n",
      "Epoch 580: val_loss improved from 1116657137221632000.00000 to 1116144008888844288.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch580-val_Loss1116144008888844288.000-mae635290304.000.h5\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 6542470218243899392.0000 - mae: 635290304.0000 - val_loss: 1116144008888844288.0000 - val_mae: 569724160.0000\n",
      "Epoch 581/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1907870102615752704.0000 - mae: 557711680.0000\n",
      "Epoch 581: val_loss improved from 1116144008888844288.00000 to 1115662422795878400.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch581-val_Loss1115662422795878400.000-mae635098304.000.h5\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 6540353108604616704.0000 - mae: 635098304.0000 - val_loss: 1115662422795878400.0000 - val_mae: 569521408.0000\n",
      "Epoch 582/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 529814899207962624.0000 - mae: 454045952.0000\n",
      "Epoch 582: val_loss improved from 1115662422795878400.00000 to 1115143522027044864.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch582-val_Loss1115143522027044864.000-mae634908416.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 6538935288360599552.0000 - mae: 634908416.0000 - val_loss: 1115143522027044864.0000 - val_mae: 569326656.0000\n",
      "Epoch 583/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10238167393222262784.0000 - mae: 710871168.0000\n",
      "Epoch 583: val_loss improved from 1115143522027044864.00000 to 1114659943069253632.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch583-val_Loss1114659943069253632.000-mae634758784.000.h5\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 6537764858232832000.0000 - mae: 634758784.0000 - val_loss: 1114659943069253632.0000 - val_mae: 569186240.0000\n",
      "Epoch 584/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6481938254844133376.0000 - mae: 602844928.0000\n",
      "Epoch 584: val_loss improved from 1114659943069253632.00000 to 1114185435082391552.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch584-val_Loss1114185435082391552.000-mae634556544.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 6535654895419129856.0000 - mae: 634556544.0000 - val_loss: 1114185435082391552.0000 - val_mae: 568995072.0000\n",
      "Epoch 585/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 335898981219434496.0000 - mae: 404101056.0000\n",
      "Epoch 585: val_loss improved from 1114185435082391552.00000 to 1113712095326633984.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch585-val_Loss1113712095326633984.000-mae634366208.000.h5\n",
      "3/3 [==============================] - 0s 159ms/step - loss: 6533902823640268800.0000 - mae: 634366208.0000 - val_loss: 1113712095326633984.0000 - val_mae: 568821888.0000\n",
      "Epoch 586/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8103475463499808768.0000 - mae: 781095296.0000\n",
      "Epoch 586: val_loss improved from 1113712095326633984.00000 to 1113242741300527104.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch586-val_Loss1113242741300527104.000-mae634206464.000.h5\n",
      "3/3 [==============================] - 0s 169ms/step - loss: 6532475657547415552.0000 - mae: 634206464.0000 - val_loss: 1113242741300527104.0000 - val_mae: 568675456.0000\n",
      "Epoch 587/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11566455504901242880.0000 - mae: 800649472.0000\n",
      "Epoch 587: val_loss improved from 1113242741300527104.00000 to 1112726726749716480.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch587-val_Loss1112726726749716480.000-mae634022272.000.h5\n",
      "3/3 [==============================] - 0s 167ms/step - loss: 6531115011908042752.0000 - mae: 634022272.0000 - val_loss: 1112726726749716480.0000 - val_mae: 568521600.0000\n",
      "Epoch 588/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6591016955145093120.0000 - mae: 623953664.0000\n",
      "Epoch 588: val_loss improved from 1112726726749716480.00000 to 1112212773783207936.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch588-val_Loss1112212773783207936.000-mae633837184.000.h5\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 6529320608931512320.0000 - mae: 633837184.0000 - val_loss: 1112212773783207936.0000 - val_mae: 568355648.0000\n",
      "Epoch 589/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 309359347505299456.0000 - mae: 399665408.0000\n",
      "Epoch 589: val_loss improved from 1112212773783207936.00000 to 1111641783651008512.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch589-val_Loss1111641783651008512.000-mae633629824.000.h5\n",
      "3/3 [==============================] - 0s 186ms/step - loss: 6527482225489870848.0000 - mae: 633629824.0000 - val_loss: 1111641783651008512.0000 - val_mae: 568094144.0000\n",
      "Epoch 590/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11802850504873082880.0000 - mae: 883120000.0000\n",
      "Epoch 590: val_loss improved from 1111641783651008512.00000 to 1111105428135084032.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch590-val_Loss1111105428135084032.000-mae633374720.000.h5\n",
      "3/3 [==============================] - 0s 187ms/step - loss: 6525964899443539968.0000 - mae: 633374720.0000 - val_loss: 1111105428135084032.0000 - val_mae: 567890496.0000\n",
      "Epoch 591/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1949780187331690496.0000 - mae: 569598016.0000\n",
      "Epoch 591: val_loss improved from 1111105428135084032.00000 to 1110587764316831744.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch591-val_Loss1110587764316831744.000-mae633133376.000.h5\n",
      "3/3 [==============================] - 0s 176ms/step - loss: 6523960489746104320.0000 - mae: 633133376.0000 - val_loss: 1110587764316831744.0000 - val_mae: 567638848.0000\n",
      "Epoch 592/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10093984035425484800.0000 - mae: 687948800.0000\n",
      "Epoch 592: val_loss improved from 1110587764316831744.00000 to 1110075323178811392.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch592-val_Loss1110075323178811392.000-mae632927296.000.h5\n",
      "3/3 [==============================] - 0s 174ms/step - loss: 6522827443013681152.0000 - mae: 632927296.0000 - val_loss: 1110075323178811392.0000 - val_mae: 567460864.0000\n",
      "Epoch 593/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10283012074472734720.0000 - mae: 733308800.0000\n",
      "Epoch 593: val_loss improved from 1110075323178811392.00000 to 1109524467853295616.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch593-val_Loss1109524467853295616.000-mae632699136.000.h5\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 6521266686258053120.0000 - mae: 632699136.0000 - val_loss: 1109524467853295616.0000 - val_mae: 567244928.0000\n",
      "Epoch 594/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 339553414272778240.0000 - mae: 369603872.0000\n",
      "Epoch 594: val_loss improved from 1109524467853295616.00000 to 1108986600508882944.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch594-val_Loss1108986600508882944.000-mae632473024.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6519164420025745408.0000 - mae: 632473024.0000 - val_loss: 1108986600508882944.0000 - val_mae: 567003328.0000\n",
      "Epoch 595/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11673966851376807936.0000 - mae: 813083648.0000\n",
      "Epoch 595: val_loss improved from 1108986600508882944.00000 to 1108515803373764608.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch595-val_Loss1108515803373764608.000-mae632258304.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 6518046216700297216.0000 - mae: 632258304.0000 - val_loss: 1108515803373764608.0000 - val_mae: 566855552.0000\n",
      "Epoch 596/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 364746799077916672.0000 - mae: 402828096.0000\n",
      "Epoch 596: val_loss improved from 1108515803373764608.00000 to 1108016556375277568.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch596-val_Loss1108016556375277568.000-mae632113920.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 6515908216340086784.0000 - mae: 632113920.0000 - val_loss: 1108016556375277568.0000 - val_mae: 566632320.0000\n",
      "Epoch 597/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 383996876858327040.0000 - mae: 410994368.0000\n",
      "Epoch 597: val_loss improved from 1108016556375277568.00000 to 1107467419036680192.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch597-val_Loss1107467419036680192.000-mae631907456.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 6514307327410044928.0000 - mae: 631907456.0000 - val_loss: 1107467419036680192.0000 - val_mae: 566385472.0000\n",
      "Epoch 598/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17696279009240809472.0000 - mae: 988292864.0000\n",
      "Epoch 598: val_loss improved from 1107467419036680192.00000 to 1106932231751860224.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch598-val_Loss1106932231751860224.000-mae631750912.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 6513552512677576704.0000 - mae: 631750912.0000 - val_loss: 1106932231751860224.0000 - val_mae: 566223808.0000\n",
      "Epoch 599/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 556364188008579072.0000 - mae: 466533536.0000\n",
      "Epoch 599: val_loss improved from 1106932231751860224.00000 to 1106308190183620608.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch599-val_Loss1106308190183620608.000-mae631489216.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 6511191861212741632.0000 - mae: 631489216.0000 - val_loss: 1106308190183620608.0000 - val_mae: 565971008.0000\n",
      "Epoch 600/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 506540059114078208.0000 - mae: 452239360.0000\n",
      "Epoch 600: val_loss improved from 1106308190183620608.00000 to 1105691501599391744.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch600-val_Loss1105691501599391744.000-mae631239168.000.h5\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 6509446936259461120.0000 - mae: 631239168.0000 - val_loss: 1105691501599391744.0000 - val_mae: 565712512.0000\n",
      "Epoch 601/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10021827485341057024.0000 - mae: 660017536.0000\n",
      "Epoch 601: val_loss improved from 1105691501599391744.00000 to 1105119549394518016.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch601-val_Loss1105119549394518016.000-mae631002048.000.h5\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 6508260013457276928.0000 - mae: 631002048.0000 - val_loss: 1105119549394518016.0000 - val_mae: 565505152.0000\n",
      "Epoch 602/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6569806826089480192.0000 - mae: 658751488.0000\n",
      "Epoch 602: val_loss improved from 1105119549394518016.00000 to 1104517635497787392.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch602-val_Loss1104517635497787392.000-mae630796352.000.h5\n",
      "3/3 [==============================] - 0s 204ms/step - loss: 6506349062248202240.0000 - mae: 630796352.0000 - val_loss: 1104517635497787392.0000 - val_mae: 565223680.0000\n",
      "Epoch 603/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2006628374461546496.0000 - mae: 547674496.0000\n",
      "Epoch 603: val_loss improved from 1104517635497787392.00000 to 1103953448593784832.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch603-val_Loss1103953448593784832.000-mae630557120.000.h5\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 6504647018248404992.0000 - mae: 630557120.0000 - val_loss: 1103953448593784832.0000 - val_mae: 565017408.0000\n",
      "Epoch 604/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1982610367586500608.0000 - mae: 549027968.0000\n",
      "Epoch 604: val_loss improved from 1103953448593784832.00000 to 1103403692779896832.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch604-val_Loss1103403692779896832.000-mae630390784.000.h5\n",
      "3/3 [==============================] - 0s 157ms/step - loss: 6503096157097426944.0000 - mae: 630390784.0000 - val_loss: 1103403692779896832.0000 - val_mae: 564819328.0000\n",
      "Epoch 605/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16168767087190212608.0000 - mae: 900809216.0000\n",
      "Epoch 605: val_loss improved from 1103403692779896832.00000 to 1102846652701474816.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch605-val_Loss1102846652701474816.000-mae630202240.000.h5\n",
      "3/3 [==============================] - 0s 170ms/step - loss: 6502104947364986880.0000 - mae: 630202240.0000 - val_loss: 1102846652701474816.0000 - val_mae: 564632256.0000\n",
      "Epoch 606/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16188656153025052672.0000 - mae: 902355072.0000\n",
      "Epoch 606: val_loss improved from 1102846652701474816.00000 to 1102243639293116416.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch606-val_Loss1102243639293116416.000-mae630009280.000.h5\n",
      "3/3 [==============================] - 0s 193ms/step - loss: 6500282506841948160.0000 - mae: 630009280.0000 - val_loss: 1102243639293116416.0000 - val_mae: 564394112.0000\n",
      "Epoch 607/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9998354011599667200.0000 - mae: 645371776.0000\n",
      "Epoch 607: val_loss improved from 1102243639293116416.00000 to 1101591079142031360.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch607-val_Loss1101591079142031360.000-mae629759168.000.h5\n",
      "3/3 [==============================] - 0s 178ms/step - loss: 6498248410330562560.0000 - mae: 629759168.0000 - val_loss: 1101591079142031360.0000 - val_mae: 564102336.0000\n",
      "Epoch 608/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 325066901900230656.0000 - mae: 395839232.0000\n",
      "Epoch 608: val_loss improved from 1101591079142031360.00000 to 1100981193785999360.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch608-val_Loss1100981193785999360.000-mae629478272.000.h5\n",
      "3/3 [==============================] - 0s 202ms/step - loss: 6496363847400554496.0000 - mae: 629478272.0000 - val_loss: 1100981193785999360.0000 - val_mae: 563834112.0000\n",
      "Epoch 609/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6638260221011558400.0000 - mae: 650925824.0000\n",
      "Epoch 609: val_loss improved from 1100981193785999360.00000 to 1100486619711930368.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch609-val_Loss1100486619711930368.000-mae629302464.000.h5\n",
      "3/3 [==============================] - 0s 153ms/step - loss: 6495384182540206080.0000 - mae: 629302464.0000 - val_loss: 1100486619711930368.0000 - val_mae: 563724480.0000\n",
      "Epoch 610/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10071781597125804032.0000 - mae: 682607808.0000\n",
      "Epoch 610: val_loss improved from 1100486619711930368.00000 to 1099920989698916352.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch610-val_Loss1099920989698916352.000-mae629132224.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 6493506216679964672.0000 - mae: 629132224.0000 - val_loss: 1099920989698916352.0000 - val_mae: 563549184.0000\n",
      "Epoch 611/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11611549775291219968.0000 - mae: 823542784.0000\n",
      "Epoch 611: val_loss improved from 1099920989698916352.00000 to 1099354672491134976.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch611-val_Loss1099354672491134976.000-mae628904384.000.h5\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 6491887185808064512.0000 - mae: 628904384.0000 - val_loss: 1099354672491134976.0000 - val_mae: 563368832.0000\n",
      "Epoch 612/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17713862399192203264.0000 - mae: 998913408.0000\n",
      "Epoch 612: val_loss improved from 1099354672491134976.00000 to 1098850134092939264.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch612-val_Loss1098850134092939264.000-mae628750080.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 6490812413191913472.0000 - mae: 628750080.0000 - val_loss: 1098850134092939264.0000 - val_mae: 563259136.0000\n",
      "Epoch 613/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10127602702956363776.0000 - mae: 719849344.0000\n",
      "Epoch 613: val_loss improved from 1098850134092939264.00000 to 1098215715883712512.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch613-val_Loss1098215715883712512.000-mae628543296.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6488701350866583552.0000 - mae: 628543296.0000 - val_loss: 1098215715883712512.0000 - val_mae: 563006208.0000\n",
      "Epoch 614/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6440791780953686016.0000 - mae: 631838720.0000\n",
      "Epoch 614: val_loss improved from 1098215715883712512.00000 to 1097616550766051328.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch614-val_Loss1097616550766051328.000-mae628290752.000.h5\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 6486804693308669952.0000 - mae: 628290752.0000 - val_loss: 1097616550766051328.0000 - val_mae: 562732288.0000\n",
      "Epoch 615/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7855820914703728640.0000 - mae: 697787776.0000\n",
      "Epoch 615: val_loss improved from 1097616550766051328.00000 to 1097056005994315776.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch615-val_Loss1097056005994315776.000-mae628009728.000.h5\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 6485529809576263680.0000 - mae: 628009728.0000 - val_loss: 1097056005994315776.0000 - val_mae: 562539712.0000\n",
      "Epoch 616/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10088458989495910400.0000 - mae: 701821568.0000\n",
      "Epoch 616: val_loss improved from 1097056005994315776.00000 to 1096375339577245696.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch616-val_Loss1096375339577245696.000-mae627762240.000.h5\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 6483925072355524608.0000 - mae: 627762240.0000 - val_loss: 1096375339577245696.0000 - val_mae: 562248384.0000\n",
      "Epoch 617/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1889889788966731776.0000 - mae: 522564224.0000\n",
      "Epoch 617: val_loss improved from 1096375339577245696.00000 to 1095788543965396992.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch617-val_Loss1095788543965396992.000-mae627478848.000.h5\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 6481951998739480576.0000 - mae: 627478848.0000 - val_loss: 1095788543965396992.0000 - val_mae: 562010816.0000\n",
      "Epoch 618/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6591532076342706176.0000 - mae: 654646976.0000\n",
      "Epoch 618: val_loss improved from 1095788543965396992.00000 to 1095243186198020096.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch618-val_Loss1095243186198020096.000-mae627296320.000.h5\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 6480843691018682368.0000 - mae: 627296320.0000 - val_loss: 1095243186198020096.0000 - val_mae: 561824192.0000\n",
      "Epoch 619/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 399815825525047296.0000 - mae: 431759392.0000\n",
      "Epoch 619: val_loss improved from 1095243186198020096.00000 to 1094565405998972928.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch619-val_Loss1094565405998972928.000-mae627065344.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6478783206228230144.0000 - mae: 627065344.0000 - val_loss: 1094565405998972928.0000 - val_mae: 561466496.0000\n",
      "Epoch 620/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16280563230479220736.0000 - mae: 926402304.0000\n",
      "Epoch 620: val_loss improved from 1094565405998972928.00000 to 1093992560440901632.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch620-val_Loss1093992560440901632.000-mae626812352.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 6478010249553903616.0000 - mae: 626812352.0000 - val_loss: 1093992560440901632.0000 - val_mae: 561293440.0000\n",
      "Epoch 621/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1712374873612877824.0000 - mae: 465843520.0000\n",
      "Epoch 621: val_loss improved from 1093992560440901632.00000 to 1093407276657541120.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch621-val_Loss1093407276657541120.000-mae626613824.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 6475824970193698816.0000 - mae: 626613824.0000 - val_loss: 1093407276657541120.0000 - val_mae: 561059968.0000\n",
      "Epoch 622/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 412234225245224960.0000 - mae: 394279648.0000\n",
      "Epoch 622: val_loss improved from 1093407276657541120.00000 to 1092754235470118912.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch622-val_Loss1092754235470118912.000-mae626339392.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 6474214735414820864.0000 - mae: 626339392.0000 - val_loss: 1092754235470118912.0000 - val_mae: 560757632.0000\n",
      "Epoch 623/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7962537864028225536.0000 - mae: 751801920.0000\n",
      "Epoch 623: val_loss improved from 1092754235470118912.00000 to 1092212863432392704.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch623-val_Loss1092212863432392704.000-mae626155520.000.h5\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 6472655078170820608.0000 - mae: 626155520.0000 - val_loss: 1092212863432392704.0000 - val_mae: 560535360.0000\n",
      "Epoch 624/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6434771954791612416.0000 - mae: 634258944.0000\n",
      "Epoch 624: val_loss improved from 1092212863432392704.00000 to 1091626067820544000.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch624-val_Loss1091626067820544000.000-mae625976640.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6471353256403533824.0000 - mae: 625976640.0000 - val_loss: 1091626067820544000.0000 - val_mae: 560339136.0000\n",
      "Epoch 625/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 418419287389110272.0000 - mae: 438488384.0000\n",
      "Epoch 625: val_loss improved from 1091626067820544000.00000 to 1090943339819171840.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch625-val_Loss1090943339819171840.000-mae625724928.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 6469334553054937088.0000 - mae: 625724928.0000 - val_loss: 1090943339819171840.0000 - val_mae: 560031680.0000\n",
      "Epoch 626/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 535835240766111744.0000 - mae: 467101184.0000\n",
      "Epoch 626: val_loss improved from 1090943339819171840.00000 to 1090307890817794048.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch626-val_Loss1090307890817794048.000-mae625469824.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 6467992599113236480.0000 - mae: 625469824.0000 - val_loss: 1090307890817794048.0000 - val_mae: 559773184.0000\n",
      "Epoch 627/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16112402822615531520.0000 - mae: 868965888.0000\n",
      "Epoch 627: val_loss improved from 1090307890817794048.00000 to 1089694775646355456.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch627-val_Loss1089694775646355456.000-mae625284736.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 6466825467520352256.0000 - mae: 625284736.0000 - val_loss: 1089694775646355456.0000 - val_mae: 559552256.0000\n",
      "Epoch 628/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 515607834587561984.0000 - mae: 458280672.0000\n",
      "Epoch 628: val_loss improved from 1089694775646355456.00000 to 1089027165929865216.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch628-val_Loss1089027165929865216.000-mae625019712.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 6464438977532264448.0000 - mae: 625019712.0000 - val_loss: 1089027165929865216.0000 - val_mae: 559215296.0000\n",
      "Epoch 629/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8078393954002796544.0000 - mae: 759660992.0000\n",
      "Epoch 629: val_loss improved from 1089027165929865216.00000 to 1088499606506962944.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch629-val_Loss1088499606506962944.000-mae624855360.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 6463550572137021440.0000 - mae: 624855360.0000 - val_loss: 1088499606506962944.0000 - val_mae: 559067776.0000\n",
      "Epoch 630/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6531182082117337088.0000 - mae: 655108352.0000\n",
      "Epoch 630: val_loss improved from 1088499606506962944.00000 to 1087889514992500736.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch630-val_Loss1087889514992500736.000-mae624650496.000.h5\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 6461756169160491008.0000 - mae: 624650496.0000 - val_loss: 1087889514992500736.0000 - val_mae: 558824000.0000\n",
      "Epoch 631/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 402262651213709312.0000 - mae: 443244256.0000\n",
      "Epoch 631: val_loss improved from 1087889514992500736.00000 to 1087216476437348352.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch631-val_Loss1087216476437348352.000-mae624421696.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 6459764403846774784.0000 - mae: 624421696.0000 - val_loss: 1087216476437348352.0000 - val_mae: 558484736.0000\n",
      "Epoch 632/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 400752884309819392.0000 - mae: 410940416.0000\n",
      "Epoch 632: val_loss improved from 1087216476437348352.00000 to 1086580202802249728.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch632-val_Loss1086580202802249728.000-mae624142912.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6458493918160879616.0000 - mae: 624142912.0000 - val_loss: 1086580202802249728.0000 - val_mae: 558212928.0000\n",
      "Epoch 633/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 303981223816986624.0000 - mae: 406229056.0000\n",
      "Epoch 633: val_loss improved from 1086580202802249728.00000 to 1085909088392445952.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch633-val_Loss1085909088392445952.000-mae623899968.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6456885332649443328.0000 - mae: 623899968.0000 - val_loss: 1085909088392445952.0000 - val_mae: 557908544.0000\n",
      "Epoch 634/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2006278317447053312.0000 - mae: 561018688.0000\n",
      "Epoch 634: val_loss improved from 1085909088392445952.00000 to 1085268347991359488.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch634-val_Loss1085268347991359488.000-mae623669440.000.h5\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 6455139857940348928.0000 - mae: 623669440.0000 - val_loss: 1085268347991359488.0000 - val_mae: 557610240.0000\n",
      "Epoch 635/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6387163101308911616.0000 - mae: 607121472.0000\n",
      "Epoch 635: val_loss improved from 1085268347991359488.00000 to 1084689798716719104.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch635-val_Loss1084689798716719104.000-mae623414976.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 6453907305405612032.0000 - mae: 623414976.0000 - val_loss: 1084689798716719104.0000 - val_mae: 557388928.0000\n",
      "Epoch 636/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 306767661159677952.0000 - mae: 388277632.0000\n",
      "Epoch 636: val_loss improved from 1084689798716719104.00000 to 1084019852538019840.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch636-val_Loss1084019852538019840.000-mae623224320.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 6452005150289559552.0000 - mae: 623224320.0000 - val_loss: 1084019852538019840.0000 - val_mae: 557048960.0000\n",
      "Epoch 637/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6597437553295491072.0000 - mae: 668099968.0000\n",
      "Epoch 637: val_loss improved from 1084019852538019840.00000 to 1083442059177623552.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch637-val_Loss1083442059177623552.000-mae622962496.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 6450523008615317504.0000 - mae: 622962496.0000 - val_loss: 1083442059177623552.0000 - val_mae: 556812672.0000\n",
      "Epoch 638/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1735584464563601408.0000 - mae: 475071648.0000\n",
      "Epoch 638: val_loss improved from 1083442059177623552.00000 to 1082842894059962368.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch638-val_Loss1082842894059962368.000-mae622748864.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6448754444162039808.0000 - mae: 622748864.0000 - val_loss: 1082842894059962368.0000 - val_mae: 556534272.0000\n",
      "Epoch 639/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6540244256953466880.0000 - mae: 644734592.0000\n",
      "Epoch 639: val_loss improved from 1082842894059962368.00000 to 1082209850240270336.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch639-val_Loss1082209850240270336.000-mae622542592.000.h5\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 6447586762813341696.0000 - mae: 622542592.0000 - val_loss: 1082209850240270336.0000 - val_mae: 556289152.0000\n",
      "Epoch 640/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1845200001416822784.0000 - mae: 506793760.0000\n",
      "Epoch 640: val_loss improved from 1082209850240270336.00000 to 1081574126360985600.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch640-val_Loss1081574126360985600.000-mae622298176.000.h5\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 6445459757569409024.0000 - mae: 622298176.0000 - val_loss: 1081574126360985600.0000 - val_mae: 555985472.0000\n",
      "Epoch 641/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9914928566842163200.0000 - mae: 663523904.0000\n",
      "Epoch 641: val_loss improved from 1081574126360985600.00000 to 1080837797167759360.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch641-val_Loss1080837797167759360.000-mae622017792.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 6444203015778861056.0000 - mae: 622017792.0000 - val_loss: 1080837797167759360.0000 - val_mae: 555629440.0000\n",
      "Epoch 642/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6492047164749905920.0000 - mae: 637141056.0000\n",
      "Epoch 642: val_loss improved from 1080837797167759360.00000 to 1080243579852423168.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch642-val_Loss1080243579852423168.000-mae621775872.000.h5\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 6442392120127913984.0000 - mae: 621775872.0000 - val_loss: 1080243579852423168.0000 - val_mae: 555374208.0000\n",
      "Epoch 643/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16100966802175033344.0000 - mae: 914887104.0000\n",
      "Epoch 643: val_loss improved from 1080243579852423168.00000 to 1079670596855398400.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch643-val_Loss1079670596855398400.000-mae621625920.000.h5\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 6441389915279196160.0000 - mae: 621625920.0000 - val_loss: 1079670596855398400.0000 - val_mae: 555175680.0000\n",
      "Epoch 644/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10113599322865008640.0000 - mae: 729655104.0000\n",
      "Epoch 644: val_loss improved from 1079670596855398400.00000 to 1078988212451409920.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch644-val_Loss1078988212451409920.000-mae621441024.000.h5\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 6439327231465488384.0000 - mae: 621441024.0000 - val_loss: 1078988212451409920.0000 - val_mae: 554849472.0000\n",
      "Epoch 645/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16113176878801485824.0000 - mae: 884184704.0000\n",
      "Epoch 645: val_loss improved from 1078988212451409920.00000 to 1078409319579385856.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch645-val_Loss1078409319579385856.000-mae621243840.000.h5\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 6437919856581935104.0000 - mae: 621243840.0000 - val_loss: 1078409319579385856.0000 - val_mae: 554607488.0000\n",
      "Epoch 646/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8001054855860846592.0000 - mae: 738931456.0000\n",
      "Epoch 646: val_loss improved from 1078409319579385856.00000 to 1077823279881781248.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch646-val_Loss1077823279881781248.000-mae621065728.000.h5\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 6435770861105446912.0000 - mae: 621065728.0000 - val_loss: 1077823279881781248.0000 - val_mae: 554308224.0000\n",
      "Epoch 647/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 447736390554222592.0000 - mae: 440221696.0000\n",
      "Epoch 647: val_loss improved from 1077823279881781248.00000 to 1077133817371688960.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch647-val_Loss1077133817371688960.000-mae620782976.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6434107849768435712.0000 - mae: 620782976.0000 - val_loss: 1077133817371688960.0000 - val_mae: 553934272.0000\n",
      "Epoch 648/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11572168567319166976.0000 - mae: 807489408.0000\n",
      "Epoch 648: val_loss improved from 1077133817371688960.00000 to 1076515204642111488.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch648-val_Loss1076515204642111488.000-mae620614336.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6432860453826723840.0000 - mae: 620614336.0000 - val_loss: 1076515204642111488.0000 - val_mae: 553655104.0000\n",
      "Epoch 649/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16034112097159741440.0000 - mae: 887795328.0000\n",
      "Epoch 649: val_loss improved from 1076515204642111488.00000 to 1075907999345672192.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch649-val_Loss1075907999345672192.000-mae620369216.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 6431480016978051072.0000 - mae: 620369216.0000 - val_loss: 1075907999345672192.0000 - val_mae: 553381952.0000\n",
      "Epoch 650/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6409858670573649920.0000 - mae: 624870208.0000\n",
      "Epoch 650: val_loss improved from 1075907999345672192.00000 to 1075288080946036736.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch650-val_Loss1075288080946036736.000-mae620206464.000.h5\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 6429357409780629504.0000 - mae: 620206464.0000 - val_loss: 1075288080946036736.0000 - val_mae: 553065216.0000\n",
      "Epoch 651/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6498004318749196288.0000 - mae: 634060160.0000\n",
      "Epoch 651: val_loss improved from 1075288080946036736.00000 to 1074636070550765568.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch651-val_Loss1074636070550765568.000-mae619939968.000.h5\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 6427940139292426240.0000 - mae: 619939968.0000 - val_loss: 1074636070550765568.0000 - val_mae: 552748032.0000\n",
      "Epoch 652/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16023356674416836608.0000 - mae: 899376832.0000\n",
      "Epoch 652: val_loss improved from 1074636070550765568.00000 to 1074023436415664128.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch652-val_Loss1074023436415664128.000-mae619705280.000.h5\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 6426584441455378432.0000 - mae: 619705280.0000 - val_loss: 1074023436415664128.0000 - val_mae: 552470400.0000\n",
      "Epoch 653/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16067006186527916032.0000 - mae: 888711168.0000\n",
      "Epoch 653: val_loss improved from 1074023436415664128.00000 to 1073402074907017216.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch653-val_Loss1073402074907017216.000-mae619571648.000.h5\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 6424958813513711616.0000 - mae: 619571648.0000 - val_loss: 1073402074907017216.0000 - val_mae: 552183808.0000\n",
      "Epoch 654/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 312487389366845440.0000 - mae: 389659584.0000\n",
      "Epoch 654: val_loss improved from 1073402074907017216.00000 to 1072703610145472512.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch654-val_Loss1072703610145472512.000-mae619308160.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 6422733401979092992.0000 - mae: 619308160.0000 - val_loss: 1072703610145472512.0000 - val_mae: 551758912.0000\n",
      "Epoch 655/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6552209692242739200.0000 - mae: 670609216.0000\n",
      "Epoch 655: val_loss improved from 1072703610145472512.00000 to 1072130489709494272.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch655-val_Loss1072130489709494272.000-mae619057024.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 6421379903165300736.0000 - mae: 619057024.0000 - val_loss: 1072130489709494272.0000 - val_mae: 551486016.0000\n",
      "Epoch 656/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17379151268487888896.0000 - mae: 947491904.0000\n",
      "Epoch 656: val_loss improved from 1072130489709494272.00000 to 1071623683568566272.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch656-val_Loss1071623683568566272.000-mae619005376.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 6420459611932852224.0000 - mae: 619005376.0000 - val_loss: 1071623683568566272.0000 - val_mae: 551316608.0000\n",
      "Epoch 657/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 457608458783424512.0000 - mae: 447883264.0000\n",
      "Epoch 657: val_loss improved from 1071623683568566272.00000 to 1070954424584634368.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch657-val_Loss1070954424584634368.000-mae618790400.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6418001653688958976.0000 - mae: 618790400.0000 - val_loss: 1070954424584634368.0000 - val_mae: 550961920.0000\n",
      "Epoch 658/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6401778359621124096.0000 - mae: 622298304.0000\n",
      "Epoch 658: val_loss improved from 1070954424584634368.00000 to 1070430782171906048.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch658-val_Loss1070430782171906048.000-mae618643264.000.h5\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 6416559644189130752.0000 - mae: 618643264.0000 - val_loss: 1070430782171906048.0000 - val_mae: 550712000.0000\n",
      "Epoch 659/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6442930331069710336.0000 - mae: 634674944.0000\n",
      "Epoch 659: val_loss improved from 1070430782171906048.00000 to 1069837320770813952.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch659-val_Loss1069837320770813952.000-mae618473152.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 6415125881026510848.0000 - mae: 618473152.0000 - val_loss: 1069837320770813952.0000 - val_mae: 550439872.0000\n",
      "Epoch 660/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9947505996861538304.0000 - mae: 680005120.0000\n",
      "Epoch 660: val_loss improved from 1069837320770813952.00000 to 1069171222882811904.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch660-val_Loss1069171222882811904.000-mae618266432.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 6413377657538347008.0000 - mae: 618266432.0000 - val_loss: 1069171222882811904.0000 - val_mae: 550098816.0000\n",
      "Epoch 661/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10054422507546476544.0000 - mae: 699051904.0000\n",
      "Epoch 661: val_loss improved from 1069171222882811904.00000 to 1068531856871260160.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch661-val_Loss1068531856871260160.000-mae618017408.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 6411900463666429952.0000 - mae: 618017408.0000 - val_loss: 1068531856871260160.0000 - val_mae: 549788672.0000\n",
      "Epoch 662/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9835049047084236800.0000 - mae: 626352640.0000\n",
      "Epoch 662: val_loss improved from 1068531856871260160.00000 to 1067930217852436480.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch662-val_Loss1067930217852436480.000-mae617816064.000.h5\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 6410256693782904832.0000 - mae: 617816064.0000 - val_loss: 1067930217852436480.0000 - val_mae: 549513088.0000\n",
      "Epoch 663/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6505661317725028352.0000 - mae: 660391488.0000\n",
      "Epoch 663: val_loss improved from 1067930217852436480.00000 to 1067291401596698624.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch663-val_Loss1067291401596698624.000-mae617629888.000.h5\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 6408562346364502016.0000 - mae: 617629888.0000 - val_loss: 1067291401596698624.0000 - val_mae: 549197184.0000\n",
      "Epoch 664/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 341579023928786944.0000 - mae: 404445216.0000\n",
      "Epoch 664: val_loss improved from 1067291401596698624.00000 to 1066640284554625024.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch664-val_Loss1066640284554625024.000-mae617368000.000.h5\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 6406552439108927488.0000 - mae: 617368000.0000 - val_loss: 1066640284554625024.0000 - val_mae: 548830976.0000\n",
      "Epoch 665/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10113890693446369280.0000 - mae: 731845440.0000\n",
      "Epoch 665: val_loss improved from 1066640284554625024.00000 to 1066019404082315264.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch665-val_Loss1066019404082315264.000-mae617148352.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 6405467770888126464.0000 - mae: 617148352.0000 - val_loss: 1066019404082315264.0000 - val_mae: 548564736.0000\n",
      "Epoch 666/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 502238563467788288.0000 - mae: 441420448.0000\n",
      "Epoch 666: val_loss improved from 1066019404082315264.00000 to 1065438312187035648.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch666-val_Loss1065438312187035648.000-mae616953920.000.h5\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 6403494697272082432.0000 - mae: 616953920.0000 - val_loss: 1065438312187035648.0000 - val_mae: 548289664.0000\n",
      "Epoch 667/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11412051087012659200.0000 - mae: 771982592.0000\n",
      "Epoch 667: val_loss improved from 1065438312187035648.00000 to 1064834542864433152.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch667-val_Loss1064834542864433152.000-mae616804800.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 6402250050109440000.0000 - mae: 616804800.0000 - val_loss: 1064834542864433152.0000 - val_mae: 548031872.0000\n",
      "Epoch 668/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 283285786462257152.0000 - mae: 376077888.0000\n",
      "Epoch 668: val_loss improved from 1064834542864433152.00000 to 1064168376256954368.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch668-val_Loss1064168376256954368.000-mae616536384.000.h5\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 6400272578446884864.0000 - mae: 616536384.0000 - val_loss: 1064168376256954368.0000 - val_mae: 547653376.0000\n",
      "Epoch 669/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 399585134241644544.0000 - mae: 395025056.0000\n",
      "Epoch 669: val_loss improved from 1064168376256954368.00000 to 1063524268601507840.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch669-val_Loss1063524268601507840.000-mae616290496.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 6398710172423815168.0000 - mae: 616290496.0000 - val_loss: 1063524268601507840.0000 - val_mae: 547338688.0000\n",
      "Epoch 670/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10071760706404876288.0000 - mae: 746371136.0000\n",
      "Epoch 670: val_loss improved from 1063524268601507840.00000 to 1062942145914077184.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch670-val_Loss1062942145914077184.000-mae616115200.000.h5\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 6397523249621630976.0000 - mae: 616115200.0000 - val_loss: 1062942145914077184.0000 - val_mae: 547096320.0000\n",
      "Epoch 671/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6381099844437540864.0000 - mae: 587119296.0000\n",
      "Epoch 671: val_loss improved from 1062942145914077184.00000 to 1062358030361821184.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch671-val_Loss1062358030361821184.000-mae615968704.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6395413836563742720.0000 - mae: 615968704.0000 - val_loss: 1062358030361821184.0000 - val_mae: 546805824.0000\n",
      "Epoch 672/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10028660950107684864.0000 - mae: 699817024.0000\n",
      "Epoch 672: val_loss improved from 1062358030361821184.00000 to 1061747526530498560.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch672-val_Loss1061747526530498560.000-mae615775872.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6393967429017403392.0000 - mae: 615775872.0000 - val_loss: 1061747526530498560.0000 - val_mae: 546529216.0000\n",
      "Epoch 673/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 573333397636644864.0000 - mae: 502438336.0000\n",
      "Epoch 673: val_loss improved from 1061747526530498560.00000 to 1061071876635230208.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch673-val_Loss1061071876635230208.000-mae615542080.000.h5\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 6391967417366478848.0000 - mae: 615542080.0000 - val_loss: 1061071876635230208.0000 - val_mae: 546151872.0000\n",
      "Epoch 674/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 342014739771031552.0000 - mae: 403710016.0000\n",
      "Epoch 674: val_loss improved from 1061071876635230208.00000 to 1060401861737054208.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch674-val_Loss1060401861737054208.000-mae615269056.000.h5\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 6390225790948081664.0000 - mae: 615269056.0000 - val_loss: 1060401861737054208.0000 - val_mae: 545774016.0000\n",
      "Epoch 675/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 503763792253943808.0000 - mae: 441034496.0000\n",
      "Epoch 675: val_loss improved from 1060401861737054208.00000 to 1059794106684801024.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch675-val_Loss1059794106684801024.000-mae615090944.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6388764539994767360.0000 - mae: 615090944.0000 - val_loss: 1059794106684801024.0000 - val_mae: 545484480.0000\n",
      "Epoch 676/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9989043347135660032.0000 - mae: 697194368.0000\n",
      "Epoch 676: val_loss improved from 1059794106684801024.00000 to 1059243938554052608.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch676-val_Loss1059243938554052608.000-mae614929472.000.h5\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 6387501750890266624.0000 - mae: 614929472.0000 - val_loss: 1059243938554052608.0000 - val_mae: 545253952.0000\n",
      "Epoch 677/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7791755670688104448.0000 - mae: 713215808.0000\n",
      "Epoch 677: val_loss improved from 1059243938554052608.00000 to 1058719196629696512.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch677-val_Loss1058719196629696512.000-mae614787968.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 6385373096378892288.0000 - mae: 614787968.0000 - val_loss: 1058719196629696512.0000 - val_mae: 544996096.0000\n",
      "Epoch 678/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16006600117209530368.0000 - mae: 887655616.0000\n",
      "Epoch 678: val_loss improved from 1058719196629696512.00000 to 1058145182840520704.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch678-val_Loss1058145182840520704.000-mae614677504.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 6384284579867394048.0000 - mae: 614677504.0000 - val_loss: 1058145182840520704.0000 - val_mae: 544750016.0000\n",
      "Epoch 679/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 534101860684922880.0000 - mae: 454039872.0000\n",
      "Epoch 679: val_loss improved from 1058145182840520704.00000 to 1057426926869676032.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch679-val_Loss1057426926869676032.000-mae614458176.000.h5\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 6382018486402547712.0000 - mae: 614458176.0000 - val_loss: 1057426926869676032.0000 - val_mae: 544367680.0000\n",
      "Epoch 680/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 520806119405256704.0000 - mae: 462695488.0000\n",
      "Epoch 680: val_loss improved from 1057426926869676032.00000 to 1056743305515106304.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch680-val_Loss1056743305515106304.000-mae614185152.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 6380055858146967552.0000 - mae: 614185152.0000 - val_loss: 1056743305515106304.0000 - val_mae: 543980160.0000\n",
      "Epoch 681/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9894306126751596544.0000 - mae: 666744192.0000\n",
      "Epoch 681: val_loss improved from 1056743305515106304.00000 to 1056053293249200128.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch681-val_Loss1056053293249200128.000-mae613943808.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 6378865636809900032.0000 - mae: 613943808.0000 - val_loss: 1056053293249200128.0000 - val_mae: 543649024.0000\n",
      "Epoch 682/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1690951164423569408.0000 - mae: 446674720.0000\n",
      "Epoch 682: val_loss improved from 1056053293249200128.00000 to 1055522985047228416.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch682-val_Loss1055522985047228416.000-mae613740864.000.h5\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 6377060788472905728.0000 - mae: 613740864.0000 - val_loss: 1055522985047228416.0000 - val_mae: 543429824.0000\n",
      "Epoch 683/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9892940533309898752.0000 - mae: 635612032.0000\n",
      "Epoch 683: val_loss improved from 1055522985047228416.00000 to 1054919765480439808.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch683-val_Loss1054919765480439808.000-mae613641088.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6375694095519580160.0000 - mae: 613641088.0000 - val_loss: 1054919765480439808.0000 - val_mae: 543172032.0000\n",
      "Epoch 684/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 391717235191709696.0000 - mae: 392131904.0000\n",
      "Epoch 684: val_loss improved from 1054919765480439808.00000 to 1054277581970341888.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch684-val_Loss1054277581970341888.000-mae613436160.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 6373466484961705984.0000 - mae: 613436160.0000 - val_loss: 1054277581970341888.0000 - val_mae: 542824128.0000\n",
      "Epoch 685/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 306548446028890112.0000 - mae: 382178816.0000\n",
      "Epoch 685: val_loss improved from 1054277581970341888.00000 to 1053682402582331392.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch685-val_Loss1053682402582331392.000-mae613233856.000.h5\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 6372157516368838656.0000 - mae: 613233856.0000 - val_loss: 1053682402582331392.0000 - val_mae: 542545152.0000\n",
      "Epoch 686/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6267478511846424576.0000 - mae: 577177088.0000\n",
      "Epoch 686: val_loss improved from 1053682402582331392.00000 to 1053063514974846976.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch686-val_Loss1053063514974846976.000-mae613097920.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 6370618749845766144.0000 - mae: 613097920.0000 - val_loss: 1053063514974846976.0000 - val_mae: 542240640.0000\n",
      "Epoch 687/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 466993443361521664.0000 - mae: 444672000.0000\n",
      "Epoch 687: val_loss improved from 1053063514974846976.00000 to 1052461463639162880.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch687-val_Loss1052461463639162880.000-mae612879936.000.h5\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 6368866678066905088.0000 - mae: 612879936.0000 - val_loss: 1052461463639162880.0000 - val_mae: 541937408.0000\n",
      "Epoch 688/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1886769374967103488.0000 - mae: 528007744.0000\n",
      "Epoch 688: val_loss improved from 1052461463639162880.00000 to 1051810071719182336.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch688-val_Loss1051810071719182336.000-mae612704128.000.h5\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 6367074474113630208.0000 - mae: 612704128.0000 - val_loss: 1051810071719182336.0000 - val_mae: 541571840.0000\n",
      "Epoch 689/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1738791190226010112.0000 - mae: 495838688.0000\n",
      "Epoch 689: val_loss improved from 1051810071719182336.00000 to 1051245472498319360.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch689-val_Loss1051245472498319360.000-mae612541184.000.h5\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 6365703932869607424.0000 - mae: 612541184.0000 - val_loss: 1051245472498319360.0000 - val_mae: 541295680.0000\n",
      "Epoch 690/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 338222421087617024.0000 - mae: 365057856.0000\n",
      "Epoch 690: val_loss improved from 1051245472498319360.00000 to 1050574289369038848.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch690-val_Loss1050574289369038848.000-mae612375232.000.h5\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 6364094797602357248.0000 - mae: 612375232.0000 - val_loss: 1050574289369038848.0000 - val_mae: 540913152.0000\n",
      "Epoch 691/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17341229112445894656.0000 - mae: 983423424.0000\n",
      "Epoch 691: val_loss improved from 1050574289369038848.00000 to 1050004604906897408.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch691-val_Loss1050004604906897408.000-mae612267520.000.h5\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 6362993636707139584.0000 - mae: 612267520.0000 - val_loss: 1050004604906897408.0000 - val_mae: 540643776.0000\n",
      "Epoch 692/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9896464468076920832.0000 - mae: 671206784.0000\n",
      "Epoch 692: val_loss improved from 1050004604906897408.00000 to 1049288273081401344.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch692-val_Loss1049288273081401344.000-mae612038464.000.h5\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 6360962838730637312.0000 - mae: 612038464.0000 - val_loss: 1049288273081401344.0000 - val_mae: 540240640.0000\n",
      "Epoch 693/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 377655855861989376.0000 - mae: 389352640.0000\n",
      "Epoch 693: val_loss improved from 1049288273081401344.00000 to 1048624786533515264.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch693-val_Loss1048624786533515264.000-mae611816320.000.h5\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 6358952931475062784.0000 - mae: 611816320.0000 - val_loss: 1048624786533515264.0000 - val_mae: 539843328.0000\n",
      "Epoch 694/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6405747596597395456.0000 - mae: 629435328.0000\n",
      "Epoch 694: val_loss improved from 1048624786533515264.00000 to 1048007410754519040.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch694-val_Loss1048007410754519040.000-mae611624512.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 6357529613672906752.0000 - mae: 611624512.0000 - val_loss: 1048007410754519040.0000 - val_mae: 539512832.0000\n",
      "Epoch 695/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 324685302645915648.0000 - mae: 421287232.0000\n",
      "Epoch 695: val_loss improved from 1048007410754519040.00000 to 1047354781883957248.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch695-val_Loss1047354781883957248.000-mae611424128.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 6355831967719620608.0000 - mae: 611424128.0000 - val_loss: 1047354781883957248.0000 - val_mae: 539155264.0000\n",
      "Epoch 696/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11210801976222679040.0000 - mae: 723469312.0000\n",
      "Epoch 696: val_loss improved from 1047354781883957248.00000 to 1046774789500305408.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch696-val_Loss1046774789500305408.000-mae611326080.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 6354472971347689472.0000 - mae: 611326080.0000 - val_loss: 1046774789500305408.0000 - val_mae: 538882816.0000\n",
      "Epoch 697/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15871542705923293184.0000 - mae: 860579520.0000\n",
      "Epoch 697: val_loss improved from 1046774789500305408.00000 to 1046214107289616384.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch697-val_Loss1046214107289616384.000-mae611203072.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6353144761301336064.0000 - mae: 611203072.0000 - val_loss: 1046214107289616384.0000 - val_mae: 538623360.0000\n",
      "Epoch 698/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 416625571607347200.0000 - mae: 416020544.0000\n",
      "Epoch 698: val_loss improved from 1046214107289616384.00000 to 1045602503946665984.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch698-val_Loss1045602503946665984.000-mae611101056.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6350666462092328960.0000 - mae: 611101056.0000 - val_loss: 1045602503946665984.0000 - val_mae: 538271360.0000\n",
      "Epoch 699/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 381795860737949696.0000 - mae: 373625536.0000\n",
      "Epoch 699: val_loss improved from 1045602503946665984.00000 to 1044913178875527168.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch699-val_Loss1044913178875527168.000-mae610904960.000.h5\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 6349040834150662144.0000 - mae: 610904960.0000 - val_loss: 1044913178875527168.0000 - val_mae: 537867904.0000\n",
      "Epoch 700/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 404253179576844288.0000 - mae: 438062144.0000\n",
      "Epoch 700: val_loss improved from 1044913178875527168.00000 to 1044159669813116928.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch700-val_Loss1044159669813116928.000-mae610671296.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 6347535052976422912.0000 - mae: 610671296.0000 - val_loss: 1044159669813116928.0000 - val_mae: 537427456.0000\n",
      "Epoch 701/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1885609252760846336.0000 - mae: 497571616.0000\n",
      "Epoch 701: val_loss improved from 1044159669813116928.00000 to 1043618435214344192.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch701-val_Loss1043618435214344192.000-mae610496576.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6345952855744053248.0000 - mae: 610496576.0000 - val_loss: 1043618435214344192.0000 - val_mae: 537222272.0000\n",
      "Epoch 702/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 498298807147036672.0000 - mae: 437171648.0000\n",
      "Epoch 702: val_loss improved from 1043618435214344192.00000 to 1043008618577788928.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch702-val_Loss1043008618577788928.000-mae610382016.000.h5\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 6343944048000106496.0000 - mae: 610382016.0000 - val_loss: 1043008618577788928.0000 - val_mae: 536951680.0000\n",
      "Epoch 703/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17153435824956637184.0000 - mae: 949668992.0000\n",
      "Epoch 703: val_loss improved from 1043008618577788928.00000 to 1042409109862744064.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch703-val_Loss1042409109862744064.000-mae610289920.000.h5\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 6343022107500216320.0000 - mae: 610289920.0000 - val_loss: 1042409109862744064.0000 - val_mae: 536753536.0000\n",
      "Epoch 704/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6237322206431412224.0000 - mae: 577480832.0000\n",
      "Epoch 704: val_loss improved from 1042409109862744064.00000 to 1041741775024160768.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch704-val_Loss1041741775024160768.000-mae610166336.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 6340757113546997760.0000 - mae: 610166336.0000 - val_loss: 1041741775024160768.0000 - val_mae: 536451232.0000\n",
      "Epoch 705/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6420132507223588864.0000 - mae: 686856832.0000\n",
      "Epoch 705: val_loss improved from 1041741775024160768.00000 to 1041008400768434176.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch705-val_Loss1041008400768434176.000-mae610034304.000.h5\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 6338697728268173312.0000 - mae: 610034304.0000 - val_loss: 1041008400768434176.0000 - val_mae: 536071680.0000\n",
      "Epoch 706/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 269463619631054848.0000 - mae: 359479936.0000\n",
      "Epoch 706: val_loss improved from 1041008400768434176.00000 to 1040261213897883648.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch706-val_Loss1040261213897883648.000-mae609802560.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6336873088721879040.0000 - mae: 609802560.0000 - val_loss: 1040261213897883648.0000 - val_mae: 535685088.0000\n",
      "Epoch 707/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11321233625581617152.0000 - mae: 798740736.0000\n",
      "Epoch 707: val_loss improved from 1040261213897883648.00000 to 1039627963919761408.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch707-val_Loss1039627963919761408.000-mae609633536.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 6335284294419742720.0000 - mae: 609633536.0000 - val_loss: 1039627963919761408.0000 - val_mae: 535430208.0000\n",
      "Epoch 708/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9853251462082068480.0000 - mae: 681602752.0000\n",
      "Epoch 708: val_loss improved from 1039627963919761408.00000 to 1038902973440196608.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch708-val_Loss1038902973440196608.000-mae609486464.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6333313419826954240.0000 - mae: 609486464.0000 - val_loss: 1038902973440196608.0000 - val_mae: 535067872.0000\n",
      "Epoch 709/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15832193383788445696.0000 - mae: 889163008.0000\n",
      "Epoch 709: val_loss improved from 1038902973440196608.00000 to 1038344696411193344.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch709-val_Loss1038344696411193344.000-mae609353344.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6331882955199217664.0000 - mae: 609353344.0000 - val_loss: 1038344696411193344.0000 - val_mae: 534896960.0000\n",
      "Epoch 710/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15758235833657720832.0000 - mae: 891033600.0000\n",
      "Epoch 710: val_loss improved from 1038344696411193344.00000 to 1037748280072601600.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch710-val_Loss1037748280072601600.000-mae609375104.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 6329895587932012544.0000 - mae: 609375104.0000 - val_loss: 1037748280072601600.0000 - val_mae: 534675840.0000\n",
      "Epoch 711/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11407377063082983424.0000 - mae: 790531584.0000\n",
      "Epoch 711: val_loss improved from 1037748280072601600.00000 to 1037152619648253952.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch711-val_Loss1037152619648253952.000-mae609208000.000.h5\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 6327521192571830272.0000 - mae: 609208000.0000 - val_loss: 1037152619648253952.0000 - val_mae: 534446272.0000\n",
      "Epoch 712/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1827823319651450880.0000 - mae: 494368512.0000\n",
      "Epoch 712: val_loss improved from 1037152619648253952.00000 to 1036501502606180352.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch712-val_Loss1036501502606180352.000-mae609083264.000.h5\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 6325135252339556352.0000 - mae: 609083264.0000 - val_loss: 1036501502606180352.0000 - val_mae: 534122688.0000\n",
      "Epoch 713/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 293872245191737344.0000 - mae: 378760576.0000\n",
      "Epoch 713: val_loss improved from 1036501502606180352.00000 to 1035890174141136896.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch713-val_Loss1035890174141136896.000-mae608907456.000.h5\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 6323312262060703744.0000 - mae: 608907456.0000 - val_loss: 1035890174141136896.0000 - val_mae: 533841504.0000\n",
      "Epoch 714/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 256746444805898240.0000 - mae: 359026144.0000\n",
      "Epoch 714: val_loss improved from 1035890174141136896.00000 to 1035256786724061184.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch714-val_Loss1035256786724061184.000-mae608822912.000.h5\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 6321421102060929024.0000 - mae: 608822912.0000 - val_loss: 1035256786724061184.0000 - val_mae: 533541984.0000\n",
      "Epoch 715/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1951199381965242368.0000 - mae: 560973056.0000\n",
      "Epoch 715: val_loss improved from 1035256786724061184.00000 to 1034635768812797952.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch715-val_Loss1034635768812797952.000-mae608630336.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 6319445279665815552.0000 - mae: 608630336.0000 - val_loss: 1034635768812797952.0000 - val_mae: 533260544.0000\n",
      "Epoch 716/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9766611045324947456.0000 - mae: 631639552.0000\n",
      "Epoch 716: val_loss improved from 1034635768812797952.00000 to 1034057219538157568.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch716-val_Loss1034057219538157568.000-mae608558976.000.h5\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 6318060444770631680.0000 - mae: 608558976.0000 - val_loss: 1034057219538157568.0000 - val_mae: 533038528.0000\n",
      "Epoch 717/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 435615408729358336.0000 - mae: 423489152.0000\n",
      "Epoch 717: val_loss improved from 1034057219538157568.00000 to 1033419846391431168.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch717-val_Loss1033419846391431168.000-mae608438208.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 6315490336340705280.0000 - mae: 608438208.0000 - val_loss: 1033419846391431168.0000 - val_mae: 532735264.0000\n",
      "Epoch 718/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15688791778759016448.0000 - mae: 859486464.0000\n",
      "Epoch 718: val_loss improved from 1033419846391431168.00000 to 1032851124001964032.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch718-val_Loss1032851124001964032.000-mae608388032.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6314432606154784768.0000 - mae: 608388032.0000 - val_loss: 1032851124001964032.0000 - val_mae: 532528384.0000\n",
      "Epoch 719/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6339378875721580544.0000 - mae: 610992576.0000\n",
      "Epoch 719: val_loss improved from 1032851124001964032.00000 to 1032310851475865600.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch719-val_Loss1032310851475865600.000-mae608343360.000.h5\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 6311985643027169280.0000 - mae: 608343360.0000 - val_loss: 1032310851475865600.0000 - val_mae: 532312640.0000\n",
      "Epoch 720/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 435387569304240128.0000 - mae: 422916096.0000\n",
      "Epoch 720: val_loss improved from 1032310851475865600.00000 to 1031672310098034688.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch720-val_Loss1031672310098034688.000-mae608211968.000.h5\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 6309806960736731136.0000 - mae: 608211968.0000 - val_loss: 1031672310098034688.0000 - val_mae: 531998016.0000\n",
      "Epoch 721/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11362679716390633472.0000 - mae: 786136640.0000\n",
      "Epoch 721: val_loss improved from 1031672310098034688.00000 to 1031063111936770048.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch721-val_Loss1031063111936770048.000-mae608113792.000.h5\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 6308117011364839424.0000 - mae: 608113792.0000 - val_loss: 1031063111936770048.0000 - val_mae: 531730368.0000\n",
      "Epoch 722/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6431797226082664448.0000 - mae: 644862976.0000\n",
      "Epoch 722: val_loss improved from 1031063111936770048.00000 to 1030445255121436672.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch722-val_Loss1030445255121436672.000-mae608024640.000.h5\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 6306230249411575808.0000 - mae: 608024640.0000 - val_loss: 1030445255121436672.0000 - val_mae: 531437312.0000\n",
      "Epoch 723/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 556110956736806912.0000 - mae: 460638784.0000\n",
      "Epoch 723: val_loss improved from 1030445255121436672.00000 to 1029754005904949248.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch723-val_Loss1029754005904949248.000-mae607838848.000.h5\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 6303879493551390720.0000 - mae: 607838848.0000 - val_loss: 1029754005904949248.0000 - val_mae: 531056832.0000\n",
      "Epoch 724/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15758012632797282304.0000 - mae: 879150976.0000\n",
      "Epoch 724: val_loss improved from 1029754005904949248.00000 to 1029234349221871616.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch724-val_Loss1029234349221871616.000-mae607744192.000.h5\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 6302832758481747968.0000 - mae: 607744192.0000 - val_loss: 1029234349221871616.0000 - val_mae: 530874528.0000\n",
      "Epoch 725/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6282143797937700864.0000 - mae: 617097920.0000\n",
      "Epoch 725: val_loss improved from 1029234349221871616.00000 to 1028648653121650688.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch725-val_Loss1028648653121650688.000-mae607648448.000.h5\n",
      "3/3 [==============================] - 0s 162ms/step - loss: 6300250555423916032.0000 - mae: 607648448.0000 - val_loss: 1028648653121650688.0000 - val_mae: 530568672.0000\n",
      "Epoch 726/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15771205672818966528.0000 - mae: 924360704.0000\n",
      "Epoch 726: val_loss improved from 1028648653121650688.00000 to 1028075120368812032.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch726-val_Loss1028075120368812032.000-mae607534528.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 6298954231214768128.0000 - mae: 607534528.0000 - val_loss: 1028075120368812032.0000 - val_mae: 530333024.0000\n",
      "Epoch 727/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15757624505192677376.0000 - mae: 931032192.0000\n",
      "Epoch 727: val_loss improved from 1028075120368812032.00000 to 1027457057395048448.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch727-val_Loss1027457057395048448.000-mae607473472.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6296903642028965888.0000 - mae: 607473472.0000 - val_loss: 1027457057395048448.0000 - val_mae: 530042784.0000\n",
      "Epoch 728/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 478181455210479616.0000 - mae: 448737856.0000\n",
      "Epoch 728: val_loss improved from 1027457057395048448.00000 to 1026866413492502528.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch728-val_Loss1026866413492502528.000-mae607282496.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6294147166378131456.0000 - mae: 607282496.0000 - val_loss: 1026866413492502528.0000 - val_mae: 529728032.0000\n",
      "Epoch 729/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6305811335481393152.0000 - mae: 616462208.0000\n",
      "Epoch 729: val_loss improved from 1026866413492502528.00000 to 1026300439882104832.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch729-val_Loss1026300439882104832.000-mae607169856.000.h5\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 6292551225250414592.0000 - mae: 607169856.0000 - val_loss: 1026300439882104832.0000 - val_mae: 529459872.0000\n",
      "Epoch 730/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 273028253508173824.0000 - mae: 377712928.0000\n",
      "Epoch 730: val_loss improved from 1026300439882104832.00000 to 1025697082876362752.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch730-val_Loss1025697082876362752.000-mae607080640.000.h5\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 6290429717564620800.0000 - mae: 607080640.0000 - val_loss: 1025697082876362752.0000 - val_mae: 529170368.0000\n",
      "Epoch 731/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 274220605149020160.0000 - mae: 369988352.0000\n",
      "Epoch 731: val_loss improved from 1025697082876362752.00000 to 1025132208777592832.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch731-val_Loss1025132208777592832.000-mae606975552.000.h5\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 6288817283762487296.0000 - mae: 606975552.0000 - val_loss: 1025132208777592832.0000 - val_mae: 528918208.0000\n",
      "Epoch 732/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17167084062792220672.0000 - mae: 958107584.0000\n",
      "Epoch 732: val_loss improved from 1025132208777592832.00000 to 1024677973036367872.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch732-val_Loss1024677973036367872.000-mae607027264.000.h5\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 6287511063948689408.0000 - mae: 607027264.0000 - val_loss: 1024677973036367872.0000 - val_mae: 528795936.0000\n",
      "Epoch 733/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15598193120141901824.0000 - mae: 855877568.0000\n",
      "Epoch 733: val_loss improved from 1024677973036367872.00000 to 1024093513886728192.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch733-val_Loss1024093513886728192.000-mae607016448.000.h5\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 6285285652414070784.0000 - mae: 607016448.0000 - val_loss: 1024093513886728192.0000 - val_mae: 528520352.0000\n",
      "Epoch 734/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 600426532376150016.0000 - mae: 475031712.0000\n",
      "Epoch 734: val_loss improved from 1024093513886728192.00000 to 1023461569578663936.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch734-val_Loss1023461569578663936.000-mae606892224.000.h5\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 6282520380670214144.0000 - mae: 606892224.0000 - val_loss: 1023461569578663936.0000 - val_mae: 528160704.0000\n",
      "Epoch 735/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9836946804153778176.0000 - mae: 674424320.0000\n",
      "Epoch 735: val_loss improved from 1023461569578663936.00000 to 1022886181399953408.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch735-val_Loss1022886181399953408.000-mae606825280.000.h5\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 6281172379414560768.0000 - mae: 606825280.0000 - val_loss: 1022886181399953408.0000 - val_mae: 527900384.0000\n",
      "Epoch 736/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1990807089332617216.0000 - mae: 536737792.0000\n",
      "Epoch 736: val_loss improved from 1022886181399953408.00000 to 1022316977974149120.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch736-val_Loss1022316977974149120.000-mae606760768.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 6278501665670692864.0000 - mae: 606760768.0000 - val_loss: 1022316977974149120.0000 - val_mae: 527621248.0000\n",
      "Epoch 737/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9673313085661642752.0000 - mae: 632792704.0000\n",
      "Epoch 737: val_loss improved from 1022316977974149120.00000 to 1021818143292522496.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch737-val_Loss1021818143292522496.000-mae606797760.000.h5\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 6277150915635970048.0000 - mae: 606797760.0000 - val_loss: 1021818143292522496.0000 - val_mae: 527510496.0000\n",
      "Epoch 738/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1748609279306235904.0000 - mae: 471113728.0000\n",
      "Epoch 738: val_loss improved from 1021818143292522496.00000 to 1021349682619613184.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch738-val_Loss1021349682619613184.000-mae606786688.000.h5\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 6275064592322265088.0000 - mae: 606786688.0000 - val_loss: 1021349682619613184.0000 - val_mae: 527398272.0000\n",
      "Epoch 739/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9906742702773370880.0000 - mae: 702683136.0000\n",
      "Epoch 739: val_loss improved from 1021349682619613184.00000 to 1020755327865323520.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch739-val_Loss1020755327865323520.000-mae606854912.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 6273125053810868224.0000 - mae: 606854912.0000 - val_loss: 1020755327865323520.0000 - val_mae: 527180448.0000\n",
      "Epoch 740/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1921426668669370368.0000 - mae: 536434112.0000\n",
      "Epoch 740: val_loss improved from 1020755327865323520.00000 to 1020238626119745536.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch740-val_Loss1020238626119745536.000-mae606820928.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 6270617067787911168.0000 - mae: 606820928.0000 - val_loss: 1020238626119745536.0000 - val_mae: 526972896.0000\n",
      "Epoch 741/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 426151980868567040.0000 - mae: 405676480.0000\n",
      "Epoch 741: val_loss improved from 1020238626119745536.00000 to 1019678493664870400.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch741-val_Loss1019678493664870400.000-mae606763136.000.h5\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 6268715462427672576.0000 - mae: 606763136.0000 - val_loss: 1019678493664870400.0000 - val_mae: 526752992.0000\n",
      "Epoch 742/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1849531252596539392.0000 - mae: 512055712.0000\n",
      "Epoch 742: val_loss improved from 1019678493664870400.00000 to 1019196357816090624.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch742-val_Loss1019196357816090624.000-mae606795776.000.h5\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 6266883676055797760.0000 - mae: 606795776.0000 - val_loss: 1019196357816090624.0000 - val_mae: 526627712.0000\n",
      "Epoch 743/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9666917226522869760.0000 - mae: 622105088.0000\n",
      "Epoch 743: val_loss improved from 1019196357816090624.00000 to 1018723018060333056.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch743-val_Loss1018723018060333056.000-mae606956864.000.h5\n",
      "3/3 [==============================] - 0s 163ms/step - loss: 6265421325590855680.0000 - mae: 606956864.0000 - val_loss: 1018723018060333056.0000 - val_mae: 526529856.0000\n",
      "Epoch 744/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1864141837984333824.0000 - mae: 525929184.0000\n",
      "Epoch 744: val_loss improved from 1018723018060333056.00000 to 1018175048952840192.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch744-val_Loss1018175048952840192.000-mae607114496.000.h5\n",
      "3/3 [==============================] - 0s 154ms/step - loss: 6262698385044668416.0000 - mae: 607114496.0000 - val_loss: 1018175048952840192.0000 - val_mae: 526311296.0000\n",
      "Epoch 745/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9764209711929884672.0000 - mae: 673025216.0000\n",
      "Epoch 745: val_loss improved from 1018175048952840192.00000 to 1017680543598247936.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch745-val_Loss1017680543598247936.000-mae607155136.000.h5\n",
      "3/3 [==============================] - 0s 159ms/step - loss: 6260976000079757312.0000 - mae: 607155136.0000 - val_loss: 1017680543598247936.0000 - val_mae: 526233920.0000\n",
      "Epoch 746/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1733568509994074112.0000 - mae: 484812352.0000\n",
      "Epoch 746: val_loss improved from 1017680543598247936.00000 to 1017247267297427456.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch746-val_Loss1017247267297427456.000-mae607295296.000.h5\n",
      "3/3 [==============================] - 0s 163ms/step - loss: 6258982585498599424.0000 - mae: 607295296.0000 - val_loss: 1017247267297427456.0000 - val_mae: 526174528.0000\n",
      "Epoch 747/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15639308257950957568.0000 - mae: 876497472.0000\n",
      "Epoch 747: val_loss improved from 1017247267297427456.00000 to 1016787396559110144.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch747-val_Loss1016787396559110144.000-mae607470784.000.h5\n",
      "3/3 [==============================] - 0s 165ms/step - loss: 6257566964277837824.0000 - mae: 607470784.0000 - val_loss: 1016787396559110144.0000 - val_mae: 526121600.0000\n",
      "Epoch 748/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6262812734253957120.0000 - mae: 626971200.0000\n",
      "Epoch 748: val_loss improved from 1016787396559110144.00000 to 1016295983580971008.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch748-val_Loss1016295983580971008.000-mae607532608.000.h5\n",
      "3/3 [==============================] - 0s 183ms/step - loss: 6255071622638600192.0000 - mae: 607532608.0000 - val_loss: 1016295983580971008.0000 - val_mae: 526015008.0000\n",
      "Epoch 749/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11223947737244368896.0000 - mae: 795925248.0000\n",
      "Epoch 749: val_loss improved from 1016295983580971008.00000 to 1015717365586853888.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch749-val_Loss1015717365586853888.000-mae607565632.000.h5\n",
      "3/3 [==============================] - 0s 192ms/step - loss: 6252811576487706624.0000 - mae: 607565632.0000 - val_loss: 1015717365586853888.0000 - val_mae: 525822624.0000\n",
      "Epoch 750/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16948591311143829504.0000 - mae: 939853056.0000\n",
      "Epoch 750: val_loss improved from 1015717365586853888.00000 to 1015310821162483712.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch750-val_Loss1015310821162483712.000-mae607691968.000.h5\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 6251686226336677888.0000 - mae: 607691968.0000 - val_loss: 1015310821162483712.0000 - val_mae: 525832224.0000\n",
      "Epoch 751/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9793668926972887040.0000 - mae: 668244672.0000\n",
      "Epoch 751: val_loss improved from 1015310821162483712.00000 to 1014695506967789568.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch751-val_Loss1014695506967789568.000-mae607729408.000.h5\n",
      "3/3 [==============================] - 0s 161ms/step - loss: 6249011114546298880.0000 - mae: 607729408.0000 - val_loss: 1014695506967789568.0000 - val_mae: 525608384.0000\n",
      "Epoch 752/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 293675157732458496.0000 - mae: 366771424.0000\n",
      "Epoch 752: val_loss improved from 1014695506967789568.00000 to 1014118194643730432.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch752-val_Loss1014118194643730432.000-mae607601024.000.h5\n",
      "3/3 [==============================] - 0s 153ms/step - loss: 6246718083046572032.0000 - mae: 607601024.0000 - val_loss: 1014118194643730432.0000 - val_mae: 525349024.0000\n",
      "Epoch 753/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 414876832723107840.0000 - mae: 423037824.0000\n",
      "Epoch 753: val_loss improved from 1014118194643730432.00000 to 1013625201117626368.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch753-val_Loss1013625201117626368.000-mae607601344.000.h5\n",
      "3/3 [==============================] - 0s 208ms/step - loss: 6245024285383983104.0000 - mae: 607601344.0000 - val_loss: 1013625201117626368.0000 - val_mae: 525249920.0000\n",
      "Epoch 754/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1756044589250117632.0000 - mae: 460108416.0000\n",
      "Epoch 754: val_loss improved from 1013625201117626368.00000 to 1013208692369129472.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch754-val_Loss1013208692369129472.000-mae607635072.000.h5\n",
      "3/3 [==============================] - 0s 179ms/step - loss: 6242782930930761728.0000 - mae: 607635072.0000 - val_loss: 1013208692369129472.0000 - val_mae: 525184448.0000\n",
      "Epoch 755/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1930754238124654592.0000 - mae: 580308544.0000\n",
      "Epoch 755: val_loss improved from 1013208692369129472.00000 to 1012806064954933248.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch755-val_Loss1012806064954933248.000-mae607790528.000.h5\n",
      "3/3 [==============================] - 0s 179ms/step - loss: 6241493753547194368.0000 - mae: 607790528.0000 - val_loss: 1012806064954933248.0000 - val_mae: 525188864.0000\n",
      "Epoch 756/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17071613468152430592.0000 - mae: 990184064.0000\n",
      "Epoch 756: val_loss improved from 1012806064954933248.00000 to 1012448998553812992.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch756-val_Loss1012448998553812992.000-mae608060608.000.h5\n",
      "3/3 [==============================] - 0s 175ms/step - loss: 6240148501070610432.0000 - mae: 608060608.0000 - val_loss: 1012448998553812992.0000 - val_mae: 525265856.0000\n",
      "Epoch 757/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7681651126038429696.0000 - mae: 732531840.0000\n",
      "Epoch 757: val_loss improved from 1012448998553812992.00000 to 1012009743658516480.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch757-val_Loss1012009743658516480.000-mae608263552.000.h5\n",
      "3/3 [==============================] - 0s 159ms/step - loss: 6237331002524434432.0000 - mae: 608263552.0000 - val_loss: 1012009743658516480.0000 - val_mae: 525168480.0000\n",
      "Epoch 758/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6080934269566320640.0000 - mae: 583176960.0000\n",
      "Epoch 758: val_loss improved from 1012009743658516480.00000 to 1011553583771942912.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch758-val_Loss1011553583771942912.000-mae608309760.000.h5\n",
      "3/3 [==============================] - 0s 190ms/step - loss: 6235690531175792640.0000 - mae: 608309760.0000 - val_loss: 1011553583771942912.0000 - val_mae: 525103520.0000\n",
      "Epoch 759/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9686709535334465536.0000 - mae: 663930240.0000\n",
      "Epoch 759: val_loss improved from 1011553583771942912.00000 to 1010980600774918144.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch759-val_Loss1010980600774918144.000-mae608434752.000.h5\n",
      "3/3 [==============================] - 0s 191ms/step - loss: 6233719106827190272.0000 - mae: 608434752.0000 - val_loss: 1010980600774918144.0000 - val_mae: 524937248.0000\n",
      "Epoch 760/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11429418972685008896.0000 - mae: 850628032.0000\n",
      "Epoch 760: val_loss improved from 1010980600774918144.00000 to 1010419781125275648.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch760-val_Loss1010419781125275648.000-mae608493248.000.h5\n",
      "3/3 [==============================] - 0s 183ms/step - loss: 6231533277711171584.0000 - mae: 608493248.0000 - val_loss: 1010419781125275648.0000 - val_mae: 524764256.0000\n",
      "Epoch 761/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15450285716461846528.0000 - mae: 849164800.0000\n",
      "Epoch 761: val_loss improved from 1010419781125275648.00000 to 1009974204038119424.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch761-val_Loss1009974204038119424.000-mae608679744.000.h5\n",
      "3/3 [==============================] - 0s 190ms/step - loss: 6230328762722942976.0000 - mae: 608679744.0000 - val_loss: 1009974204038119424.0000 - val_mae: 524698528.0000\n",
      "Epoch 762/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11260435030612115456.0000 - mae: 791554560.0000\n",
      "Epoch 762: val_loss improved from 1009974204038119424.00000 to 1009512821471313920.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch762-val_Loss1009512821471313920.000-mae608773504.000.h5\n",
      "3/3 [==============================] - 0s 184ms/step - loss: 6227969760525549568.0000 - mae: 608773504.0000 - val_loss: 1009512821471313920.0000 - val_mae: 524593696.0000\n",
      "Epoch 763/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1993384894343938048.0000 - mae: 552796544.0000\n",
      "Epoch 763: val_loss improved from 1009512821471313920.00000 to 1008982581988818944.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch763-val_Loss1008982581988818944.000-mae608844992.000.h5\n",
      "3/3 [==============================] - 0s 168ms/step - loss: 6225888934769983488.0000 - mae: 608844992.0000 - val_loss: 1008982581988818944.0000 - val_mae: 524389504.0000\n",
      "Epoch 764/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6074258584718278656.0000 - mae: 577844480.0000\n",
      "Epoch 764: val_loss improved from 1008982581988818944.00000 to 1008461001160392704.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch764-val_Loss1008461001160392704.000-mae608865216.000.h5\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 6224323230212030464.0000 - mae: 608865216.0000 - val_loss: 1008461001160392704.0000 - val_mae: 524203200.0000\n",
      "Epoch 765/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 281197951320064000.0000 - mae: 376875776.0000\n",
      "Epoch 765: val_loss improved from 1008461001160392704.00000 to 1007935778199699456.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch765-val_Loss1007935778199699456.000-mae608860736.000.h5\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 6222288583944830976.0000 - mae: 608860736.0000 - val_loss: 1007935778199699456.0000 - val_mae: 524007136.0000\n",
      "Epoch 766/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 301043603625476096.0000 - mae: 360002688.0000\n",
      "Epoch 766: val_loss improved from 1007935778199699456.00000 to 1007369392272441344.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch766-val_Loss1007369392272441344.000-mae608815360.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 6220303965456695296.0000 - mae: 608815360.0000 - val_loss: 1007369392272441344.0000 - val_mae: 523747296.0000\n",
      "Epoch 767/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 305601457279729664.0000 - mae: 401971168.0000\n",
      "Epoch 767: val_loss improved from 1007369392272441344.00000 to 1006812901949833216.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch767-val_Loss1006812901949833216.000-mae608814976.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 6218602471212711936.0000 - mae: 608814976.0000 - val_loss: 1006812901949833216.0000 - val_mae: 523510368.0000\n",
      "Epoch 768/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1745589608059502592.0000 - mae: 453958368.0000\n",
      "Epoch 768: val_loss improved from 1006812901949833216.00000 to 1006360452915003392.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch768-val_Loss1006360452915003392.000-mae608829120.000.h5\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 6216715709259448320.0000 - mae: 608829120.0000 - val_loss: 1006360452915003392.0000 - val_mae: 523401504.0000\n",
      "Epoch 769/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 361285295955509248.0000 - mae: 427355712.0000\n",
      "Epoch 769: val_loss improved from 1006360452915003392.00000 to 1005805611859836928.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch769-val_Loss1005805611859836928.000-mae608921664.000.h5\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 6215019712573603840.0000 - mae: 608921664.0000 - val_loss: 1005805611859836928.0000 - val_mae: 523199328.0000\n",
      "Epoch 770/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6071000182009364480.0000 - mae: 589581568.0000\n",
      "Epoch 770: val_loss improved from 1005805611859836928.00000 to 1005400922861338624.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch770-val_Loss1005400922861338624.000-mae609028480.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 6213293479317995520.0000 - mae: 609028480.0000 - val_loss: 1005400922861338624.0000 - val_mae: 523175904.0000\n",
      "Epoch 771/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 253386972926574592.0000 - mae: 366557760.0000\n",
      "Epoch 771: val_loss improved from 1005400922861338624.00000 to 1004868346916634624.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch771-val_Loss1004868346916634624.000-mae609141248.000.h5\n",
      "3/3 [==============================] - 0s 151ms/step - loss: 6211169772608946176.0000 - mae: 609141248.0000 - val_loss: 1004868346916634624.0000 - val_mae: 523021184.0000\n",
      "Epoch 772/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1848474621922246656.0000 - mae: 493808928.0000\n",
      "Epoch 772: val_loss improved from 1004868346916634624.00000 to 1004420158489362432.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch772-val_Loss1004420158489362432.000-mae609225792.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 6209220338492899328.0000 - mae: 609225792.0000 - val_loss: 1004420158489362432.0000 - val_mae: 522963264.0000\n",
      "Epoch 773/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7714710142150770688.0000 - mae: 725499584.0000\n",
      "Epoch 773: val_loss improved from 1004420158489362432.00000 to 1004043300878942208.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch773-val_Loss1004043300878942208.000-mae609520640.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6208120277109309440.0000 - mae: 609520640.0000 - val_loss: 1004043300878942208.0000 - val_mae: 523002560.0000\n",
      "Epoch 774/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6290610037471576064.0000 - mae: 631394048.0000\n",
      "Epoch 774: val_loss improved from 1004043300878942208.00000 to 1003485642325229568.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch774-val_Loss1003485642325229568.000-mae609660736.000.h5\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 6206022958679326720.0000 - mae: 609660736.0000 - val_loss: 1003485642325229568.0000 - val_mae: 522853472.0000\n",
      "Epoch 775/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 342357890478112768.0000 - mae: 393095488.0000\n",
      "Epoch 775: val_loss improved from 1003485642325229568.00000 to 1002931557184307200.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch775-val_Loss1002931557184307200.000-mae609654976.000.h5\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 6203497380470325248.0000 - mae: 609654976.0000 - val_loss: 1002931557184307200.0000 - val_mae: 522641216.0000\n",
      "Epoch 776/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 232481906808061952.0000 - mae: 368013408.0000\n",
      "Epoch 776: val_loss improved from 1002931557184307200.00000 to 1002340913281761280.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch776-val_Loss1002340913281761280.000-mae609669696.000.h5\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 6201789838912389120.0000 - mae: 609669696.0000 - val_loss: 1002340913281761280.0000 - val_mae: 522419616.0000\n",
      "Epoch 777/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16920515281728569344.0000 - mae: 977405184.0000\n",
      "Epoch 777: val_loss improved from 1002340913281761280.00000 to 1001958626832678912.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch777-val_Loss1001958626832678912.000-mae609809024.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 6200909679854354432.0000 - mae: 609809024.0000 - val_loss: 1001958626832678912.0000 - val_mae: 522421856.0000\n",
      "Epoch 778/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6082689090124251136.0000 - mae: 593316224.0000\n",
      "Epoch 778: val_loss improved from 1001958626832678912.00000 to 1001439863502798848.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch778-val_Loss1001439863502798848.000-mae609923840.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 6198166948098867200.0000 - mae: 609923840.0000 - val_loss: 1001439863502798848.0000 - val_mae: 522222112.0000\n",
      "Epoch 779/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9558539464884617216.0000 - mae: 634639296.0000\n",
      "Epoch 779: val_loss improved from 1001439863502798848.00000 to 1000904538779025408.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch779-val_Loss1000904538779025408.000-mae609925632.000.h5\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 6196426421192097792.0000 - mae: 609925632.0000 - val_loss: 1000904538779025408.0000 - val_mae: 522052416.0000\n",
      "Epoch 780/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1861764281528221696.0000 - mae: 539537216.0000\n",
      "Epoch 780: val_loss improved from 1000904538779025408.00000 to 1000394640261644288.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch780-val_Loss1000394640261644288.000-mae609945920.000.h5\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 6194301065215606784.0000 - mae: 609945920.0000 - val_loss: 1000394640261644288.0000 - val_mae: 521868384.0000\n",
      "Epoch 781/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1892615753169895424.0000 - mae: 521392384.0000\n",
      "Epoch 781: val_loss improved from 1000394640261644288.00000 to 999951399636697088.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch781-val_Loss999951399636697088.000-mae610031808.000.h5\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 6192489619808845824.0000 - mae: 610031808.0000 - val_loss: 999951399636697088.0000 - val_mae: 521717728.0000\n",
      "Epoch 782/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7697275736024940544.0000 - mae: 720878016.0000\n",
      "Epoch 782: val_loss improved from 999951399636697088.00000 to 999600724146913280.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch782-val_Loss999600724146913280.000-mae610185280.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6191309843832242176.0000 - mae: 610185280.0000 - val_loss: 999600724146913280.0000 - val_mae: 521715424.0000\n",
      "Epoch 783/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 308209808098459648.0000 - mae: 391655360.0000\n",
      "Epoch 783: val_loss improved from 999600724146913280.00000 to 999047257481281536.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch783-val_Loss999047257481281536.000-mae610289600.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6188884870937182208.0000 - mae: 610289600.0000 - val_loss: 999047257481281536.0000 - val_mae: 521487200.0000\n",
      "Epoch 784/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6072269568183631872.0000 - mae: 603612608.0000\n",
      "Epoch 784: val_loss improved from 999047257481281536.00000 to 998678440049639424.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch784-val_Loss998678440049639424.000-mae610381120.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 6187290029321093120.0000 - mae: 610381120.0000 - val_loss: 998678440049639424.0000 - val_mae: 521416064.0000\n",
      "Epoch 785/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 384348651859738624.0000 - mae: 397072288.0000\n",
      "Epoch 785: val_loss improved from 998678440049639424.00000 to 998192318471208960.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch785-val_Loss998192318471208960.000-mae610457664.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 6185242738670174208.0000 - mae: 610457664.0000 - val_loss: 998192318471208960.0000 - val_mae: 521269728.0000\n",
      "Epoch 786/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 297300007051067392.0000 - mae: 378885760.0000\n",
      "Epoch 786: val_loss improved from 998192318471208960.00000 to 997677678309933056.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch786-val_Loss997677678309933056.000-mae610477376.000.h5\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 6183264717251805184.0000 - mae: 610477376.0000 - val_loss: 997677678309933056.0000 - val_mae: 521053024.0000\n",
      "Epoch 787/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11079672020470857728.0000 - mae: 771441216.0000\n",
      "Epoch 787: val_loss improved from 997677678309933056.00000 to 997298415517827072.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch787-val_Loss997298415517827072.000-mae610622272.000.h5\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 6182007975461257216.0000 - mae: 610622272.0000 - val_loss: 997298415517827072.0000 - val_mae: 521029024.0000\n",
      "Epoch 788/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 351562761307684864.0000 - mae: 423208256.0000\n",
      "Epoch 788: val_loss improved from 997298415517827072.00000 to 996948221064380416.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch788-val_Loss996948221064380416.000-mae610809664.000.h5\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 6180095924740554752.0000 - mae: 610809664.0000 - val_loss: 996948221064380416.0000 - val_mae: 520993824.0000\n",
      "Epoch 789/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6063526251719557120.0000 - mae: 595673856.0000\n",
      "Epoch 789: val_loss improved from 996948221064380416.00000 to 996566209493204992.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch789-val_Loss996566209493204992.000-mae610980800.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 6178410373415174144.0000 - mae: 610980800.0000 - val_loss: 996566209493204992.0000 - val_mae: 520922240.0000\n",
      "Epoch 790/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16888267705197527040.0000 - mae: 983297664.0000\n",
      "Epoch 790: val_loss improved from 996566209493204992.00000 to 996246870084812800.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch790-val_Loss996246870084812800.000-mae611224128.000.h5\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 6176934828810698752.0000 - mae: 611224128.0000 - val_loss: 996246870084812800.0000 - val_mae: 520919328.0000\n",
      "Epoch 791/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6083409270240444416.0000 - mae: 590682112.0000\n",
      "Epoch 791: val_loss improved from 996246870084812800.00000 to 995801018119749632.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch791-val_Loss995801018119749632.000-mae611329024.000.h5\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 6174629152927252480.0000 - mae: 611329024.0000 - val_loss: 995801018119749632.0000 - val_mae: 520764288.0000\n",
      "Epoch 792/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9646337667385786368.0000 - mae: 685054592.0000\n",
      "Epoch 792: val_loss improved from 995801018119749632.00000 to 995293593503531008.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch792-val_Loss995293593503531008.000-mae611374080.000.h5\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 6172977136706519040.0000 - mae: 611374080.0000 - val_loss: 995293593503531008.0000 - val_mae: 520569568.0000\n",
      "Epoch 793/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1802023416744640512.0000 - mae: 507967360.0000\n",
      "Epoch 793: val_loss improved from 995293593503531008.00000 to 994807128327716864.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch793-val_Loss994807128327716864.000-mae611378368.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6170948537753272320.0000 - mae: 611378368.0000 - val_loss: 994807128327716864.0000 - val_mae: 520350976.0000\n",
      "Epoch 794/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9589990994997149696.0000 - mae: 681741824.0000\n",
      "Epoch 794: val_loss improved from 994807128327716864.00000 to 994327260221669376.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch794-val_Loss994327260221669376.000-mae611386368.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 6169602185765060608.0000 - mae: 611386368.0000 - val_loss: 994327260221669376.0000 - val_mae: 520156544.0000\n",
      "Epoch 795/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7682411988084850688.0000 - mae: 749706752.0000\n",
      "Epoch 795: val_loss improved from 994327260221669376.00000 to 993962084922294272.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch795-val_Loss993962084922294272.000-mae611479040.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 6167827573997830144.0000 - mae: 611479040.0000 - val_loss: 993962084922294272.0000 - val_mae: 520075072.0000\n",
      "Epoch 796/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6115949866620289024.0000 - mae: 625227008.0000\n",
      "Epoch 796: val_loss improved from 993962084922294272.00000 to 993513965214498816.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch796-val_Loss993513965214498816.000-mae611561728.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 6165878139881783296.0000 - mae: 611561728.0000 - val_loss: 993513965214498816.0000 - val_mae: 519918400.0000\n",
      "Epoch 797/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7683064548235935744.0000 - mae: 724554240.0000\n",
      "Epoch 797: val_loss improved from 993513965214498816.00000 to 993092646102630400.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch797-val_Loss993092646102630400.000-mae611627328.000.h5\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 6164284947533135872.0000 - mae: 611627328.0000 - val_loss: 993092646102630400.0000 - val_mae: 519815424.0000\n",
      "Epoch 798/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6159520213894168576.0000 - mae: 634500224.0000\n",
      "Epoch 798: val_loss improved from 993092646102630400.00000 to 992610785131757568.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch798-val_Loss992610785131757568.000-mae611659840.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 6162880871184465920.0000 - mae: 611659840.0000 - val_loss: 992610785131757568.0000 - val_mae: 519679808.0000\n",
      "Epoch 799/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11096178988538658816.0000 - mae: 751354880.0000\n",
      "Epoch 799: val_loss improved from 992610785131757568.00000 to 992163558777159680.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch799-val_Loss992163558777159680.000-mae611795648.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6161005104347480064.0000 - mae: 611795648.0000 - val_loss: 992163558777159680.0000 - val_mae: 519579136.0000\n",
      "Epoch 800/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15244810082974695424.0000 - mae: 885069376.0000\n",
      "Epoch 800: val_loss improved from 992163558777159680.00000 to 991745057163837440.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch800-val_Loss991745057163837440.000-mae611863296.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 6159772551812743168.0000 - mae: 611863296.0000 - val_loss: 991745057163837440.0000 - val_mae: 519500416.0000\n",
      "Epoch 801/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7415945894849150976.0000 - mae: 654453056.0000\n",
      "Epoch 801: val_loss improved from 991745057163837440.00000 to 991391426736553984.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch801-val_Loss991391426736553984.000-mae612069248.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6157701071906013184.0000 - mae: 612069248.0000 - val_loss: 991391426736553984.0000 - val_mae: 519447520.0000\n",
      "Epoch 802/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6073856713218326528.0000 - mae: 621241088.0000\n",
      "Epoch 802: val_loss improved from 991391426736553984.00000 to 990925027647946752.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch802-val_Loss990925027647946752.000-mae612153728.000.h5\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 6155974288894590976.0000 - mae: 612153728.0000 - val_loss: 990925027647946752.0000 - val_mae: 519290688.0000\n",
      "Epoch 803/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15292873034759667712.0000 - mae: 873595456.0000\n",
      "Epoch 803: val_loss improved from 990925027647946752.00000 to 990425574491029504.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch803-val_Loss990425574491029504.000-mae612204672.000.h5\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 6154644429580795904.0000 - mae: 612204672.0000 - val_loss: 990425574491029504.0000 - val_mae: 519117216.0000\n",
      "Epoch 804/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6070366863311765504.0000 - mae: 613380992.0000\n",
      "Epoch 804: val_loss improved from 990425574491029504.00000 to 989938078523064320.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch804-val_Loss989938078523064320.000-mae612185664.000.h5\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 6152394828790366208.0000 - mae: 612185664.0000 - val_loss: 989938078523064320.0000 - val_mae: 518915104.0000\n",
      "Epoch 805/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 489531713744011264.0000 - mae: 449977664.0000\n",
      "Epoch 805: val_loss improved from 989938078523064320.00000 to 989437044818182144.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch805-val_Loss989437044818182144.000-mae612176448.000.h5\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 6150698832104521728.0000 - mae: 612176448.0000 - val_loss: 989437044818182144.0000 - val_mae: 518739840.0000\n",
      "Epoch 806/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 298634092612681728.0000 - mae: 379213344.0000\n",
      "Epoch 806: val_loss improved from 989437044818182144.00000 to 989001294616199168.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch806-val_Loss989001294616199168.000-mae612185600.000.h5\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 6149312347941896192.0000 - mae: 612185600.0000 - val_loss: 989001294616199168.0000 - val_mae: 518607328.0000\n",
      "Epoch 807/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9461847312826368000.0000 - mae: 653763712.0000\n",
      "Epoch 807: val_loss improved from 989001294616199168.00000 to 988594337874968576.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch807-val_Loss988594337874968576.000-mae612302272.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6147655933674651648.0000 - mae: 612302272.0000 - val_loss: 988594337874968576.0000 - val_mae: 518515808.0000\n",
      "Epoch 808/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9512237930727342080.0000 - mae: 662569792.0000\n",
      "Epoch 808: val_loss improved from 988594337874968576.00000 to 988227238430244864.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch808-val_Loss988227238430244864.000-mae612433856.000.h5\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 6146197431500406784.0000 - mae: 612433856.0000 - val_loss: 988227238430244864.0000 - val_mae: 518454368.0000\n",
      "Epoch 809/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 389869093584371712.0000 - mae: 409932416.0000\n",
      "Epoch 809: val_loss improved from 988227238430244864.00000 to 987849556186103808.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch809-val_Loss987849556186103808.000-mae612545024.000.h5\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 6144296925651795968.0000 - mae: 612545024.0000 - val_loss: 987849556186103808.0000 - val_mae: 518351712.0000\n",
      "Epoch 810/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9715863086144946176.0000 - mae: 715558656.0000\n",
      "Epoch 810: val_loss improved from 987849556186103808.00000 to 987364808997208064.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch810-val_Loss987364808997208064.000-mae612705536.000.h5\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 6142751562058956800.0000 - mae: 612705536.0000 - val_loss: 987364808997208064.0000 - val_mae: 518180192.0000\n",
      "Epoch 811/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1826201952317341696.0000 - mae: 512589888.0000\n",
      "Epoch 811: val_loss improved from 987364808997208064.00000 to 987046843978350592.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch811-val_Loss987046843978350592.000-mae612722496.000.h5\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 6140655892896415744.0000 - mae: 612722496.0000 - val_loss: 987046843978350592.0000 - val_mae: 518139168.0000\n",
      "Epoch 812/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 385699608052891648.0000 - mae: 429978880.0000\n",
      "Epoch 812: val_loss improved from 987046843978350592.00000 to 986611780971134976.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch812-val_Loss986611780971134976.000-mae612842688.000.h5\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 6139214433152401408.0000 - mae: 612842688.0000 - val_loss: 986611780971134976.0000 - val_mae: 518057664.0000\n",
      "Epoch 813/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 230422452809760768.0000 - mae: 358104064.0000\n",
      "Epoch 813: val_loss improved from 986611780971134976.00000 to 986330031116517376.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch813-val_Loss986330031116517376.000-mae612953088.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 6137453565280518144.0000 - mae: 612953088.0000 - val_loss: 986330031116517376.0000 - val_mae: 518082784.0000\n",
      "Epoch 814/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9434570628364500992.0000 - mae: 632667840.0000\n",
      "Epoch 814: val_loss improved from 986330031116517376.00000 to 986030963953762304.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch814-val_Loss986030963953762304.000-mae613228992.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 6136397484362039296.0000 - mae: 613228992.0000 - val_loss: 986030963953762304.0000 - val_mae: 518141792.0000\n",
      "Epoch 815/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6004138879924305920.0000 - mae: 606557184.0000\n",
      "Epoch 815: val_loss improved from 986030963953762304.00000 to 985737325629669376.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch815-val_Loss985737325629669376.000-mae613498176.000.h5\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 6134595934559928320.0000 - mae: 613498176.0000 - val_loss: 985737325629669376.0000 - val_mae: 518172224.0000\n",
      "Epoch 816/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5996380725878718464.0000 - mae: 590686336.0000\n",
      "Epoch 816: val_loss improved from 985737325629669376.00000 to 985334423337566208.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch816-val_Loss985334423337566208.000-mae613609280.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 6132831218397347840.0000 - mae: 613609280.0000 - val_loss: 985334423337566208.0000 - val_mae: 518111840.0000\n",
      "Epoch 817/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 563514483922698240.0000 - mae: 491777088.0000\n",
      "Epoch 817: val_loss improved from 985334423337566208.00000 to 984871460222795776.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch817-val_Loss984871460222795776.000-mae613668032.000.h5\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 6130761387758059520.0000 - mae: 613668032.0000 - val_loss: 984871460222795776.0000 - val_mae: 517949248.0000\n",
      "Epoch 818/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 356908552482193408.0000 - mae: 428832992.0000\n",
      "Epoch 818: val_loss improved from 984871460222795776.00000 to 984384514010644480.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch818-val_Loss984384514010644480.000-mae613665472.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6129257255851261952.0000 - mae: 613665472.0000 - val_loss: 984384514010644480.0000 - val_mae: 517791680.0000\n",
      "Epoch 819/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 314616215676911616.0000 - mae: 385904800.0000\n",
      "Epoch 819: val_loss improved from 984384514010644480.00000 to 984007862558654464.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch819-val_Loss984007862558654464.000-mae613722560.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6127543666979373056.0000 - mae: 613722560.0000 - val_loss: 984007862558654464.0000 - val_mae: 517728768.0000\n",
      "Epoch 820/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9461042470314835968.0000 - mae: 667590784.0000\n",
      "Epoch 820: val_loss improved from 984007862558654464.00000 to 983725425509269504.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch820-val_Loss983725425509269504.000-mae613912832.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6126124747223728128.0000 - mae: 613912832.0000 - val_loss: 983725425509269504.0000 - val_mae: 517759776.0000\n",
      "Epoch 821/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11025687099058683904.0000 - mae: 763513344.0000\n",
      "Epoch 821: val_loss improved from 983725425509269504.00000 to 983532529938071552.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch821-val_Loss983532529938071552.000-mae614232832.000.h5\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 6124787191328538624.0000 - mae: 614232832.0000 - val_loss: 983532529938071552.0000 - val_mae: 517875552.0000\n",
      "Epoch 822/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 281382445935230976.0000 - mae: 380959360.0000\n",
      "Epoch 822: val_loss improved from 983532529938071552.00000 to 983086196936671232.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch822-val_Loss983086196936671232.000-mae614396992.000.h5\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 6122374313061384192.0000 - mae: 614396992.0000 - val_loss: 983086196936671232.0000 - val_mae: 517743456.0000\n",
      "Epoch 823/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15287983506550947840.0000 - mae: 904773248.0000\n",
      "Epoch 823: val_loss improved from 983086196936671232.00000 to 982913092574773248.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch823-val_Loss982913092574773248.000-mae614611200.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 6121906470863765504.0000 - mae: 614611200.0000 - val_loss: 982913092574773248.0000 - val_mae: 517923904.0000\n",
      "Epoch 824/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7491336658386681856.0000 - mae: 711071488.0000\n",
      "Epoch 824: val_loss improved from 982913092574773248.00000 to 982677384769568768.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch824-val_Loss982677384769568768.000-mae614887104.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 6119548568177999872.0000 - mae: 614887104.0000 - val_loss: 982677384769568768.0000 - val_mae: 518035968.0000\n",
      "Epoch 825/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 315055814169591808.0000 - mae: 387174336.0000\n",
      "Epoch 825: val_loss improved from 982677384769568768.00000 to 982191057032708096.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch825-val_Loss982191057032708096.000-mae615016448.000.h5\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 6117539210678239232.0000 - mae: 615016448.0000 - val_loss: 982191057032708096.0000 - val_mae: 517922304.0000\n",
      "Epoch 826/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10983470250598596608.0000 - mae: 772751616.0000\n",
      "Epoch 826: val_loss improved from 982191057032708096.00000 to 981964007881572352.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch826-val_Loss981964007881572352.000-mae615288192.000.h5\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 6116480930736504832.0000 - mae: 615288192.0000 - val_loss: 981964007881572352.0000 - val_mae: 518064896.0000\n",
      "Epoch 827/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 398925530344194048.0000 - mae: 404482432.0000\n",
      "Epoch 827: val_loss improved from 981964007881572352.00000 to 981613126233358336.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch827-val_Loss981613126233358336.000-mae615489216.000.h5\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 6114514454190227456.0000 - mae: 615489216.0000 - val_loss: 981613126233358336.0000 - val_mae: 518057984.0000\n",
      "Epoch 828/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1834114450307678208.0000 - mae: 506614592.0000\n",
      "Epoch 828: val_loss improved from 981613126233358336.00000 to 981258602452877312.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch828-val_Loss981258602452877312.000-mae615638848.000.h5\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 6112876731620655104.0000 - mae: 615638848.0000 - val_loss: 981258602452877312.0000 - val_mae: 518030656.0000\n",
      "Epoch 829/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10986129969226186752.0000 - mae: 764612800.0000\n",
      "Epoch 829: val_loss improved from 981258602452877312.00000 to 980872673871527936.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch829-val_Loss980872673871527936.000-mae615851264.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 6111391841167343616.0000 - mae: 615851264.0000 - val_loss: 980872673871527936.0000 - val_mae: 518018784.0000\n",
      "Epoch 830/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 293700652658327552.0000 - mae: 386146752.0000\n",
      "Epoch 830: val_loss improved from 980872673871527936.00000 to 980479323586691072.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch830-val_Loss980479323586691072.000-mae615958336.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6109871216586129408.0000 - mae: 615958336.0000 - val_loss: 980479323586691072.0000 - val_mae: 517992384.0000\n",
      "Epoch 831/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7503133318641090560.0000 - mae: 741189248.0000\n",
      "Epoch 831: val_loss improved from 980479323586691072.00000 to 980213997687013376.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch831-val_Loss980213997687013376.000-mae616194816.000.h5\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 6108475386574667776.0000 - mae: 616194816.0000 - val_loss: 980213997687013376.0000 - val_mae: 518092000.0000\n",
      "Epoch 832/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1848575776992002048.0000 - mae: 500092160.0000\n",
      "Epoch 832: val_loss improved from 980213997687013376.00000 to 979782714251018240.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch832-val_Loss979782714251018240.000-mae616272192.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 6106247776016793600.0000 - mae: 616272192.0000 - val_loss: 979782714251018240.0000 - val_mae: 518003488.0000\n",
      "Epoch 833/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16597287450464747520.0000 - mae: 994750912.0000\n",
      "Epoch 833: val_loss improved from 979782714251018240.00000 to 979559513390579712.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch833-val_Loss979559513390579712.000-mae616468928.000.h5\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 6105820615749402624.0000 - mae: 616468928.0000 - val_loss: 979559513390579712.0000 - val_mae: 518179968.0000\n",
      "Epoch 834/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 394142620403630080.0000 - mae: 419558368.0000\n",
      "Epoch 834: val_loss improved from 979559513390579712.00000 to 979073185653719040.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch834-val_Loss979073185653719040.000-mae616644864.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6103390695052017664.0000 - mae: 616644864.0000 - val_loss: 979073185653719040.0000 - val_mae: 518070880.0000\n",
      "Epoch 835/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 316901584954982400.0000 - mae: 445868288.0000\n",
      "Epoch 835: val_loss improved from 979073185653719040.00000 to 978674681408126976.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch835-val_Loss978674681408126976.000-mae616669376.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 6101792005145231360.0000 - mae: 616669376.0000 - val_loss: 978674681408126976.0000 - val_mae: 518004704.0000\n",
      "Epoch 836/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9481284479382192128.0000 - mae: 671971200.0000\n",
      "Epoch 836: val_loss improved from 978674681408126976.00000 to 978317889884913664.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch836-val_Loss978317889884913664.000-mae616833728.000.h5\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 6100505026784919552.0000 - mae: 616833728.0000 - val_loss: 978317889884913664.0000 - val_mae: 518034880.0000\n",
      "Epoch 837/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1961288500661714944.0000 - mae: 544704704.0000\n",
      "Epoch 837: val_loss improved from 978317889884913664.00000 to 978039369845702656.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch837-val_Loss978039369845702656.000-mae617038592.000.h5\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 6098593525820030976.0000 - mae: 617038592.0000 - val_loss: 978039369845702656.0000 - val_mae: 518103552.0000\n",
      "Epoch 838/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 291377796905697280.0000 - mae: 381144448.0000\n",
      "Epoch 838: val_loss improved from 978039369845702656.00000 to 977729445005623296.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch838-val_Loss977729445005623296.000-mae617281664.000.h5\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 6097096540738813952.0000 - mae: 617281664.0000 - val_loss: 977729445005623296.0000 - val_mae: 518140128.0000\n",
      "Epoch 839/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5972245895893221376.0000 - mae: 609182976.0000\n",
      "Epoch 839: val_loss improved from 977729445005623296.00000 to 977562044360294400.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch839-val_Loss977562044360294400.000-mae617524672.000.h5\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 6095979986680807424.0000 - mae: 617524672.0000 - val_loss: 977562044360294400.0000 - val_mae: 518335904.0000\n",
      "Epoch 840/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 340856541710123008.0000 - mae: 384577088.0000\n",
      "Epoch 840: val_loss improved from 977562044360294400.00000 to 977119490930114560.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch840-val_Loss977119490930114560.000-mae617768768.000.h5\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 6093828242425249792.0000 - mae: 617768768.0000 - val_loss: 977119490930114560.0000 - val_mae: 518234624.0000\n",
      "Epoch 841/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9403262034763579392.0000 - mae: 665497408.0000\n",
      "Epoch 841: val_loss improved from 977119490930114560.00000 to 976968789117632512.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch841-val_Loss976968789117632512.000-mae618029632.000.h5\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 6092952481413726208.0000 - mae: 618029632.0000 - val_loss: 976968789117632512.0000 - val_mae: 518455392.0000\n",
      "Epoch 842/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9461960562524028928.0000 - mae: 716131392.0000\n",
      "Epoch 842: val_loss improved from 976968789117632512.00000 to 976622717832790016.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch842-val_Loss976622717832790016.000-mae618306432.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6090972260972101632.0000 - mae: 618306432.0000 - val_loss: 976622717832790016.0000 - val_mae: 518479392.0000\n",
      "Epoch 843/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 288047410544902144.0000 - mae: 404181696.0000\n",
      "Epoch 843: val_loss improved from 976622717832790016.00000 to 976319802379337728.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch843-val_Loss976319802379337728.000-mae618493504.000.h5\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 6089121782902554624.0000 - mae: 618493504.0000 - val_loss: 976319802379337728.0000 - val_mae: 518503904.0000\n",
      "Epoch 844/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 357770603958108160.0000 - mae: 428047424.0000\n",
      "Epoch 844: val_loss improved from 976319802379337728.00000 to 976143949238370304.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch844-val_Loss976143949238370304.000-mae618742784.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6087877135739912192.0000 - mae: 618742784.0000 - val_loss: 976143949238370304.0000 - val_mae: 518651840.0000\n",
      "Epoch 845/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9452030873013583872.0000 - mae: 661674304.0000\n",
      "Epoch 845: val_loss improved from 976143949238370304.00000 to 975916281611943936.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch845-val_Loss975916281611943936.000-mae619020992.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 6086498348158681088.0000 - mae: 619020992.0000 - val_loss: 975916281611943936.0000 - val_mae: 518754592.0000\n",
      "Epoch 846/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1784606740243808256.0000 - mae: 506287616.0000\n",
      "Epoch 846: val_loss improved from 975916281611943936.00000 to 975635974866337792.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch846-val_Loss975635974866337792.000-mae619271616.000.h5\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 6084248747368251392.0000 - mae: 619271616.0000 - val_loss: 975635974866337792.0000 - val_mae: 518794528.0000\n",
      "Epoch 847/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6046131977768140800.0000 - mae: 629549312.0000\n",
      "Epoch 847: val_loss improved from 975635974866337792.00000 to 975533720284954624.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch847-val_Loss975533720284954624.000-mae619588736.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 6083569249182285824.0000 - mae: 619588736.0000 - val_loss: 975533720284954624.0000 - val_mae: 519059616.0000\n",
      "Epoch 848/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 480184627957334016.0000 - mae: 446909888.0000\n",
      "Epoch 848: val_loss improved from 975533720284954624.00000 to 975168957302439936.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch848-val_Loss975168957302439936.000-mae619797312.000.h5\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 6081466982949978112.0000 - mae: 619797312.0000 - val_loss: 975168957302439936.0000 - val_mae: 519074528.0000\n",
      "Epoch 849/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 235366853520654336.0000 - mae: 366850432.0000\n",
      "Epoch 849: val_loss improved from 975168957302439936.00000 to 974859788376604672.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch849-val_Loss974859788376604672.000-mae619951040.000.h5\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 6079954604705972224.0000 - mae: 619951040.0000 - val_loss: 974859788376604672.0000 - val_mae: 519130752.0000\n",
      "Epoch 850/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 321450195839614976.0000 - mae: 405127200.0000\n",
      "Epoch 850: val_loss improved from 974859788376604672.00000 to 974692525170229248.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch850-val_Loss974692525170229248.000-mae620195392.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 6078734146799140864.0000 - mae: 620195392.0000 - val_loss: 974692525170229248.0000 - val_mae: 519330048.0000\n",
      "Epoch 851/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7454061564937633792.0000 - mae: 715137664.0000\n",
      "Epoch 851: val_loss improved from 974692525170229248.00000 to 974563057676058624.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch851-val_Loss974563057676058624.000-mae620610304.000.h5\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 6077328421183029248.0000 - mae: 620610304.0000 - val_loss: 974563057676058624.0000 - val_mae: 519567776.0000\n",
      "Epoch 852/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 312697293008535552.0000 - mae: 394266624.0000\n",
      "Epoch 852: val_loss improved from 974563057676058624.00000 to 974224201936273408.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch852-val_Loss974224201936273408.000-mae620816320.000.h5\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 6075363044148379648.0000 - mae: 620816320.0000 - val_loss: 974224201936273408.0000 - val_mae: 519590624.0000\n",
      "Epoch 853/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9467797869755891712.0000 - mae: 717114432.0000\n",
      "Epoch 853: val_loss improved from 974224201936273408.00000 to 973895860276428800.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch853-val_Loss973895860276428800.000-mae620986496.000.h5\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 6074189315485728768.0000 - mae: 620986496.0000 - val_loss: 973895860276428800.0000 - val_mae: 519683136.0000\n",
      "Epoch 854/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10897169582934458368.0000 - mae: 802388864.0000\n",
      "Epoch 854: val_loss improved from 973895860276428800.00000 to 973747013889818624.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch854-val_Loss973747013889818624.000-mae621320704.000.h5\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 6072903436637044736.0000 - mae: 621320704.0000 - val_loss: 973747013889818624.0000 - val_mae: 519913664.0000\n",
      "Epoch 855/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 389814049283506176.0000 - mae: 424462144.0000\n",
      "Epoch 855: val_loss improved from 973747013889818624.00000 to 973441006059913216.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch855-val_Loss973441006059913216.000-mae621539840.000.h5\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 6071097488788422656.0000 - mae: 621539840.0000 - val_loss: 973441006059913216.0000 - val_mae: 519982688.0000\n",
      "Epoch 856/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1840703136298172416.0000 - mae: 513732992.0000\n",
      "Epoch 856: val_loss improved from 973441006059913216.00000 to 973146543102099456.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch856-val_Loss973146543102099456.000-mae621716736.000.h5\n",
      "3/3 [==============================] - 0s 168ms/step - loss: 6069559272021164032.0000 - mae: 621716736.0000 - val_loss: 973146543102099456.0000 - val_mae: 520042368.0000\n",
      "Epoch 857/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7561780169355034624.0000 - mae: 756030784.0000\n",
      "Epoch 857: val_loss improved from 973146543102099456.00000 to 972926159740207104.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch857-val_Loss972926159740207104.000-mae621987392.000.h5\n",
      "3/3 [==============================] - 0s 159ms/step - loss: 6068376197509677056.0000 - mae: 621987392.0000 - val_loss: 972926159740207104.0000 - val_mae: 520173088.0000\n",
      "Epoch 858/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15000653630422384640.0000 - mae: 889455744.0000\n",
      "Epoch 858: val_loss improved from 972926159740207104.00000 to 972631559343439872.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch858-val_Loss972631559343439872.000-mae622168896.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 6067122204498198528.0000 - mae: 622168896.0000 - val_loss: 972631559343439872.0000 - val_mae: 520251488.0000\n",
      "Epoch 859/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1896594198556049408.0000 - mae: 562471104.0000\n",
      "Epoch 859: val_loss improved from 972631559343439872.00000 to 972220479433605120.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch859-val_Loss972220479433605120.000-mae622277376.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6064982554870546432.0000 - mae: 622277376.0000 - val_loss: 972220479433605120.0000 - val_mae: 520176128.0000\n",
      "Epoch 860/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 298749129016737792.0000 - mae: 400501600.0000\n",
      "Epoch 860: val_loss improved from 972220479433605120.00000 to 971816202751967232.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch860-val_Loss971816202751967232.000-mae622249280.000.h5\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 6063660392138145792.0000 - mae: 622249280.0000 - val_loss: 971816202751967232.0000 - val_mae: 520112608.0000\n",
      "Epoch 861/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9599631512949489664.0000 - mae: 767106944.0000\n",
      "Epoch 861: val_loss improved from 971816202751967232.00000 to 971476865975844864.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch861-val_Loss971476865975844864.000-mae622372800.000.h5\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 6062521847847583744.0000 - mae: 622372800.0000 - val_loss: 971476865975844864.0000 - val_mae: 520134048.0000\n",
      "Epoch 862/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9458461916524445696.0000 - mae: 696055552.0000\n",
      "Epoch 862: val_loss improved from 971476865975844864.00000 to 971236416526745600.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch862-val_Loss971236416526745600.000-mae622536192.000.h5\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 6061000673510555648.0000 - mae: 622536192.0000 - val_loss: 971236416526745600.0000 - val_mae: 520237856.0000\n",
      "Epoch 863/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1816914789914378240.0000 - mae: 521489888.0000\n",
      "Epoch 863: val_loss improved from 971236416526745600.00000 to 971028883707002880.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch863-val_Loss971028883707002880.000-mae622737152.000.h5\n",
      "3/3 [==============================] - 0s 151ms/step - loss: 6059217815406116864.0000 - mae: 622737152.0000 - val_loss: 971028883707002880.0000 - val_mae: 520325888.0000\n",
      "Epoch 864/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1829708707215179776.0000 - mae: 521068896.0000\n",
      "Epoch 864: val_loss improved from 971028883707002880.00000 to 970849182275338240.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch864-val_Loss970849182275338240.000-mae622943104.000.h5\n",
      "3/3 [==============================] - 0s 169ms/step - loss: 6057732924952805376.0000 - mae: 622943104.0000 - val_loss: 970849182275338240.0000 - val_mae: 520433728.0000\n",
      "Epoch 865/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 324192583997718528.0000 - mae: 407609664.0000\n",
      "Epoch 865: val_loss improved from 970849182275338240.00000 to 970520909334970368.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch865-val_Loss970520909334970368.000-mae623080640.000.h5\n",
      "3/3 [==============================] - 0s 169ms/step - loss: 6056244186208796672.0000 - mae: 623080640.0000 - val_loss: 970520909334970368.0000 - val_mae: 520424928.0000\n",
      "Epoch 866/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14847877589254537216.0000 - mae: 813740032.0000\n",
      "Epoch 866: val_loss improved from 970520909334970368.00000 to 970483113622765568.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch866-val_Loss970483113622765568.000-mae623407744.000.h5\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 6055929725883252736.0000 - mae: 623407744.0000 - val_loss: 970483113622765568.0000 - val_mae: 520725344.0000\n",
      "Epoch 867/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1850654953480126464.0000 - mae: 539580928.0000\n",
      "Epoch 867: val_loss improved from 970483113622765568.00000 to 970247818134421504.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch867-val_Loss970247818134421504.000-mae623689152.000.h5\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 6053210083871948800.0000 - mae: 623689152.0000 - val_loss: 970247818134421504.0000 - val_mae: 520780992.0000\n",
      "Epoch 868/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 412488865266270208.0000 - mae: 438283360.0000\n",
      "Epoch 868: val_loss improved from 970247818134421504.00000 to 969972046874279936.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch868-val_Loss969972046874279936.000-mae623913536.000.h5\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 6052253508755783680.0000 - mae: 623913536.0000 - val_loss: 969972046874279936.0000 - val_mae: 520854400.0000\n",
      "Epoch 869/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 235604141873823744.0000 - mae: 357182368.0000\n",
      "Epoch 869: val_loss improved from 969972046874279936.00000 to 969674903856873472.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch869-val_Loss969674903856873472.000-mae624037824.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 6050322216581595136.0000 - mae: 624037824.0000 - val_loss: 969674903856873472.0000 - val_mae: 520869280.0000\n",
      "Epoch 870/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16531655402379542528.0000 - mae: 986293568.0000\n",
      "Epoch 870: val_loss improved from 969674903856873472.00000 to 969602542247870464.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch870-val_Loss969602542247870464.000-mae624362560.000.h5\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 6049851625604907008.0000 - mae: 624362560.0000 - val_loss: 969602542247870464.0000 - val_mae: 521136832.0000\n",
      "Epoch 871/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1950846301293772800.0000 - mae: 545868864.0000\n",
      "Epoch 871: val_loss improved from 969602542247870464.00000 to 969204038002278400.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch871-val_Loss969204038002278400.000-mae624603840.000.h5\n",
      "3/3 [==============================] - 0s 158ms/step - loss: 6047262825477308416.0000 - mae: 624603840.0000 - val_loss: 969204038002278400.0000 - val_mae: 521040352.0000\n",
      "Epoch 872/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 243333090141798400.0000 - mae: 351663616.0000\n",
      "Epoch 872: val_loss improved from 969204038002278400.00000 to 968897274258128896.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch872-val_Loss968897274258128896.000-mae624669248.000.h5\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 6046044566593732608.0000 - mae: 624669248.0000 - val_loss: 968897274258128896.0000 - val_mae: 521024736.0000\n",
      "Epoch 873/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1884402401310408704.0000 - mae: 513530048.0000\n",
      "Epoch 873: val_loss improved from 968897274258128896.00000 to 968765607740702720.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch873-val_Loss968765607740702720.000-mae624921152.000.h5\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 6044501402024148992.0000 - mae: 624921152.0000 - val_loss: 968765607740702720.0000 - val_mae: 521182816.0000\n",
      "Epoch 874/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1955281318883360768.0000 - mae: 546495808.0000\n",
      "Epoch 874: val_loss improved from 968765607740702720.00000 to 968551065534332928.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch874-val_Loss968551065534332928.000-mae625212032.000.h5\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 6043387596745211904.0000 - mae: 625212032.0000 - val_loss: 968551065534332928.0000 - val_mae: 521284704.0000\n",
      "Epoch 875/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 463236412129411072.0000 - mae: 460543552.0000\n",
      "Epoch 875: val_loss improved from 968551065534332928.00000 to 968235849294544896.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch875-val_Loss968235849294544896.000-mae625448128.000.h5\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 6041654766419836928.0000 - mae: 625448128.0000 - val_loss: 968235849294544896.0000 - val_mae: 521269152.0000\n",
      "Epoch 876/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1889254683562737664.0000 - mae: 534979584.0000\n",
      "Epoch 876: val_loss improved from 968235849294544896.00000 to 967997736307654656.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch876-val_Loss967997736307654656.000-mae625655552.000.h5\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 6040442005094400000.0000 - mae: 625655552.0000 - val_loss: 967997736307654656.0000 - val_mae: 521329920.0000\n",
      "Epoch 877/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 389381769415098368.0000 - mae: 454416512.0000\n",
      "Epoch 877: val_loss improved from 967997736307654656.00000 to 967549066844045312.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch877-val_Loss967549066844045312.000-mae625792512.000.h5\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 6038840016652730368.0000 - mae: 625792512.0000 - val_loss: 967549066844045312.0000 - val_mae: 521184832.0000\n",
      "Epoch 878/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 322765349185388544.0000 - mae: 422694976.0000\n",
      "Epoch 878: val_loss improved from 967549066844045312.00000 to 967401526127493120.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch878-val_Loss967401526127493120.000-mae625887488.000.h5\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 6037663539211010048.0000 - mae: 625887488.0000 - val_loss: 967401526127493120.0000 - val_mae: 521316224.0000\n",
      "Epoch 879/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5922347859201490944.0000 - mae: 615565184.0000\n",
      "Epoch 879: val_loss improved from 967401526127493120.00000 to 967236255785943040.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch879-val_Loss967236255785943040.000-mae626310592.000.h5\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 6036159407304212480.0000 - mae: 626310592.0000 - val_loss: 967236255785943040.0000 - val_mae: 521409920.0000\n",
      "Epoch 880/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7350940018635964416.0000 - mae: 703836672.0000\n",
      "Epoch 880: val_loss improved from 967236255785943040.00000 to 967192069162401792.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch880-val_Loss967192069162401792.000-mae626614656.000.h5\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 6034647578816020480.0000 - mae: 626614656.0000 - val_loss: 967192069162401792.0000 - val_mae: 521594944.0000\n",
      "Epoch 881/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5820323075748528128.0000 - mae: 580825216.0000\n",
      "Epoch 881: val_loss improved from 967192069162401792.00000 to 967022744371724288.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch881-val_Loss967022744371724288.000-mae626954688.000.h5\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 6033235256130142208.0000 - mae: 626954688.0000 - val_loss: 967022744371724288.0000 - val_mae: 521688160.0000\n",
      "Epoch 882/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1914820390492831744.0000 - mae: 549424896.0000\n",
      "Epoch 882: val_loss improved from 967022744371724288.00000 to 966791434613030912.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch882-val_Loss966791434613030912.000-mae627203712.000.h5\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 6031522766769881088.0000 - mae: 627203712.0000 - val_loss: 966791434613030912.0000 - val_mae: 521722080.0000\n",
      "Epoch 883/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16366501059306192896.0000 - mae: 993109056.0000\n",
      "Epoch 883: val_loss improved from 966791434613030912.00000 to 966763946822336512.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch883-val_Loss966763946822336512.000-mae627661824.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6031178069874573312.0000 - mae: 627661824.0000 - val_loss: 966763946822336512.0000 - val_mae: 521963552.0000\n",
      "Epoch 884/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5939930699397070848.0000 - mae: 615487616.0000\n",
      "Epoch 884: val_loss improved from 966763946822336512.00000 to 966545350166839296.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch884-val_Loss966545350166839296.000-mae628007040.000.h5\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 6028850403758571520.0000 - mae: 628007040.0000 - val_loss: 966545350166839296.0000 - val_mae: 522016032.0000\n",
      "Epoch 885/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1992605065721937920.0000 - mae: 578497408.0000\n",
      "Epoch 885: val_loss improved from 966545350166839296.00000 to 966256384767164416.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch885-val_Loss966256384767164416.000-mae628179136.000.h5\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 6027572771247095808.0000 - mae: 628179136.0000 - val_loss: 966256384767164416.0000 - val_mae: 522042528.0000\n",
      "Epoch 886/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 326687616399310848.0000 - mae: 435939104.0000\n",
      "Epoch 886: val_loss improved from 966256384767164416.00000 to 965880832826802176.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch886-val_Loss965880832826802176.000-mae628283136.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 6025924053561245696.0000 - mae: 628283136.0000 - val_loss: 965880832826802176.0000 - val_mae: 521989408.0000\n",
      "Epoch 887/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1839643756844810240.0000 - mae: 529410912.0000\n",
      "Epoch 887: val_loss improved from 965880832826802176.00000 to 965643750632062976.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch887-val_Loss965643750632062976.000-mae628388672.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 6024823992177655808.0000 - mae: 628388672.0000 - val_loss: 965643750632062976.0000 - val_mae: 522108800.0000\n",
      "Epoch 888/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1962521053196451840.0000 - mae: 566369024.0000\n",
      "Epoch 888: val_loss improved from 965643750632062976.00000 to 965346676334133248.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch888-val_Loss965346676334133248.000-mae628626560.000.h5\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 6023202212526686208.0000 - mae: 628626560.0000 - val_loss: 965346676334133248.0000 - val_mae: 522157664.0000\n",
      "Epoch 889/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14879794212785618944.0000 - mae: 904118144.0000\n",
      "Epoch 889: val_loss improved from 965346676334133248.00000 to 965150001191714816.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch889-val_Loss965150001191714816.000-mae628940736.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 6022817383456964608.0000 - mae: 628940736.0000 - val_loss: 965150001191714816.0000 - val_mae: 522359168.0000\n",
      "Epoch 890/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 355873224845688832.0000 - mae: 419014848.0000\n",
      "Epoch 890: val_loss improved from 965150001191714816.00000 to 964830593063845888.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch890-val_Loss964830593063845888.000-mae629112768.000.h5\n",
      "3/3 [==============================] - 0s 182ms/step - loss: 6020736007945584640.0000 - mae: 629112768.0000 - val_loss: 964830593063845888.0000 - val_mae: 522378976.0000\n",
      "Epoch 891/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16301445155313942528.0000 - mae: 962780800.0000\n",
      "Epoch 891: val_loss improved from 964830593063845888.00000 to 964779809370537984.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch891-val_Loss964779809370537984.000-mae629473344.000.h5\n",
      "3/3 [==============================] - 0s 165ms/step - loss: 6019988340038696960.0000 - mae: 629473344.0000 - val_loss: 964779809370537984.0000 - val_mae: 522648736.0000\n",
      "Epoch 892/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5810069030307889152.0000 - mae: 602595712.0000\n",
      "Epoch 892: val_loss improved from 964779809370537984.00000 to 964490431654002688.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch892-val_Loss964490431654002688.000-mae629696448.000.h5\n",
      "3/3 [==============================] - 0s 162ms/step - loss: 6018101578085433344.0000 - mae: 629696448.0000 - val_loss: 964490431654002688.0000 - val_mae: 522682176.0000\n",
      "Epoch 893/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 331927579579645952.0000 - mae: 421810432.0000\n",
      "Epoch 893: val_loss improved from 964490431654002688.00000 to 964043480177311744.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch893-val_Loss964043480177311744.000-mae629711040.000.h5\n",
      "3/3 [==============================] - 0s 192ms/step - loss: 6016615038364680192.0000 - mae: 629711040.0000 - val_loss: 964043480177311744.0000 - val_mae: 522530176.0000\n",
      "Epoch 894/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9426569482249175040.0000 - mae: 708897280.0000\n",
      "Epoch 894: val_loss improved from 964043480177311744.00000 to 963792035611934720.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch894-val_Loss963792035611934720.000-mae629809280.000.h5\n",
      "3/3 [==============================] - 0s 176ms/step - loss: 6015792603667103744.0000 - mae: 629809280.0000 - val_loss: 963792035611934720.0000 - val_mae: 522631680.0000\n",
      "Epoch 895/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14832130383721529344.0000 - mae: 877796096.0000\n",
      "Epoch 895: val_loss improved from 963792035611934720.00000 to 963594260957888512.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch895-val_Loss963594260957888512.000-mae630029632.000.h5\n",
      "3/3 [==============================] - 0s 158ms/step - loss: 6014655158888169472.0000 - mae: 630029632.0000 - val_loss: 963594260957888512.0000 - val_mae: 522751296.0000\n",
      "Epoch 896/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9375817125022662656.0000 - mae: 709712512.0000\n",
      "Epoch 896: val_loss improved from 963594260957888512.00000 to 963262964360544256.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch896-val_Loss963262964360544256.000-mae630239040.000.h5\n",
      "3/3 [==============================] - 0s 170ms/step - loss: 6012926176853491712.0000 - mae: 630239040.0000 - val_loss: 963262964360544256.0000 - val_mae: 522748000.0000\n",
      "Epoch 897/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 377184440251580416.0000 - mae: 445173248.0000\n",
      "Epoch 897: val_loss improved from 963262964360544256.00000 to 962959499151278080.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch897-val_Loss962959499151278080.000-mae630269184.000.h5\n",
      "3/3 [==============================] - 0s 162ms/step - loss: 6011411049830416384.0000 - mae: 630269184.0000 - val_loss: 962959499151278080.0000 - val_mae: 522708800.0000\n",
      "Epoch 898/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 324699046541262848.0000 - mae: 425374208.0000\n",
      "Epoch 898: val_loss improved from 962959499151278080.00000 to 962754234074267648.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch898-val_Loss962754234074267648.000-mae630378368.000.h5\n",
      "3/3 [==============================] - 0s 182ms/step - loss: 6010392352307281920.0000 - mae: 630378368.0000 - val_loss: 962754234074267648.0000 - val_mae: 522793120.0000\n",
      "Epoch 899/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1928472476619112448.0000 - mae: 584243392.0000\n",
      "Epoch 899: val_loss improved from 962754234074267648.00000 to 962500727924588544.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch899-val_Loss962500727924588544.000-mae630613632.000.h5\n",
      "3/3 [==============================] - 0s 182ms/step - loss: 6008757378516779008.0000 - mae: 630613632.0000 - val_loss: 962500727924588544.0000 - val_mae: 522804512.0000\n",
      "Epoch 900/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 326480873853550592.0000 - mae: 405151168.0000\n",
      "Epoch 900: val_loss improved from 962500727924588544.00000 to 962323362955132928.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch900-val_Loss962323362955132928.000-mae630758656.000.h5\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 6007693601016905728.0000 - mae: 630758656.0000 - val_loss: 962323362955132928.0000 - val_mae: 522918848.0000\n",
      "Epoch 901/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7434921266521309184.0000 - mae: 737743232.0000\n",
      "Epoch 901: val_loss did not improve from 962323362955132928.00000\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 6006491285051932672.0000 - mae: 631191680.0000 - val_loss: 962404520657158144.0000 - val_mae: 523232608.0000\n",
      "Epoch 902/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5927119189910224896.0000 - mae: 640460032.0000\n",
      "Epoch 902: val_loss improved from 962323362955132928.00000 to 962317178202226688.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch902-val_Loss962317178202226688.000-mae631627648.000.h5\n",
      "3/3 [==============================] - 0s 162ms/step - loss: 6005212553028829184.0000 - mae: 631627648.0000 - val_loss: 962317178202226688.0000 - val_mae: 523417792.0000\n",
      "Epoch 903/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1796623027945865216.0000 - mae: 494078720.0000\n",
      "Epoch 903: val_loss improved from 962317178202226688.00000 to 962213755389739008.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch903-val_Loss962213755389739008.000-mae631966784.000.h5\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 6003631455308087296.0000 - mae: 631966784.0000 - val_loss: 962213755389739008.0000 - val_mae: 523588096.0000\n",
      "Epoch 904/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1851207870389944320.0000 - mae: 512768992.0000\n",
      "Epoch 904: val_loss improved from 962213755389739008.00000 to 962036802737143808.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch904-val_Loss962036802737143808.000-mae632298112.000.h5\n",
      "3/3 [==============================] - 0s 176ms/step - loss: 6002204838971047936.0000 - mae: 632298112.0000 - val_loss: 962036802737143808.0000 - val_mae: 523668160.0000\n",
      "Epoch 905/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7404858419594657792.0000 - mae: 746910592.0000\n",
      "Epoch 905: val_loss did not improve from 962036802737143808.00000\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 6001588012947865600.0000 - mae: 632745344.0000 - val_loss: 962155412553990144.0000 - val_mae: 524044480.0000\n",
      "Epoch 906/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1944672680942764032.0000 - mae: 538909696.0000\n",
      "Epoch 906: val_loss did not improve from 962036802737143808.00000\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 5999600095924846592.0000 - mae: 633213504.0000 - val_loss: 962062709979873280.0000 - val_mae: 524208896.0000\n",
      "Epoch 907/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1837551935972966400.0000 - mae: 535807744.0000\n",
      "Epoch 907: val_loss improved from 962036802737143808.00000 to 961868577458094080.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch907-val_Loss961868577458094080.000-mae633476096.000.h5\n",
      "3/3 [==============================] - 0s 157ms/step - loss: 5998154787890135040.0000 - mae: 633476096.0000 - val_loss: 961868577458094080.0000 - val_mae: 524277664.0000\n",
      "Epoch 908/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9268428923849408512.0000 - mae: 682161920.0000\n",
      "Epoch 908: val_loss improved from 961868577458094080.00000 to 961819168154320896.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch908-val_Loss961819168154320896.000-mae633800704.000.h5\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 5997614377925083136.0000 - mae: 633800704.0000 - val_loss: 961819168154320896.0000 - val_mae: 524543136.0000\n",
      "Epoch 909/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9254628953409191936.0000 - mae: 688867712.0000\n",
      "Epoch 909: val_loss improved from 961819168154320896.00000 to 961603388997369856.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch909-val_Loss961603388997369856.000-mae634222272.000.h5\n",
      "3/3 [==============================] - 0s 167ms/step - loss: 5996017337285738496.0000 - mae: 634222272.0000 - val_loss: 961603388997369856.0000 - val_mae: 524648992.0000\n",
      "Epoch 910/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9396725438136451072.0000 - mae: 723058304.0000\n",
      "Epoch 910: val_loss improved from 961603388997369856.00000 to 961507113010462720.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch910-val_Loss961507113010462720.000-mae634543360.000.h5\n",
      "3/3 [==============================] - 0s 176ms/step - loss: 5994858452030062592.0000 - mae: 634543360.0000 - val_loss: 961507113010462720.0000 - val_mae: 524841760.0000\n",
      "Epoch 911/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 428127253507866624.0000 - mae: 469781408.0000\n",
      "Epoch 911: val_loss improved from 961507113010462720.00000 to 961350844920365056.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch911-val_Loss961350844920365056.000-mae634814912.000.h5\n",
      "3/3 [==============================] - 0s 159ms/step - loss: 5992985433972146176.0000 - mae: 634814912.0000 - val_loss: 961350844920365056.0000 - val_mae: 524914496.0000\n",
      "Epoch 912/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7317365331570196480.0000 - mae: 718412928.0000\n",
      "Epoch 912: val_loss did not improve from 961350844920365056.00000\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 5992224022169911296.0000 - mae: 635195072.0000 - val_loss: 961409737511927808.0000 - val_mae: 525205440.0000\n",
      "Epoch 913/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 471420626930761728.0000 - mae: 490814144.0000\n",
      "Epoch 913: val_loss improved from 961350844920365056.00000 to 961122558818648064.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch913-val_Loss961122558818648064.000-mae635475072.000.h5\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 5990602242518941696.0000 - mae: 635475072.0000 - val_loss: 961122558818648064.0000 - val_mae: 525172096.0000\n",
      "Epoch 914/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14781866209657749504.0000 - mae: 900846080.0000\n",
      "Epoch 914: val_loss did not improve from 961122558818648064.00000\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 5990110760821325824.0000 - mae: 635837824.0000 - val_loss: 961126475828822016.0000 - val_mae: 525441408.0000\n",
      "Epoch 915/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5745732206920204288.0000 - mae: 595357248.0000\n",
      "Epoch 915: val_loss improved from 961122558818648064.00000 to 960990067667501056.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch915-val_Loss960990067667501056.000-mae636175360.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 5988071716507615232.0000 - mae: 636175360.0000 - val_loss: 960990067667501056.0000 - val_mae: 525513248.0000\n",
      "Epoch 916/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1848329211509473280.0000 - mae: 537625152.0000\n",
      "Epoch 916: val_loss improved from 960990067667501056.00000 to 960700964828872704.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch916-val_Loss960700964828872704.000-mae636368832.000.h5\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 5986700075751964672.0000 - mae: 636368832.0000 - val_loss: 960700964828872704.0000 - val_mae: 525459200.0000\n",
      "Epoch 917/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1812034744993447936.0000 - mae: 513738112.0000\n",
      "Epoch 917: val_loss improved from 960700964828872704.00000 to 960650318574518272.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch917-val_Loss960650318574518272.000-mae636612928.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 5985884238124154880.0000 - mae: 636612928.0000 - val_loss: 960650318574518272.0000 - val_mae: 525655904.0000\n",
      "Epoch 918/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 372764609666351104.0000 - mae: 416680128.0000\n",
      "Epoch 918: val_loss improved from 960650318574518272.00000 to 960497830055641088.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch918-val_Loss960497830055641088.000-mae636943744.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 5984592311961518080.0000 - mae: 636943744.0000 - val_loss: 960497830055641088.0000 - val_mae: 525761120.0000\n",
      "Epoch 919/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5949394195977338880.0000 - mae: 672049152.0000\n",
      "Epoch 919: val_loss improved from 960497830055641088.00000 to 960369736951005184.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch919-val_Loss960369736951005184.000-mae637313472.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 5983604950519775232.0000 - mae: 637313472.0000 - val_loss: 960369736951005184.0000 - val_mae: 525871488.0000\n",
      "Epoch 920/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5820655678015930368.0000 - mae: 623468352.0000\n",
      "Epoch 920: val_loss improved from 960369736951005184.00000 to 960176429062946816.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch920-val_Loss960176429062946816.000-mae637584064.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 5982088723985072128.0000 - mae: 637584064.0000 - val_loss: 960176429062946816.0000 - val_mae: 525890272.0000\n",
      "Epoch 921/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14672626430903320576.0000 - mae: 861344512.0000\n",
      "Epoch 921: val_loss improved from 960176429062946816.00000 to 960133685548417024.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch921-val_Loss960133685548417024.000-mae637885888.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 5981499935508398080.0000 - mae: 637885888.0000 - val_loss: 960133685548417024.0000 - val_mae: 526107776.0000\n",
      "Epoch 922/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5917873396632256512.0000 - mae: 678995392.0000\n",
      "Epoch 922: val_loss improved from 960133685548417024.00000 to 960040501937963008.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch922-val_Loss960040501937963008.000-mae638245440.000.h5\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 5979792393950461952.0000 - mae: 638245440.0000 - val_loss: 960040501937963008.0000 - val_mae: 526246592.0000\n",
      "Epoch 923/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9161274368897056768.0000 - mae: 648882624.0000\n",
      "Epoch 923: val_loss improved from 960040501937963008.00000 to 959766173786832896.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch923-val_Loss959766173786832896.000-mae638498752.000.h5\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 5978527405822705664.0000 - mae: 638498752.0000 - val_loss: 959766173786832896.0000 - val_mae: 526245312.0000\n",
      "Epoch 924/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10804197078712975360.0000 - mae: 780066240.0000\n",
      "Epoch 924: val_loss improved from 959766173786832896.00000 to 959714153142943744.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch924-val_Loss959714153142943744.000-mae638726784.000.h5\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 5977545541939101696.0000 - mae: 638726784.0000 - val_loss: 959714153142943744.0000 - val_mae: 526420704.0000\n",
      "Epoch 925/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9233610689132625920.0000 - mae: 669400320.0000\n",
      "Epoch 925: val_loss improved from 959714153142943744.00000 to 959483805456924672.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch925-val_Loss959483805456924672.000-mae639030336.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 5976430087392722944.0000 - mae: 639030336.0000 - val_loss: 959483805456924672.0000 - val_mae: 526429792.0000\n",
      "Epoch 926/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9125566629273403392.0000 - mae: 656071936.0000\n",
      "Epoch 926: val_loss improved from 959483805456924672.00000 to 959337226813046784.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch926-val_Loss959337226813046784.000-mae639258624.000.h5\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 5975176644137058304.0000 - mae: 639258624.0000 - val_loss: 959337226813046784.0000 - val_mae: 526488800.0000\n",
      "Epoch 927/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7349479867194277888.0000 - mae: 751809600.0000\n",
      "Epoch 927: val_loss improved from 959337226813046784.00000 to 959162129586323456.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch927-val_Loss959162129586323456.000-mae639442176.000.h5\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 5973752776579088384.0000 - mae: 639442176.0000 - val_loss: 959162129586323456.0000 - val_mae: 526483680.0000\n",
      "Epoch 928/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5726371456422510592.0000 - mae: 589391488.0000\n",
      "Epoch 928: val_loss improved from 959162129586323456.00000 to 959061249394475008.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch928-val_Loss959061249394475008.000-mae639627008.000.h5\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 5972913299451281408.0000 - mae: 639627008.0000 - val_loss: 959061249394475008.0000 - val_mae: 526605536.0000\n",
      "Epoch 929/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7380964382655643648.0000 - mae: 768821504.0000\n",
      "Epoch 929: val_loss improved from 959061249394475008.00000 to 958931300863967232.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch929-val_Loss958931300863967232.000-mae639912192.000.h5\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 5971757712730488832.0000 - mae: 639912192.0000 - val_loss: 958931300863967232.0000 - val_mae: 526676288.0000\n",
      "Epoch 930/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 216119318360883200.0000 - mae: 350954752.0000\n",
      "Epoch 930: val_loss improved from 958931300863967232.00000 to 958661302039871488.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch930-val_Loss958661302039871488.000-mae640057216.000.h5\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 5970562543591096320.0000 - mae: 640057216.0000 - val_loss: 958661302039871488.0000 - val_mae: 526640160.0000\n",
      "Epoch 931/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1793679360440401920.0000 - mae: 500195552.0000\n",
      "Epoch 931: val_loss improved from 958661302039871488.00000 to 958378796271009792.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch931-val_Loss958378796271009792.000-mae640183040.000.h5\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 5969204646730792960.0000 - mae: 640183040.0000 - val_loss: 958378796271009792.0000 - val_mae: 526590624.0000\n",
      "Epoch 932/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 407676234152017920.0000 - mae: 466220448.0000\n",
      "Epoch 932: val_loss improved from 958378796271009792.00000 to 958111202628599808.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch932-val_Loss958111202628599808.000-mae640240704.000.h5\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 5968513603672735744.0000 - mae: 640240704.0000 - val_loss: 958111202628599808.0000 - val_mae: 526583648.0000\n",
      "Epoch 933/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5829974588817145856.0000 - mae: 625303680.0000\n",
      "Epoch 933: val_loss improved from 958111202628599808.00000 to 957981254098092032.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch933-val_Loss957981254098092032.000-mae640473088.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 5967201336544985088.0000 - mae: 640473088.0000 - val_loss: 957981254098092032.0000 - val_mae: 526681792.0000\n",
      "Epoch 934/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 362006472504115200.0000 - mae: 414899328.0000\n",
      "Epoch 934: val_loss improved from 957981254098092032.00000 to 957761970247827456.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch934-val_Loss957761970247827456.000-mae640642176.000.h5\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 5965872576742817792.0000 - mae: 640642176.0000 - val_loss: 957761970247827456.0000 - val_mae: 526712416.0000\n",
      "Epoch 935/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7400169002502193152.0000 - mae: 778606720.0000\n",
      "Epoch 935: val_loss did not improve from 957761970247827456.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5964996265975480320.0000 - mae: 640940864.0000 - val_loss: 957769735548698624.0000 - val_mae: 526935904.0000\n",
      "Epoch 936/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 434759610725826560.0000 - mae: 458781472.0000\n",
      "Epoch 936: val_loss improved from 957761970247827456.00000 to 957561584253665280.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch936-val_Loss957561584253665280.000-mae641217792.000.h5\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 5963863768998871040.0000 - mae: 641217792.0000 - val_loss: 957561584253665280.0000 - val_mae: 527028352.0000\n",
      "Epoch 937/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 388133651918880768.0000 - mae: 444045952.0000\n",
      "Epoch 937: val_loss improved from 957561584253665280.00000 to 957547428041457664.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch937-val_Loss957547428041457664.000-mae641502016.000.h5\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 5962510819940892672.0000 - mae: 641502016.0000 - val_loss: 957547428041457664.0000 - val_mae: 527224928.0000\n",
      "Epoch 938/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 391532723396673536.0000 - mae: 446622528.0000\n",
      "Epoch 938: val_loss improved from 957547428041457664.00000 to 957473142287106048.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch938-val_Loss957473142287106048.000-mae641799552.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 5961359081510797312.0000 - mae: 641799552.0000 - val_loss: 957473142287106048.0000 - val_mae: 527401152.0000\n",
      "Epoch 939/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10718485749281325056.0000 - mae: 804040000.0000\n",
      "Epoch 939: val_loss did not improve from 957473142287106048.00000\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 5960706521359712256.0000 - mae: 642296832.0000 - val_loss: 957604740085055488.0000 - val_mae: 527759552.0000\n",
      "Epoch 940/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5861073175697162240.0000 - mae: 647043264.0000\n",
      "Epoch 940: val_loss did not improve from 957473142287106048.00000\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 5959147963627339776.0000 - mae: 642773312.0000 - val_loss: 957600548196974592.0000 - val_mae: 527946528.0000\n",
      "Epoch 941/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16235780121879904256.0000 - mae: 1021573120.0000\n",
      "Epoch 941: val_loss did not improve from 957473142287106048.00000\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 5958637790232051712.0000 - mae: 643244352.0000 - val_loss: 957730702885912576.0000 - val_mae: 528268896.0000\n",
      "Epoch 942/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5830394052503142400.0000 - mae: 636658688.0000\n",
      "Epoch 942: val_loss did not improve from 957473142287106048.00000\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 5956747729743904768.0000 - mae: 643607872.0000 - val_loss: 957638137750749184.0000 - val_mae: 528373504.0000\n",
      "Epoch 943/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2001362675837173760.0000 - mae: 583907584.0000\n",
      "Epoch 943: val_loss improved from 957473142287106048.00000 to 957462215890305024.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch943-val_Loss957462215890305024.000-mae643811648.000.h5\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 5955498134778937344.0000 - mae: 643811648.0000 - val_loss: 957462215890305024.0000 - val_mae: 528413856.0000\n",
      "Epoch 944/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 562339655748419584.0000 - mae: 514349888.0000\n",
      "Epoch 944: val_loss improved from 957462215890305024.00000 to 957236403689750528.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch944-val_Loss957236403689750528.000-mae644030016.000.h5\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 5954632269372063744.0000 - mae: 644030016.0000 - val_loss: 957236403689750528.0000 - val_mae: 528435872.0000\n",
      "Epoch 945/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9258392581711069184.0000 - mae: 705213440.0000\n",
      "Epoch 945: val_loss did not improve from 957236403689750528.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5953879103907037184.0000 - mae: 644381568.0000 - val_loss: 957288286894686208.0000 - val_mae: 528702560.0000\n",
      "Epoch 946/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 484403282274418688.0000 - mae: 496079744.0000\n",
      "Epoch 946: val_loss improved from 957236403689750528.00000 to 956994854729023488.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch946-val_Loss956994854729023488.000-mae644631872.000.h5\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 5952298006186295296.0000 - mae: 644631872.0000 - val_loss: 956994854729023488.0000 - val_mae: 528610400.0000\n",
      "Epoch 947/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 407031061344681984.0000 - mae: 460843392.0000\n",
      "Epoch 947: val_loss improved from 956994854729023488.00000 to 956922767997927424.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch947-val_Loss956922767997927424.000-mae644766976.000.h5\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 5951191897488752640.0000 - mae: 644766976.0000 - val_loss: 956922767997927424.0000 - val_mae: 528740800.0000\n",
      "Epoch 948/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9140495798155345920.0000 - mae: 670236672.0000\n",
      "Epoch 948: val_loss did not improve from 956922767997927424.00000\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 5950296895023742976.0000 - mae: 645194240.0000 - val_loss: 957008529904893952.0000 - val_mae: 528984160.0000\n",
      "Epoch 949/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 420025845675982848.0000 - mae: 461009312.0000\n",
      "Epoch 949: val_loss improved from 956922767997927424.00000 to 956888820576419840.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch949-val_Loss956888820576419840.000-mae645494784.000.h5\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 5948875776244842496.0000 - mae: 645494784.0000 - val_loss: 956888820576419840.0000 - val_mae: 529010560.0000\n",
      "Epoch 950/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10780830257599479808.0000 - mae: 805802368.0000\n",
      "Epoch 950: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 5948332617500721152.0000 - mae: 645931328.0000 - val_loss: 957195103284232192.0000 - val_mae: 529445440.0000\n",
      "Epoch 951/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7377670795574640640.0000 - mae: 752702336.0000\n",
      "Epoch 951: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 5946735027105562624.0000 - mae: 646554752.0000 - val_loss: 957337971076366336.0000 - val_mae: 529690784.0000\n",
      "Epoch 952/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 377087339630952448.0000 - mae: 456600352.0000\n",
      "Epoch 952: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 5945556900396400640.0000 - mae: 646856448.0000 - val_loss: 957206304558940160.0000 - val_mae: 529718880.0000\n",
      "Epoch 953/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16208784912394747904.0000 - mae: 1049216512.0000\n",
      "Epoch 953: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 5945339746849914880.0000 - mae: 647389696.0000 - val_loss: 957456718332166144.0000 - val_mae: 530118208.0000\n",
      "Epoch 954/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2044271391989039104.0000 - mae: 615438720.0000\n",
      "Epoch 954: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 5943134676280410112.0000 - mae: 647722432.0000 - val_loss: 957266296662130688.0000 - val_mae: 530100192.0000\n",
      "Epoch 955/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5749295174350012416.0000 - mae: 645153792.0000\n",
      "Epoch 955: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 5942433737617702912.0000 - mae: 647996160.0000 - val_loss: 957276329705734144.0000 - val_mae: 530237120.0000\n",
      "Epoch 956/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 453727354536067072.0000 - mae: 454081504.0000\n",
      "Epoch 956: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5941200085571338240.0000 - mae: 648333184.0000 - val_loss: 957202456268242944.0000 - val_mae: 530356960.0000\n",
      "Epoch 957/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9288377363312148480.0000 - mae: 731436608.0000\n",
      "Epoch 957: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 5940368854780739584.0000 - mae: 648757632.0000 - val_loss: 957233654910681088.0000 - val_mae: 530574848.0000\n",
      "Epoch 958/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9215859073902182400.0000 - mae: 707608064.0000\n",
      "Epoch 958: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 5939371047978532864.0000 - mae: 649163264.0000 - val_loss: 957316255721717760.0000 - val_mae: 530804256.0000\n",
      "Epoch 959/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5794565366600433664.0000 - mae: 651986752.0000\n",
      "Epoch 959: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 5937929588234518528.0000 - mae: 649595136.0000 - val_loss: 957393977449906176.0000 - val_mae: 530966752.0000\n",
      "Epoch 960/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10778475103692783616.0000 - mae: 840151040.0000\n",
      "Epoch 960: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 5937187967641583616.0000 - mae: 649987520.0000 - val_loss: 957480976307453952.0000 - val_mae: 531202112.0000\n",
      "Epoch 961/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9234558468155768832.0000 - mae: 740675072.0000\n",
      "Epoch 961: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 5936053821397532672.0000 - mae: 650334336.0000 - val_loss: 957296808109801472.0000 - val_mae: 531208704.0000\n",
      "Epoch 962/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10654331444823851008.0000 - mae: 803426688.0000\n",
      "Epoch 962: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 5935064810688348160.0000 - mae: 650644032.0000 - val_loss: 957468881679548416.0000 - val_mae: 531486176.0000\n",
      "Epoch 963/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9072232068989059072.0000 - mae: 690382912.0000\n",
      "Epoch 963: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 5934027421467541504.0000 - mae: 651053632.0000 - val_loss: 957435690172284928.0000 - val_mae: 531586816.0000\n",
      "Epoch 964/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1885511258787020800.0000 - mae: 563091328.0000\n",
      "Epoch 964: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 5932823456235126784.0000 - mae: 651351872.0000 - val_loss: 957420915484786688.0000 - val_mae: 531678304.0000\n",
      "Epoch 965/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10773309598065491968.0000 - mae: 848110528.0000\n",
      "Epoch 965: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 5931862483072450560.0000 - mae: 651579520.0000 - val_loss: 957270625989165056.0000 - val_mae: 531687104.0000\n",
      "Epoch 966/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 314253273760530432.0000 - mae: 417639840.0000\n",
      "Epoch 966: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 5930983423526043648.0000 - mae: 651787456.0000 - val_loss: 957236060092366848.0000 - val_mae: 531756576.0000\n",
      "Epoch 967/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14647083676278456320.0000 - mae: 932738688.0000\n",
      "Epoch 967: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5930389137491230720.0000 - mae: 652092864.0000 - val_loss: 957251453255155712.0000 - val_mae: 531886336.0000\n",
      "Epoch 968/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9100504910985691136.0000 - mae: 673268288.0000\n",
      "Epoch 968: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 5929038387456507904.0000 - mae: 652335936.0000 - val_loss: 957018631667974144.0000 - val_mae: 531806176.0000\n",
      "Epoch 969/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10651240717638172672.0000 - mae: 801372736.0000\n",
      "Epoch 969: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 5928147233282195456.0000 - mae: 652539264.0000 - val_loss: 957107417231917056.0000 - val_mae: 531969120.0000\n",
      "Epoch 970/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9141401245980819456.0000 - mae: 704257600.0000\n",
      "Epoch 970: val_loss did not improve from 956888820576419840.00000\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 5927135132828827648.0000 - mae: 652821760.0000 - val_loss: 957052579089481728.0000 - val_mae: 532014336.0000\n",
      "Epoch 971/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1971355079369818112.0000 - mae: 570499328.0000\n",
      "Epoch 971: val_loss improved from 956888820576419840.00000 to 956801409402011648.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch971-val_Loss956801409402011648.000-mae652960192.000.h5\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 5925853102270840832.0000 - mae: 652960192.0000 - val_loss: 956801409402011648.0000 - val_mae: 531874624.0000\n",
      "Epoch 972/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 366169670203473920.0000 - mae: 442086912.0000\n",
      "Epoch 972: val_loss improved from 956801409402011648.00000 to 956613599072092160.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch972-val_Loss956613599072092160.000-mae652960896.000.h5\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 5925074098282561536.0000 - mae: 652960896.0000 - val_loss: 956613599072092160.0000 - val_mae: 531812448.0000\n",
      "Epoch 973/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9258289227618058240.0000 - mae: 774554816.0000\n",
      "Epoch 973: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 5924561176108204032.0000 - mae: 653264896.0000 - val_loss: 956796942636023808.0000 - val_mae: 532089920.0000\n",
      "Epoch 974/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5739334148758175744.0000 - mae: 659570944.0000\n",
      "Epoch 974: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5923080133945589760.0000 - mae: 653664768.0000 - val_loss: 956927097324961792.0000 - val_mae: 532277792.0000\n",
      "Epoch 975/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 440915845049221120.0000 - mae: 496814528.0000\n",
      "Epoch 975: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 5922016906201530368.0000 - mae: 653904320.0000 - val_loss: 956754886316261376.0000 - val_mae: 532266240.0000\n",
      "Epoch 976/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1989206612719435776.0000 - mae: 564545984.0000\n",
      "Epoch 976: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 5921066378399318016.0000 - mae: 654189568.0000 - val_loss: 956844221636018176.0000 - val_mae: 532471808.0000\n",
      "Epoch 977/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9127963014866141184.0000 - mae: 719199424.0000\n",
      "Epoch 977: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 5920298369527316480.0000 - mae: 654600320.0000 - val_loss: 956951080422342656.0000 - val_mae: 532684704.0000\n",
      "Epoch 978/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16105301077011726336.0000 - mae: 1017756992.0000\n",
      "Epoch 978: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 5919848669271556096.0000 - mae: 655201856.0000 - val_loss: 957309108896137216.0000 - val_mae: 533085312.0000\n",
      "Epoch 979/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1956487758016937984.0000 - mae: 560114560.0000\n",
      "Epoch 979: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5917846458597376000.0000 - mae: 655525120.0000 - val_loss: 957210496447021056.0000 - val_mae: 533084448.0000\n",
      "Epoch 980/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14580953549425868800.0000 - mae: 955159488.0000\n",
      "Epoch 980: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 5917809075202031616.0000 - mae: 655849024.0000 - val_loss: 957442836997865472.0000 - val_mae: 533387040.0000\n",
      "Epoch 981/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 463869902625701888.0000 - mae: 491825280.0000\n",
      "Epoch 981: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 5915929460074348544.0000 - mae: 656166656.0000 - val_loss: 957390404037115904.0000 - val_mae: 533431456.0000\n",
      "Epoch 982/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9075834618837467136.0000 - mae: 689947776.0000\n",
      "Epoch 982: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 5915469314458124288.0000 - mae: 656475328.0000 - val_loss: 957476646980419584.0000 - val_mae: 533633440.0000\n",
      "Epoch 983/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5823539147259772928.0000 - mae: 695384320.0000\n",
      "Epoch 983: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 5914325272609423360.0000 - mae: 656845440.0000 - val_loss: 957578695403372544.0000 - val_mae: 533814560.0000\n",
      "Epoch 984/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5781029278950883328.0000 - mae: 688316160.0000\n",
      "Epoch 984: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 5913449511597899776.0000 - mae: 657184704.0000 - val_loss: 957566394617036800.0000 - val_mae: 533877888.0000\n",
      "Epoch 985/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14545322775616159744.0000 - mae: 936679168.0000\n",
      "Epoch 985: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 5912828837284020224.0000 - mae: 657514496.0000 - val_loss: 957613673617031168.0000 - val_mae: 534019584.0000\n",
      "Epoch 986/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9105255900729311232.0000 - mae: 704687296.0000\n",
      "Epoch 986: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 5911485234074877952.0000 - mae: 657671616.0000 - val_loss: 957416723596705792.0000 - val_mae: 533982048.0000\n",
      "Epoch 987/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2077917547310612480.0000 - mae: 603270848.0000\n",
      "Epoch 987: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5910357135144779776.0000 - mae: 657770048.0000 - val_loss: 957328212910669824.0000 - val_mae: 533991328.0000\n",
      "Epoch 988/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10715393922584018944.0000 - mae: 847366272.0000\n",
      "Epoch 988: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 5909906885133205504.0000 - mae: 658076160.0000 - val_loss: 957509494890299392.0000 - val_mae: 534223616.0000\n",
      "Epoch 989/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 388601837713883136.0000 - mae: 458219072.0000\n",
      "Epoch 989: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 5908883789563559936.0000 - mae: 658333824.0000 - val_loss: 957402567384498176.0000 - val_mae: 534216608.0000\n",
      "Epoch 990/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16033881199717908480.0000 - mae: 1033545472.0000\n",
      "Epoch 990: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 5908399454691524608.0000 - mae: 658684096.0000 - val_loss: 957601235391741952.0000 - val_mae: 534438400.0000\n",
      "Epoch 991/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1886679627330486272.0000 - mae: 544761216.0000\n",
      "Epoch 991: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 5906835949156827136.0000 - mae: 658810944.0000 - val_loss: 957379683798745088.0000 - val_mae: 534351968.0000\n",
      "Epoch 992/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14478911173786861568.0000 - mae: 919870592.0000\n",
      "Epoch 992: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5906611099028946944.0000 - mae: 659029184.0000 - val_loss: 957464414913560576.0000 - val_mae: 534508800.0000\n",
      "Epoch 993/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 341799957046493184.0000 - mae: 442410592.0000\n",
      "Epoch 993: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 5905081128598896640.0000 - mae: 659169600.0000 - val_loss: 957303954935382016.0000 - val_mae: 534443168.0000\n",
      "Epoch 994/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 313496088206114816.0000 - mae: 438131040.0000\n",
      "Epoch 994: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 5904534671319891968.0000 - mae: 659287552.0000 - val_loss: 957217368394694656.0000 - val_mae: 534458880.0000\n",
      "Epoch 995/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10724131741489954816.0000 - mae: 843099520.0000\n",
      "Epoch 995: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5903616579110699008.0000 - mae: 659544512.0000 - val_loss: 957215925285683200.0000 - val_mae: 534558304.0000\n",
      "Epoch 996/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5767863176964079616.0000 - mae: 688380032.0000\n",
      "Epoch 996: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 5902858465843347456.0000 - mae: 659789888.0000 - val_loss: 957262448371433472.0000 - val_mae: 534661408.0000\n",
      "Epoch 997/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1929988153398001664.0000 - mae: 580991872.0000\n",
      "Epoch 997: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 5901786442006265856.0000 - mae: 659988480.0000 - val_loss: 957256401057480704.0000 - val_mae: 534728832.0000\n",
      "Epoch 998/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 334413575370047488.0000 - mae: 452957952.0000\n",
      "Epoch 998: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 5900797981052895232.0000 - mae: 660109184.0000 - val_loss: 957093123580755968.0000 - val_mae: 534666496.0000\n",
      "Epoch 999/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7253551875717332992.0000 - mae: 764367040.0000\n",
      "Epoch 999: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 5900237230122729472.0000 - mae: 660316992.0000 - val_loss: 957218811503706112.0000 - val_mae: 534864672.0000\n",
      "Epoch 1000/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10615651725270319104.0000 - mae: 803936640.0000\n",
      "Epoch 1000: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 5899430188587941888.0000 - mae: 660663616.0000 - val_loss: 957262860688293888.0000 - val_mae: 535013248.0000\n",
      "Epoch 1001/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9137312162237120512.0000 - mae: 707726336.0000\n",
      "Epoch 1001: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 5898632492901990400.0000 - mae: 660930368.0000 - val_loss: 957191942188302336.0000 - val_mae: 535073952.0000\n",
      "Epoch 1002/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5757860369930387456.0000 - mae: 681564928.0000\n",
      "Epoch 1002: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5897422480355622912.0000 - mae: 661087168.0000 - val_loss: 957077592979013632.0000 - val_mae: 535038016.0000\n",
      "Epoch 1003/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10594878652086747136.0000 - mae: 766520256.0000\n",
      "Epoch 1003: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 5896751228506865664.0000 - mae: 661214208.0000 - val_loss: 956987501745012736.0000 - val_mae: 535094336.0000\n",
      "Epoch 1004/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 407298483188400128.0000 - mae: 479819840.0000\n",
      "Epoch 1004: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 5895727033425592320.0000 - mae: 661285568.0000 - val_loss: 956819894941253632.0000 - val_mae: 535064128.0000\n",
      "Epoch 1005/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7264579977343926272.0000 - mae: 768061312.0000\n",
      "Epoch 1005: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 5895195419553562624.0000 - mae: 661558400.0000 - val_loss: 957028939589484544.0000 - val_mae: 535320704.0000\n",
      "Epoch 1006/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9034987212109774848.0000 - mae: 717493888.0000\n",
      "Epoch 1006: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 5894318559030411264.0000 - mae: 661825984.0000 - val_loss: 956992930583674880.0000 - val_mae: 535406048.0000\n",
      "Epoch 1007/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7378327204016422912.0000 - mae: 806815360.0000\n",
      "Epoch 1007: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 5893314704914251776.0000 - mae: 662061888.0000 - val_loss: 957083571573489664.0000 - val_mae: 535559040.0000\n",
      "Epoch 1008/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 354112047736160256.0000 - mae: 433106336.0000\n",
      "Epoch 1008: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 5892458735112028160.0000 - mae: 662234752.0000 - val_loss: 956970253156352000.0000 - val_mae: 535566944.0000\n",
      "Epoch 1009/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1864273367062806528.0000 - mae: 536372096.0000\n",
      "Epoch 1009: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 5891496662437724160.0000 - mae: 662306560.0000 - val_loss: 956748083088064512.0000 - val_mae: 535510048.0000\n",
      "Epoch 1010/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1976477704043626496.0000 - mae: 585547008.0000\n",
      "Epoch 1010: val_loss did not improve from 956613599072092160.00000\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 5890649488728522752.0000 - mae: 662277184.0000 - val_loss: 956623357237788672.0000 - val_mae: 535508064.0000\n",
      "Epoch 1011/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 287095233475248128.0000 - mae: 430026240.0000\n",
      "Epoch 1011: val_loss improved from 956613599072092160.00000 to 956496569803210752.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch1011-val_Loss956496569803210752.000-mae662355712.000.h5\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 5889964492984418304.0000 - mae: 662355712.0000 - val_loss: 956496569803210752.0000 - val_mae: 535536960.0000\n",
      "Epoch 1012/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 381348462584659968.0000 - mae: 458428160.0000\n",
      "Epoch 1012: val_loss improved from 956496569803210752.00000 to 956405104179675136.00000, saving model to D:/TFT(IGZO_SAL)_DL/DL/DNN/TrainResults/checkpoints/OnOffratio_lr0.001_epoch30000_Adam_zscore_patience300_batch128_240525\\epoch1012-val_Loss956405104179675136.000-mae662487168.000.h5\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 5889089281728708608.0000 - mae: 662487168.0000 - val_loss: 956405104179675136.0000 - val_mae: 535588800.0000\n",
      "Epoch 1013/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 464713606001328128.0000 - mae: 476523968.0000\n",
      "Epoch 1013: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 5888248155333459968.0000 - mae: 662613568.0000 - val_loss: 956503166872977408.0000 - val_mae: 535780576.0000\n",
      "Epoch 1014/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5763468428987858944.0000 - mae: 669411584.0000\n",
      "Epoch 1014: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 5887552164473077760.0000 - mae: 662972736.0000 - val_loss: 956718396274114560.0000 - val_mae: 536064672.0000\n",
      "Epoch 1015/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7323663883929911296.0000 - mae: 770810624.0000\n",
      "Epoch 1015: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 5886531267926687744.0000 - mae: 663384000.0000 - val_loss: 957008667343847424.0000 - val_mae: 536396672.0000\n",
      "Epoch 1016/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 440518783912640512.0000 - mae: 464649088.0000\n",
      "Epoch 1016: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 5885634066438422528.0000 - mae: 663750400.0000 - val_loss: 957031963246460928.0000 - val_mae: 536574688.0000\n",
      "Epoch 1017/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2028590294592651264.0000 - mae: 591951936.0000\n",
      "Epoch 1017: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 5884578535275757568.0000 - mae: 664008960.0000 - val_loss: 957051204699947008.0000 - val_mae: 536720224.0000\n",
      "Epoch 1018/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14503167499807227904.0000 - mae: 966177024.0000\n",
      "Epoch 1018: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 5884704429357137920.0000 - mae: 664460928.0000 - val_loss: 957484412281290752.0000 - val_mae: 537187136.0000\n",
      "Epoch 1019/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5649628843338629120.0000 - mae: 623506688.0000\n",
      "Epoch 1019: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 5883209093543362560.0000 - mae: 664939904.0000 - val_loss: 957666381455687680.0000 - val_mae: 537439680.0000\n",
      "Epoch 1020/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10635069100616843264.0000 - mae: 824238592.0000\n",
      "Epoch 1020: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5882384459822530560.0000 - mae: 665270400.0000 - val_loss: 957784922553057280.0000 - val_mae: 537660032.0000\n",
      "Epoch 1021/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10686657086680465408.0000 - mae: 839438336.0000\n",
      "Epoch 1021: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 5881419638369157120.0000 - mae: 665512704.0000 - val_loss: 957776401337942016.0000 - val_mae: 537774080.0000\n",
      "Epoch 1022/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14375028215682957312.0000 - mae: 902260864.0000\n",
      "Epoch 1022: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 5880866034264571904.0000 - mae: 665787968.0000 - val_loss: 957830208688226304.0000 - val_mae: 537915584.0000\n",
      "Epoch 1023/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 392392404050640896.0000 - mae: 459072960.0000\n",
      "Epoch 1023: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 5879750029962379264.0000 - mae: 665868032.0000 - val_loss: 957625355928076288.0000 - val_mae: 537881792.0000\n",
      "Epoch 1024/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 327487854705901568.0000 - mae: 429374976.0000\n",
      "Epoch 1024: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 5878780260706680832.0000 - mae: 665787008.0000 - val_loss: 957396176473161728.0000 - val_mae: 537801472.0000\n",
      "Epoch 1025/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 459103554079031296.0000 - mae: 470212736.0000\n",
      "Epoch 1025: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 5877979816241659904.0000 - mae: 665755008.0000 - val_loss: 957246093135970304.0000 - val_mae: 537799808.0000\n",
      "Epoch 1026/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 378982828957761536.0000 - mae: 453675968.0000\n",
      "Epoch 1026: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 5877408619951030272.0000 - mae: 665783680.0000 - val_loss: 957172425856909312.0000 - val_mae: 537872448.0000\n",
      "Epoch 1027/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 341416880323428352.0000 - mae: 464677760.0000\n",
      "Epoch 1027: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 5876572441358106624.0000 - mae: 665947136.0000 - val_loss: 957145487822028800.0000 - val_mae: 537984320.0000\n",
      "Epoch 1028/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9122380244576108544.0000 - mae: 751267392.0000\n",
      "Epoch 1028: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 5875946269486088192.0000 - mae: 666167040.0000 - val_loss: 957171463784235008.0000 - val_mae: 538158592.0000\n",
      "Epoch 1029/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5627750211213328384.0000 - mae: 643768320.0000\n",
      "Epoch 1029: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5874825317381570560.0000 - mae: 666300160.0000 - val_loss: 957114632776974336.0000 - val_mae: 538209024.0000\n",
      "Epoch 1030/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5578847782300549120.0000 - mae: 611349888.0000\n",
      "Epoch 1030: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 5874162311870021632.0000 - mae: 666447488.0000 - val_loss: 957206441997893632.0000 - val_mae: 538396992.0000\n",
      "Epoch 1031/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10604645613876281344.0000 - mae: 840980480.0000\n",
      "Epoch 1031: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 5873281603056173056.0000 - mae: 666669376.0000 - val_loss: 957105493086568448.0000 - val_mae: 538472000.0000\n",
      "Epoch 1032/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 351622719051137024.0000 - mae: 467822720.0000\n",
      "Epoch 1032: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 5872491054195802112.0000 - mae: 666728640.0000 - val_loss: 957044607630180352.0000 - val_mae: 538525120.0000\n",
      "Epoch 1033/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5819028400806821888.0000 - mae: 715103552.0000\n",
      "Epoch 1033: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 5871831896974950400.0000 - mae: 666910400.0000 - val_loss: 957059794634539008.0000 - val_mae: 538651200.0000\n",
      "Epoch 1034/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9112788105135390720.0000 - mae: 743794496.0000\n",
      "Epoch 1034: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 5870913804765757440.0000 - mae: 667051968.0000 - val_loss: 956937405246472192.0000 - val_mae: 538715008.0000\n",
      "Epoch 1035/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1877264234383933440.0000 - mae: 550369920.0000\n",
      "Epoch 1035: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 5869918196986806272.0000 - mae: 667135552.0000 - val_loss: 956978293335130112.0000 - val_mae: 538853760.0000\n",
      "Epoch 1036/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 330388538178666496.0000 - mae: 453644416.0000\n",
      "Epoch 1036: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 5869280480242696192.0000 - mae: 667287744.0000 - val_loss: 956909024102580224.0000 - val_mae: 538924672.0000\n",
      "Epoch 1037/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5609728665878265856.0000 - mae: 618310336.0000\n",
      "Epoch 1037: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 5868495978696278016.0000 - mae: 667407616.0000 - val_loss: 956933831833681920.0000 - val_mae: 539048896.0000\n",
      "Epoch 1038/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1917898060977930240.0000 - mae: 558171520.0000\n",
      "Epoch 1038: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 5867355235382460416.0000 - mae: 667497792.0000 - val_loss: 956794056418000896.0000 - val_mae: 539059648.0000\n",
      "Epoch 1039/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14307906329342115840.0000 - mae: 896726848.0000\n",
      "Epoch 1039: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 5867329396859207680.0000 - mae: 667760000.0000 - val_loss: 956961594502283264.0000 - val_mae: 539330432.0000\n",
      "Epoch 1040/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10703173950352916480.0000 - mae: 850324608.0000\n",
      "Epoch 1040: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 5866045717033779200.0000 - mae: 667940416.0000 - val_loss: 956790689163640832.0000 - val_mae: 539377472.0000\n",
      "Epoch 1041/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 381253183030165504.0000 - mae: 491194944.0000\n",
      "Epoch 1041: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 5865052308278083584.0000 - mae: 667884864.0000 - val_loss: 956601092127326208.0000 - val_mae: 539363456.0000\n",
      "Epoch 1042/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15788480100002955264.0000 - mae: 980644096.0000\n",
      "Epoch 1042: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 5864862092766478336.0000 - mae: 668157376.0000 - val_loss: 956890882160721920.0000 - val_mae: 539691136.0000\n",
      "Epoch 1043/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 399637498482917376.0000 - mae: 469384320.0000\n",
      "Epoch 1043: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 5863640535348019200.0000 - mae: 668352960.0000 - val_loss: 956788352701431808.0000 - val_mae: 539718720.0000\n",
      "Epoch 1044/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8974184219093762048.0000 - mae: 705612992.0000\n",
      "Epoch 1044: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 5862835143080673280.0000 - mae: 668457344.0000 - val_loss: 956650501431099392.0000 - val_mae: 539745856.0000\n",
      "Epoch 1045/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1881759175357235200.0000 - mae: 551733184.0000\n",
      "Epoch 1045: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 5861754323150569472.0000 - mae: 668443392.0000 - val_loss: 956568793973260288.0000 - val_mae: 539765696.0000\n",
      "Epoch 1046/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9033779398586662912.0000 - mae: 732317184.0000\n",
      "Epoch 1046: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 5861328812150620160.0000 - mae: 668603392.0000 - val_loss: 956573879214538752.0000 - val_mae: 539895936.0000\n",
      "Epoch 1047/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14334034024152956928.0000 - mae: 952844992.0000\n",
      "Epoch 1047: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 5860841178743701504.0000 - mae: 668877824.0000 - val_loss: 956846832976134144.0000 - val_mae: 540198912.0000\n",
      "Epoch 1048/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5667201238173745152.0000 - mae: 659214848.0000\n",
      "Epoch 1048: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 5859626218395009024.0000 - mae: 669156288.0000 - val_loss: 956924829582229504.0000 - val_mae: 540334400.0000\n",
      "Epoch 1049/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14350970901267218432.0000 - mae: 916336768.0000\n",
      "Epoch 1049: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 5859178717162504192.0000 - mae: 669419008.0000 - val_loss: 957056496099655680.0000 - val_mae: 540539264.0000\n",
      "Epoch 1050/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5631008064166428672.0000 - mae: 661279872.0000\n",
      "Epoch 1050: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 5857927472930095104.0000 - mae: 669521024.0000 - val_loss: 956912666234847232.0000 - val_mae: 540489152.0000\n",
      "Epoch 1051/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 454714441099902976.0000 - mae: 507046688.0000\n",
      "Epoch 1051: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 5857195747941810176.0000 - mae: 669433280.0000 - val_loss: 956621226934009856.0000 - val_mae: 540369024.0000\n",
      "Epoch 1052/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14422809692491218944.0000 - mae: 973704064.0000\n",
      "Epoch 1052: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 5857057209476710400.0000 - mae: 669611456.0000 - val_loss: 956894661731942400.0000 - val_mae: 540688448.0000\n",
      "Epoch 1053/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 333146456938512384.0000 - mae: 448807872.0000\n",
      "Epoch 1053: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 5855552527814098944.0000 - mae: 669779072.0000 - val_loss: 956758734606958592.0000 - val_mae: 540678720.0000\n",
      "Epoch 1054/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7184240311969775616.0000 - mae: 736784256.0000\n",
      "Epoch 1054: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 5855124268035080192.0000 - mae: 670036608.0000 - val_loss: 957030863734833152.0000 - val_mae: 540951936.0000\n",
      "Epoch 1055/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1884645805697007616.0000 - mae: 553856640.0000\n",
      "Epoch 1055: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 5853955487174754304.0000 - mae: 670226560.0000 - val_loss: 956924073667985408.0000 - val_mae: 540944768.0000\n",
      "Epoch 1056/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5744548032897089536.0000 - mae: 666675392.0000\n",
      "Epoch 1056: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 5853442015244582912.0000 - mae: 670328448.0000 - val_loss: 956928884031356928.0000 - val_mae: 541034304.0000\n",
      "Epoch 1057/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 389869918218092544.0000 - mae: 450553024.0000\n",
      "Epoch 1057: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 5852732830244667392.0000 - mae: 670425728.0000 - val_loss: 956949637313331200.0000 - val_mae: 541164608.0000\n",
      "Epoch 1058/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5698218461193306112.0000 - mae: 696522880.0000\n",
      "Epoch 1058: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 5851952176988946432.0000 - mae: 670622848.0000 - val_loss: 956986814550245376.0000 - val_mae: 541274880.0000\n",
      "Epoch 1059/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1952699803020296192.0000 - mae: 559340800.0000\n",
      "Epoch 1059: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 5850888949244887040.0000 - mae: 670753408.0000 - val_loss: 956928128117112832.0000 - val_mae: 541314368.0000\n",
      "Epoch 1060/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5621026697609478144.0000 - mae: 655499008.0000\n",
      "Epoch 1060: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 5850515115291443200.0000 - mae: 670944320.0000 - val_loss: 957017051120009216.0000 - val_mae: 541476288.0000\n",
      "Epoch 1061/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5680792851160498176.0000 - mae: 678199360.0000\n",
      "Epoch 1061: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 5849529403117142016.0000 - mae: 671074560.0000 - val_loss: 956931220493565952.0000 - val_mae: 541495424.0000\n",
      "Epoch 1062/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1993183683716055040.0000 - mae: 612614400.0000\n",
      "Epoch 1062: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 5848715214756773888.0000 - mae: 671056960.0000 - val_loss: 956719564505219072.0000 - val_mae: 541448960.0000\n",
      "Epoch 1063/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 377009136866426880.0000 - mae: 481375328.0000\n",
      "Epoch 1063: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 5848087943373127680.0000 - mae: 671084224.0000 - val_loss: 956690771044466688.0000 - val_mae: 541559488.0000\n",
      "Epoch 1064/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8968136355385180160.0000 - mae: 706782848.0000\n",
      "Epoch 1064: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5847584367047606272.0000 - mae: 671349760.0000 - val_loss: 956815222016835584.0000 - val_mae: 541776192.0000\n",
      "Epoch 1065/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8992005103556755456.0000 - mae: 742732800.0000\n",
      "Epoch 1065: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 5846847694256996352.0000 - mae: 671693952.0000 - val_loss: 957102194551685120.0000 - val_mae: 542079936.0000\n",
      "Epoch 1066/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8970172650919821312.0000 - mae: 696320768.0000\n",
      "Epoch 1066: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5845842740629209088.0000 - mae: 672001536.0000 - val_loss: 957186513349640192.0000 - val_mae: 542272384.0000\n",
      "Epoch 1067/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10609752845387300864.0000 - mae: 850532288.0000\n",
      "Epoch 1067: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 5845176986338590720.0000 - mae: 672291776.0000 - val_loss: 957429986455715840.0000 - val_mae: 542561792.0000\n",
      "Epoch 1068/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7358831763344326656.0000 - mae: 819260672.0000\n",
      "Epoch 1068: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 5844373793094500352.0000 - mae: 672621440.0000 - val_loss: 957605564718776320.0000 - val_mae: 542763904.0000\n",
      "Epoch 1069/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10579781257925754880.0000 - mae: 830630080.0000\n",
      "Epoch 1069: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5843559054978318336.0000 - mae: 672802496.0000 - val_loss: 957547909077794816.0000 - val_mae: 542822720.0000\n",
      "Epoch 1070/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10562499134160371712.0000 - mae: 826654720.0000\n",
      "Epoch 1070: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 5842898798245838848.0000 - mae: 673005632.0000 - val_loss: 957635732569063424.0000 - val_mae: 542987776.0000\n",
      "Epoch 1071/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10557322633416802304.0000 - mae: 837609344.0000\n",
      "Epoch 1071: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 5842256133699403776.0000 - mae: 673221248.0000 - val_loss: 957789458038521856.0000 - val_mae: 543175552.0000\n",
      "Epoch 1072/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1965936548629184512.0000 - mae: 560608576.0000\n",
      "Epoch 1072: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5841255578118127616.0000 - mae: 673364992.0000 - val_loss: 957700672474578944.0000 - val_mae: 543151872.0000\n",
      "Epoch 1073/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 457708376902598656.0000 - mae: 529340864.0000\n",
      "Epoch 1073: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 5840842711501897728.0000 - mae: 673367808.0000 - val_loss: 957668374320513024.0000 - val_mae: 543207296.0000\n",
      "Epoch 1074/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14199486786262007808.0000 - mae: 923880384.0000\n",
      "Epoch 1074: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 5840460631211245568.0000 - mae: 673655040.0000 - val_loss: 957885115550138368.0000 - val_mae: 543458112.0000\n",
      "Epoch 1075/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5674939051254218752.0000 - mae: 660059072.0000\n",
      "Epoch 1075: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 5839301745955569664.0000 - mae: 673824832.0000 - val_loss: 957808218455670784.0000 - val_mae: 543460736.0000\n",
      "Epoch 1076/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10515476320375275520.0000 - mae: 827511040.0000\n",
      "Epoch 1076: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5838771231595167744.0000 - mae: 673949184.0000 - val_loss: 957887795609731072.0000 - val_mae: 543620288.0000\n",
      "Epoch 1077/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14191993614518714368.0000 - mae: 909822912.0000\n",
      "Epoch 1077: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 5838350668397543424.0000 - mae: 674213888.0000 - val_loss: 957962424961466368.0000 - val_mae: 543759296.0000\n",
      "Epoch 1078/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 271414823273627648.0000 - mae: 423388384.0000\n",
      "Epoch 1078: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5837203328013959168.0000 - mae: 674175168.0000 - val_loss: 957627348792901632.0000 - val_mae: 543564992.0000\n",
      "Epoch 1079/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 356468747831083008.0000 - mae: 452224416.0000\n",
      "Epoch 1079: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 5836445764502421504.0000 - mae: 674103232.0000 - val_loss: 957616903432437760.0000 - val_mae: 543625408.0000\n",
      "Epoch 1080/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15898003552757350400.0000 - mae: 1069466304.0000\n",
      "Epoch 1080: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5836530426897760256.0000 - mae: 674510976.0000 - val_loss: 957933081744900096.0000 - val_mae: 543982080.0000\n",
      "Epoch 1081/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 293639217446125568.0000 - mae: 424073984.0000\n",
      "Epoch 1081: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 5835186823688617984.0000 - mae: 674640128.0000 - val_loss: 957771728413523968.0000 - val_mae: 543952640.0000\n",
      "Epoch 1082/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 483158875629944832.0000 - mae: 510750880.0000\n",
      "Epoch 1082: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 5834487534293352448.0000 - mae: 674689536.0000 - val_loss: 957705620276903936.0000 - val_mae: 543980800.0000\n",
      "Epoch 1083/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10559133529067749376.0000 - mae: 859394240.0000\n",
      "Epoch 1083: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5834049378909683712.0000 - mae: 674902592.0000 - val_loss: 957957752037048320.0000 - val_mae: 544249280.0000\n",
      "Epoch 1084/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1953026495412699136.0000 - mae: 576849344.0000\n",
      "Epoch 1084: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 5833033979921432576.0000 - mae: 675101248.0000 - val_loss: 957906693465833472.0000 - val_mae: 544273664.0000\n",
      "Epoch 1085/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5745589820164407296.0000 - mae: 713054272.0000\n",
      "Epoch 1085: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 5832758002502860800.0000 - mae: 675267520.0000 - val_loss: 958064679542849536.0000 - val_mae: 544423552.0000\n",
      "Epoch 1086/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10582559723809144832.0000 - mae: 865686080.0000\n",
      "Epoch 1086: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 5831885540026220544.0000 - mae: 675452672.0000 - val_loss: 958096840257961984.0000 - val_mae: 544536448.0000\n",
      "Epoch 1087/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7275055024621748224.0000 - mae: 794429440.0000\n",
      "Epoch 1087: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 5831148867235610624.0000 - mae: 675659648.0000 - val_loss: 958283413637300224.0000 - val_mae: 544722112.0000\n",
      "Epoch 1088/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8998497169962958848.0000 - mae: 739905536.0000\n",
      "Epoch 1088: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5830784379131002880.0000 - mae: 675880960.0000 - val_loss: 958446141358211072.0000 - val_mae: 544971584.0000\n",
      "Epoch 1089/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8958524974490976256.0000 - mae: 730570112.0000\n",
      "Epoch 1089: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5830107079968292864.0000 - mae: 676146368.0000 - val_loss: 958532453020991488.0000 - val_mae: 545158208.0000\n",
      "Epoch 1090/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 370994464665108480.0000 - mae: 477126400.0000\n",
      "Epoch 1090: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 5829273650154438656.0000 - mae: 676273024.0000 - val_loss: 958518915284074496.0000 - val_mae: 545249792.0000\n",
      "Epoch 1091/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8892636190440685568.0000 - mae: 712816128.0000\n",
      "Epoch 1091: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 5828671117782417408.0000 - mae: 676411008.0000 - val_loss: 958437345265188864.0000 - val_mae: 545305216.0000\n",
      "Epoch 1092/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5599864397309673472.0000 - mae: 652623488.0000\n",
      "Epoch 1092: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 5827922900119715840.0000 - mae: 676463104.0000 - val_loss: 958529223205584896.0000 - val_mae: 545441344.0000\n",
      "Epoch 1093/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 480584712750891008.0000 - mae: 503728096.0000\n",
      "Epoch 1093: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 5827504535945347072.0000 - mae: 676527680.0000 - val_loss: 958333029099503616.0000 - val_mae: 545409152.0000\n",
      "Epoch 1094/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 388533324395577344.0000 - mae: 447238336.0000\n",
      "Epoch 1094: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 5826532017910579200.0000 - mae: 676524160.0000 - val_loss: 958449165015187456.0000 - val_mae: 545592448.0000\n",
      "Epoch 1095/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8831367004494495744.0000 - mae: 691176064.0000\n",
      "Epoch 1095: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 5826400626271059968.0000 - mae: 676899648.0000 - val_loss: 958932950131408896.0000 - val_mae: 546064192.0000\n",
      "Epoch 1096/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8941025697179107328.0000 - mae: 721127488.0000\n",
      "Epoch 1096: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 5825449548713033728.0000 - mae: 677352576.0000 - val_loss: 959134366917722112.0000 - val_mae: 546344576.0000\n",
      "Epoch 1097/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8982580639639273472.0000 - mae: 740828288.0000\n",
      "Epoch 1097: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5824965213840998400.0000 - mae: 677711424.0000 - val_loss: 959437625968558080.0000 - val_mae: 546657920.0000\n",
      "Epoch 1098/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2032049907929448448.0000 - mae: 590371840.0000\n",
      "Epoch 1098: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 5823869000748105728.0000 - mae: 677960192.0000 - val_loss: 959476452472913920.0000 - val_mae: 546762944.0000\n",
      "Epoch 1099/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14237584864164446208.0000 - mae: 958371584.0000\n",
      "Epoch 1099: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 5823915729992286208.0000 - mae: 678199168.0000 - val_loss: 959790156884213760.0000 - val_mae: 547086528.0000\n",
      "Epoch 1100/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5767138598801375232.0000 - mae: 742383168.0000\n",
      "Epoch 1100: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 5822844805666832384.0000 - mae: 678451392.0000 - val_loss: 959858876360949760.0000 - val_mae: 547230336.0000\n",
      "Epoch 1101/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 422745075370426368.0000 - mae: 483119360.0000\n",
      "Epoch 1101: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 5821982788550656000.0000 - mae: 678497984.0000 - val_loss: 959754903792648192.0000 - val_mae: 547228480.0000\n",
      "Epoch 1102/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7179531103668011008.0000 - mae: 757020928.0000\n",
      "Epoch 1102: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5821446776632115200.0000 - mae: 678615232.0000 - val_loss: 959874063365308416.0000 - val_mae: 547382272.0000\n",
      "Epoch 1103/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 443556253503848448.0000 - mae: 470679904.0000\n",
      "Epoch 1103: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 5820926707632177152.0000 - mae: 678693824.0000 - val_loss: 959708586865328128.0000 - val_mae: 547382784.0000\n",
      "Epoch 1104/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14180139779659661312.0000 - mae: 941889728.0000\n",
      "Epoch 1104: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 5820857988155441152.0000 - mae: 678928064.0000 - val_loss: 960105785440862208.0000 - val_mae: 547771072.0000\n",
      "Epoch 1105/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5550251683885350912.0000 - mae: 640665152.0000\n",
      "Epoch 1105: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 5819641928295120896.0000 - mae: 679246464.0000 - val_loss: 960238482750439424.0000 - val_mae: 547939584.0000\n",
      "Epoch 1106/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 481019913197060096.0000 - mae: 540753408.0000\n",
      "Epoch 1106: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 5819077329074257920.0000 - mae: 679345408.0000 - val_loss: 960160211266437120.0000 - val_mae: 548010304.0000\n",
      "Epoch 1107/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 335077542954270720.0000 - mae: 461919456.0000\n",
      "Epoch 1107: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 5818291178260398080.0000 - mae: 679442944.0000 - val_loss: 960177459855097856.0000 - val_mae: 548102912.0000\n",
      "Epoch 1108/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8816818816391577600.0000 - mae: 689211008.0000\n",
      "Epoch 1108: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5817904699923234816.0000 - mae: 679671936.0000 - val_loss: 960497830055641088.0000 - val_mae: 548445632.0000\n",
      "Epoch 1109/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5713262529040351232.0000 - mae: 709462016.0000\n",
      "Epoch 1109: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5817500079644213248.0000 - mae: 680157248.0000 - val_loss: 960937840865181696.0000 - val_mae: 548832640.0000\n",
      "Epoch 1110/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 410105742532542464.0000 - mae: 482295168.0000\n",
      "Epoch 1110: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 5816419259714109440.0000 - mae: 680428672.0000 - val_loss: 961028756732903424.0000 - val_mae: 548971392.0000\n",
      "Epoch 1111/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5592639506403557376.0000 - mae: 654647936.0000\n",
      "Epoch 1111: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 5816110296946704384.0000 - mae: 680668928.0000 - val_loss: 961326449506123776.0000 - val_mae: 549263488.0000\n",
      "Epoch 1112/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5689971024473358336.0000 - mae: 734066176.0000\n",
      "Epoch 1112: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 5815495120190963712.0000 - mae: 681005952.0000 - val_loss: 961540854273540096.0000 - val_mae: 549487680.0000\n",
      "Epoch 1113/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1975847134125096960.0000 - mae: 595820544.0000\n",
      "Epoch 1113: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5814806276156162048.0000 - mae: 681219520.0000 - val_loss: 961592256442138624.0000 - val_mae: 549620864.0000\n",
      "Epoch 1114/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7256606319019294720.0000 - mae: 797710848.0000\n",
      "Epoch 1114: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 5814145469667868672.0000 - mae: 681387200.0000 - val_loss: 961653348056956928.0000 - val_mae: 549723264.0000\n",
      "Epoch 1115/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8887767552952893440.0000 - mae: 723755264.0000\n",
      "Epoch 1115: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 5813766687912099840.0000 - mae: 681498816.0000 - val_loss: 961630808068587520.0000 - val_mae: 549817472.0000\n",
      "Epoch 1116/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5774935235753934848.0000 - mae: 728009600.0000\n",
      "Epoch 1116: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 5813301044737736704.0000 - mae: 681678784.0000 - val_loss: 961759725806944256.0000 - val_mae: 549992128.0000\n",
      "Epoch 1117/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 403506439382892544.0000 - mae: 492007104.0000\n",
      "Epoch 1117: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 5812599556319215616.0000 - mae: 681705472.0000 - val_loss: 961528484767727616.0000 - val_mae: 549890432.0000\n",
      "Epoch 1118/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9015818876146941952.0000 - mae: 783616448.0000\n",
      "Epoch 1118: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 5812118519982063616.0000 - mae: 681814400.0000 - val_loss: 961752785139793920.0000 - val_mae: 550160704.0000\n",
      "Epoch 1119/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7137524811684642816.0000 - mae: 752529920.0000\n",
      "Epoch 1119: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 5811563266610036736.0000 - mae: 682187968.0000 - val_loss: 962177815103406080.0000 - val_mae: 550508800.0000\n",
      "Epoch 1120/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 389391493221056512.0000 - mae: 499140160.0000\n",
      "Epoch 1120: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 5810806802610126848.0000 - mae: 682406848.0000 - val_loss: 962143111767654400.0000 - val_mae: 550533632.0000\n",
      "Epoch 1121/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2031511559548698624.0000 - mae: 610883904.0000\n",
      "Epoch 1121: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 5810122356621836288.0000 - mae: 682492352.0000 - val_loss: 962229698308341760.0000 - val_mae: 550649408.0000\n",
      "Epoch 1122/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14150029653733015552.0000 - mae: 933577792.0000\n",
      "Epoch 1122: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 5810331263831113728.0000 - mae: 682871296.0000 - val_loss: 962623529629515776.0000 - val_mae: 551039040.0000\n",
      "Epoch 1123/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10516564836886773760.0000 - mae: 844993280.0000\n",
      "Epoch 1123: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 5809274633156820992.0000 - mae: 683234752.0000 - val_loss: 962730938171654144.0000 - val_mae: 551227712.0000\n",
      "Epoch 1124/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10628319198733926400.0000 - mae: 908933504.0000\n",
      "Epoch 1124: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 5808723228075491328.0000 - mae: 683450624.0000 - val_loss: 962943693671628800.0000 - val_mae: 551441280.0000\n",
      "Epoch 1125/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10511088169468821504.0000 - mae: 849353600.0000\n",
      "Epoch 1125: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 5808137738133700608.0000 - mae: 683729536.0000 - val_loss: 963095907312599040.0000 - val_mae: 551599680.0000\n",
      "Epoch 1126/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10432713881129320448.0000 - mae: 822491008.0000\n",
      "Epoch 1126: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 5807546200877957120.0000 - mae: 683933440.0000 - val_loss: 963111025597480960.0000 - val_mae: 551669376.0000\n",
      "Epoch 1127/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1992562871963222016.0000 - mae: 598350464.0000\n",
      "Epoch 1127: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 5806845262215249920.0000 - mae: 683950400.0000 - val_loss: 963057905441964032.0000 - val_mae: 551654144.0000\n",
      "Epoch 1128/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 387494011029422080.0000 - mae: 479035584.0000\n",
      "Epoch 1128: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 5806639103785041920.0000 - mae: 684024576.0000 - val_loss: 963126418760269824.0000 - val_mae: 551770688.0000\n",
      "Epoch 1129/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5549279715606396928.0000 - mae: 644858880.0000\n",
      "Epoch 1129: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 5806072855296737280.0000 - mae: 684214848.0000 - val_loss: 963290039834378240.0000 - val_mae: 551939712.0000\n",
      "Epoch 1130/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2058230379298422784.0000 - mae: 620312384.0000\n",
      "Epoch 1130: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 5805441735622393856.0000 - mae: 684396928.0000 - val_loss: 963330721764605952.0000 - val_mae: 552048512.0000\n",
      "Epoch 1131/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 474130923093229568.0000 - mae: 503539200.0000\n",
      "Epoch 1131: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 5805066802157322240.0000 - mae: 684421568.0000 - val_loss: 963061410135277568.0000 - val_mae: 551922240.0000\n",
      "Epoch 1132/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5587896213241331712.0000 - mae: 687913600.0000\n",
      "Epoch 1132: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 5804426336634142720.0000 - mae: 684478144.0000 - val_loss: 963283167886704640.0000 - val_mae: 552131584.0000\n",
      "Epoch 1133/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5723473693527506944.0000 - mae: 731531392.0000\n",
      "Epoch 1133: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5804125070448132096.0000 - mae: 684830272.0000 - val_loss: 963627108867768320.0000 - val_mae: 552438336.0000\n",
      "Epoch 1134/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 330264808760803328.0000 - mae: 468509760.0000\n",
      "Epoch 1134: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5803497249308672000.0000 - mae: 684998400.0000 - val_loss: 963440260610523136.0000 - val_mae: 552372288.0000\n",
      "Epoch 1135/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5626772745376235520.0000 - mae: 693732032.0000\n",
      "Epoch 1135: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 5803024459308728320.0000 - mae: 685151488.0000 - val_loss: 963853676982566912.0000 - val_mae: 552712064.0000\n",
      "Epoch 1136/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 368867596860129280.0000 - mae: 467816000.0000\n",
      "Epoch 1136: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5802438969366937600.0000 - mae: 685483008.0000 - val_loss: 963988229718016000.0000 - val_mae: 552861952.0000\n",
      "Epoch 1137/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 466793400964743168.0000 - mae: 513855264.0000\n",
      "Epoch 1137: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 5801702296576327680.0000 - mae: 685638976.0000 - val_loss: 964008708122083328.0000 - val_mae: 552943744.0000\n",
      "Epoch 1138/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8834249374226710528.0000 - mae: 732655616.0000\n",
      "Epoch 1138: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 5801418622576361472.0000 - mae: 685871232.0000 - val_loss: 964178239071191040.0000 - val_mae: 553159616.0000\n",
      "Epoch 1139/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15787980921723944960.0000 - mae: 1076208896.0000\n",
      "Epoch 1139: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 5801271288018239488.0000 - mae: 686274560.0000 - val_loss: 964596328367652864.0000 - val_mae: 553516480.0000\n",
      "Epoch 1140/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13992491627705270272.0000 - mae: 893543040.0000\n",
      "Epoch 1140: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 5800603334704365568.0000 - mae: 686663168.0000 - val_loss: 964847154457739264.0000 - val_mae: 553752704.0000\n",
      "Epoch 1141/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7325656748755255296.0000 - mae: 826195584.0000\n",
      "Epoch 1141: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 5799674247378894848.0000 - mae: 686816768.0000 - val_loss: 964796645642338304.0000 - val_mae: 553757120.0000\n",
      "Epoch 1142/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14147362238524030976.0000 - mae: 988462144.0000\n",
      "Epoch 1142: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 5799580239134720000.0000 - mae: 686921792.0000 - val_loss: 964939513434472448.0000 - val_mae: 553929344.0000\n",
      "Epoch 1143/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 340258029427490816.0000 - mae: 468001376.0000\n",
      "Epoch 1143: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5798834770251087872.0000 - mae: 686998784.0000 - val_loss: 964825576542044160.0000 - val_mae: 553923712.0000\n",
      "Epoch 1144/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15745033997543014400.0000 - mae: 1069066368.0000\n",
      "Epoch 1144: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 5798616517192974336.0000 - mae: 687232960.0000 - val_loss: 965035170946088960.0000 - val_mae: 554129856.0000\n",
      "Epoch 1145/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8864020850572001280.0000 - mae: 757615744.0000\n",
      "Epoch 1145: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 5797848508320972800.0000 - mae: 687319680.0000 - val_loss: 964856637745528832.0000 - val_mae: 554090944.0000\n",
      "Epoch 1146/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 411997658446561280.0000 - mae: 487775040.0000\n",
      "Epoch 1146: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 5797374618809401344.0000 - mae: 687239424.0000 - val_loss: 964693978744094720.0000 - val_mae: 554037824.0000\n",
      "Epoch 1147/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8911416398798913536.0000 - mae: 775894528.0000\n",
      "Epoch 1147: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 5796918871239688192.0000 - mae: 687315328.0000 - val_loss: 964763247976644608.0000 - val_mae: 554133248.0000\n",
      "Epoch 1148/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2187135198393532416.0000 - mae: 675809280.0000\n",
      "Epoch 1148: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 5796458175867650048.0000 - mae: 687443392.0000 - val_loss: 964920478139416576.0000 - val_mae: 554261120.0000\n",
      "Epoch 1149/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7191503685782863872.0000 - mae: 775491456.0000\n",
      "Epoch 1149: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5796045309251420160.0000 - mae: 687695744.0000 - val_loss: 965194325254209536.0000 - val_mae: 554455360.0000\n",
      "Epoch 1150/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8809794586357530624.0000 - mae: 724252032.0000\n",
      "Epoch 1150: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 5795646736286351360.0000 - mae: 687911808.0000 - val_loss: 965303176905359360.0000 - val_mae: 554579840.0000\n",
      "Epoch 1151/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7225514878964858880.0000 - mae: 804316928.0000\n",
      "Epoch 1151: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 5795028260995727360.0000 - mae: 688063936.0000 - val_loss: 965389969604476928.0000 - val_mae: 554659264.0000\n",
      "Epoch 1152/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 467702284764053504.0000 - mae: 518395040.0000\n",
      "Epoch 1152: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 5794509841263230976.0000 - mae: 688067328.0000 - val_loss: 965207794271649792.0000 - val_mae: 554567360.0000\n",
      "Epoch 1153/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10496882479237955584.0000 - mae: 850931072.0000\n",
      "Epoch 1153: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 5794185485333037056.0000 - mae: 688101184.0000 - val_loss: 965214597499846656.0000 - val_mae: 554643008.0000\n",
      "Epoch 1154/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8935354965958852608.0000 - mae: 784782976.0000\n",
      "Epoch 1154: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 5793840788437729280.0000 - mae: 688208256.0000 - val_loss: 965207588113219584.0000 - val_mae: 554706816.0000\n",
      "Epoch 1155/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 433905118392352768.0000 - mae: 505567648.0000\n",
      "Epoch 1155: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5793200322914549760.0000 - mae: 688205952.0000 - val_loss: 965094956890849280.0000 - val_mae: 554649472.0000\n",
      "Epoch 1156/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7241751916928040960.0000 - mae: 780177792.0000\n",
      "Epoch 1156: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 5792731930961117184.0000 - mae: 688362624.0000 - val_loss: 965479373643710464.0000 - val_mae: 554936960.0000\n",
      "Epoch 1157/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7203134869537292288.0000 - mae: 741491584.0000\n",
      "Epoch 1157: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 5792514777414631424.0000 - mae: 688806400.0000 - val_loss: 965938007431446528.0000 - val_mae: 555288768.0000\n",
      "Epoch 1158/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8911687978170974208.0000 - mae: 789215872.0000\n",
      "Epoch 1158: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 5791920491379818496.0000 - mae: 689067456.0000 - val_loss: 965928317985226752.0000 - val_mae: 555355584.0000\n",
      "Epoch 1159/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 387952232500297728.0000 - mae: 492265024.0000\n",
      "Epoch 1159: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 5791222301496180736.0000 - mae: 689136768.0000 - val_loss: 965955049861677056.0000 - val_mae: 555393152.0000\n",
      "Epoch 1160/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15721103126964469760.0000 - mae: 1097662976.0000\n",
      "Epoch 1160: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 5791191515170603008.0000 - mae: 689407808.0000 - val_loss: 966436635954642944.0000 - val_mae: 555774848.0000\n",
      "Epoch 1161/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8823556073890775040.0000 - mae: 739616128.0000\n",
      "Epoch 1161: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 5790537305752076288.0000 - mae: 689848256.0000 - val_loss: 966649185296187392.0000 - val_mae: 555961920.0000\n",
      "Epoch 1162/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14049989488768188416.0000 - mae: 950300160.0000\n",
      "Epoch 1162: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 5790201954705604608.0000 - mae: 690159744.0000 - val_loss: 966899873947320320.0000 - val_mae: 556179584.0000\n",
      "Epoch 1163/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5701488408774311936.0000 - mae: 742383616.0000\n",
      "Epoch 1163: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 5789454836554530816.0000 - mae: 690334080.0000 - val_loss: 966913755281620992.0000 - val_mae: 556239168.0000\n",
      "Epoch 1164/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7193754935840735232.0000 - mae: 777476032.0000\n",
      "Epoch 1164: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5788905080740642816.0000 - mae: 690410944.0000 - val_loss: 966854244214767616.0000 - val_mae: 556220032.0000\n",
      "Epoch 1165/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5547615055001944064.0000 - mae: 656402752.0000\n",
      "Epoch 1165: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5788574127740682240.0000 - mae: 690415360.0000 - val_loss: 966805384666808320.0000 - val_mae: 556217280.0000\n",
      "Epoch 1166/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15687895676782379008.0000 - mae: 1041849024.0000\n",
      "Epoch 1166: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 5788559284333707264.0000 - mae: 690647232.0000 - val_loss: 967087340679856128.0000 - val_mae: 556481152.0000\n",
      "Epoch 1167/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 385886043993276416.0000 - mae: 489347232.0000\n",
      "Epoch 1167: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 5787594462880333824.0000 - mae: 690718464.0000 - val_loss: 966895338461855744.0000 - val_mae: 556387648.0000\n",
      "Epoch 1168/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14055048341767585792.0000 - mae: 942647872.0000\n",
      "Epoch 1168: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 5787591164345450496.0000 - mae: 690866880.0000 - val_loss: 967126991817932800.0000 - val_mae: 556610176.0000\n",
      "Epoch 1169/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14053672852721238016.0000 - mae: 948852928.0000\n",
      "Epoch 1169: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 5787168402124570624.0000 - mae: 691163840.0000 - val_loss: 967265049246695424.0000 - val_mae: 556753728.0000\n",
      "Epoch 1170/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 501743096040521728.0000 - mae: 541926400.0000\n",
      "Epoch 1170: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 5786452620054888448.0000 - mae: 691175616.0000 - val_loss: 967069885932765184.0000 - val_mae: 556664448.0000\n",
      "Epoch 1171/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5701557678006861824.0000 - mae: 743295040.0000\n",
      "Epoch 1171: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5785983128589828096.0000 - mae: 691209088.0000 - val_loss: 967108300120260608.0000 - val_mae: 556730880.0000\n",
      "Epoch 1172/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7289895133061840896.0000 - mae: 820331840.0000\n",
      "Epoch 1172: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 5785521333706162176.0000 - mae: 691325504.0000 - val_loss: 967146851746709504.0000 - val_mae: 556790400.0000\n",
      "Epoch 1173/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2040280714536026112.0000 - mae: 611960576.0000\n",
      "Epoch 1173: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 5785225565078290432.0000 - mae: 691277312.0000 - val_loss: 966794939306344448.0000 - val_mae: 556588736.0000\n",
      "Epoch 1174/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8824797972274348032.0000 - mae: 746884096.0000\n",
      "Epoch 1174: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 5784914953043443712.0000 - mae: 691262144.0000 - val_loss: 966983024514170880.0000 - val_mae: 556765312.0000\n",
      "Epoch 1175/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 377938292911374336.0000 - mae: 480464192.0000\n",
      "Epoch 1175: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 5784447110845825024.0000 - mae: 691466304.0000 - val_loss: 967001991089750016.0000 - val_mae: 556812608.0000\n",
      "Epoch 1176/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1954919854435729408.0000 - mae: 564732800.0000\n",
      "Epoch 1176: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 5783958927683092480.0000 - mae: 691563264.0000 - val_loss: 967050850637709312.0000 - val_mae: 556872384.0000\n",
      "Epoch 1177/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5572481609975726080.0000 - mae: 686574272.0000\n",
      "Epoch 1177: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 5783643367845920768.0000 - mae: 691680960.0000 - val_loss: 967094899822297088.0000 - val_mae: 556923648.0000\n",
      "Epoch 1178/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 447866476523683840.0000 - mae: 529967360.0000\n",
      "Epoch 1178: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 5783166179799465984.0000 - mae: 691669824.0000 - val_loss: 966882419200229376.0000 - val_mae: 556822272.0000\n",
      "Epoch 1179/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5551569998327054336.0000 - mae: 672779648.0000\n",
      "Epoch 1179: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 5782708782962311168.0000 - mae: 691693376.0000 - val_loss: 967001853650796544.0000 - val_mae: 556902400.0000\n",
      "Epoch 1180/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 314960465895620608.0000 - mae: 429303424.0000\n",
      "Epoch 1180: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5782267329043759104.0000 - mae: 691761536.0000 - val_loss: 967011062060679168.0000 - val_mae: 556952064.0000\n",
      "Epoch 1181/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1911781615231565824.0000 - mae: 530796160.0000\n",
      "Epoch 1181: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 5781850614136832000.0000 - mae: 691921024.0000 - val_loss: 967108918595551232.0000 - val_mae: 557052096.0000\n",
      "Epoch 1182/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14015177851121172480.0000 - mae: 942562304.0000\n",
      "Epoch 1182: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 5782038630625181696.0000 - mae: 692257728.0000 - val_loss: 967514981983584256.0000 - val_mae: 557347968.0000\n",
      "Epoch 1183/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14004741286750322688.0000 - mae: 939712512.0000\n",
      "Epoch 1183: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 5781481178229899264.0000 - mae: 692614336.0000 - val_loss: 967838375841103872.0000 - val_mae: 557603200.0000\n",
      "Epoch 1184/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2067942502945521664.0000 - mae: 602954688.0000\n",
      "Epoch 1184: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 5780566384555589632.0000 - mae: 692727232.0000 - val_loss: 967735021748092928.0000 - val_mae: 557577344.0000\n",
      "Epoch 1185/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13966909290661806080.0000 - mae: 925672960.0000\n",
      "Epoch 1185: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 5780654345485811712.0000 - mae: 692953728.0000 - val_loss: 968042335248056320.0000 - val_mae: 557822464.0000\n",
      "Epoch 1186/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 457313274271105024.0000 - mae: 541761984.0000\n",
      "Epoch 1186: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 5779902279532412928.0000 - mae: 693041088.0000 - val_loss: 967867856496623616.0000 - val_mae: 557749056.0000\n",
      "Epoch 1187/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5618567639853957120.0000 - mae: 689577344.0000\n",
      "Epoch 1187: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 5779677429404532736.0000 - mae: 693138624.0000 - val_loss: 968115040454443008.0000 - val_mae: 557936256.0000\n",
      "Epoch 1188/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7171475531727110144.0000 - mae: 770320576.0000\n",
      "Epoch 1188: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 5779196393067380736.0000 - mae: 693372160.0000 - val_loss: 968302369748025344.0000 - val_mae: 558056384.0000\n",
      "Epoch 1189/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 368729951748227072.0000 - mae: 488112064.0000\n",
      "Epoch 1189: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 5778808815218589696.0000 - mae: 693430208.0000 - val_loss: 968236673928265728.0000 - val_mae: 558049856.0000\n",
      "Epoch 1190/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 391786504424259584.0000 - mae: 480846400.0000\n",
      "Epoch 1190: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 5778587263625592832.0000 - mae: 693394432.0000 - val_loss: 967921114091094016.0000 - val_mae: 557894784.0000\n",
      "Epoch 1191/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7209834193885331456.0000 - mae: 809184256.0000\n",
      "Epoch 1191: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 5778167799939596288.0000 - mae: 693464960.0000 - val_loss: 968291512070701056.0000 - val_mae: 558174400.0000\n",
      "Epoch 1192/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8797471809789231104.0000 - mae: 733255744.0000\n",
      "Epoch 1192: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 5777792316718710784.0000 - mae: 693755904.0000 - val_loss: 968440908213125120.0000 - val_mae: 558324992.0000\n",
      "Epoch 1193/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 375842864267001856.0000 - mae: 466056736.0000\n",
      "Epoch 1193: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 5777254105776914432.0000 - mae: 693840896.0000 - val_loss: 968297490665177088.0000 - val_mae: 558289728.0000\n",
      "Epoch 1194/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15666776257436057600.0000 - mae: 1068242496.0000\n",
      "Epoch 1194: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 5777338768172253184.0000 - mae: 694084928.0000 - val_loss: 968708295697104896.0000 - val_mae: 558597504.0000\n",
      "Epoch 1195/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2153482033685331968.0000 - mae: 667617280.0000\n",
      "Epoch 1195: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 5776490494951424000.0000 - mae: 694202368.0000 - val_loss: 968528044509626368.0000 - val_mae: 558511552.0000\n",
      "Epoch 1196/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2089917204899299328.0000 - mae: 627631232.0000\n",
      "Epoch 1196: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 5776146897567744000.0000 - mae: 694137984.0000 - val_loss: 968413282983477248.0000 - val_mae: 558456512.0000\n",
      "Epoch 1197/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14132309924339777536.0000 - mae: 1028467776.0000\n",
      "Epoch 1197: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 5776231559963082752.0000 - mae: 694347520.0000 - val_loss: 968874459391852544.0000 - val_mae: 558804736.0000\n",
      "Epoch 1198/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2015077846443098112.0000 - mae: 601424832.0000\n",
      "Epoch 1198: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 5775367343823650816.0000 - mae: 694660544.0000 - val_loss: 969055535213051904.0000 - val_mae: 558941632.0000\n",
      "Epoch 1199/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 532149780869021696.0000 - mae: 544361152.0000\n",
      "Epoch 1199: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 5775172730265534464.0000 - mae: 694841344.0000 - val_loss: 969098416166535168.0000 - val_mae: 559002432.0000\n",
      "Epoch 1200/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5515568689098784768.0000 - mae: 662105792.0000\n",
      "Epoch 1200: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 5774743920730701824.0000 - mae: 695038848.0000 - val_loss: 969336185556041728.0000 - val_mae: 559171392.0000\n",
      "Epoch 1201/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 411048298875453440.0000 - mae: 503338880.0000\n",
      "Epoch 1201: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 5774268931707502592.0000 - mae: 695131072.0000 - val_loss: 969188301242105856.0000 - val_mae: 559124672.0000\n",
      "Epoch 1202/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8854507326212669440.0000 - mae: 763507712.0000\n",
      "Epoch 1202: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 5773969864544747520.0000 - mae: 695203520.0000 - val_loss: 969314676359823360.0000 - val_mae: 559256832.0000\n",
      "Epoch 1203/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 421113022157684736.0000 - mae: 480500960.0000\n",
      "Epoch 1203: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 5773490477475037184.0000 - mae: 695329920.0000 - val_loss: 969366628284235776.0000 - val_mae: 559314752.0000\n",
      "Epoch 1204/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10444976734313906176.0000 - mae: 842887040.0000\n",
      "Epoch 1204: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 5773376128265748480.0000 - mae: 695602240.0000 - val_loss: 969727130659192832.0000 - val_mae: 559581440.0000\n",
      "Epoch 1205/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 468718542745763840.0000 - mae: 521563840.0000\n",
      "Epoch 1205: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 5772958863603007488.0000 - mae: 695819904.0000 - val_loss: 969797293244940288.0000 - val_mae: 559656064.0000\n",
      "Epoch 1206/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 409805094821822464.0000 - mae: 503802496.0000\n",
      "Epoch 1206: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 5772661995463507968.0000 - mae: 695822592.0000 - val_loss: 969545505082179584.0000 - val_mae: 559516160.0000\n",
      "Epoch 1207/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8760149987096002560.0000 - mae: 718793984.0000\n",
      "Epoch 1207: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 5772294758579830784.0000 - mae: 695877696.0000 - val_loss: 969749052172271616.0000 - val_mae: 559699136.0000\n",
      "Epoch 1208/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10494099615308054528.0000 - mae: 914788480.0000\n",
      "Epoch 1208: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 5771776338847334400.0000 - mae: 696109952.0000 - val_loss: 970015821180960768.0000 - val_mae: 559888512.0000\n",
      "Epoch 1209/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5549464433559863296.0000 - mae: 688379072.0000\n",
      "Epoch 1209: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 5771416248789237760.0000 - mae: 696373184.0000 - val_loss: 970320179743424512.0000 - val_mae: 560078784.0000\n",
      "Epoch 1210/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13917843584272302080.0000 - mae: 915770624.0000\n",
      "Epoch 1210: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 5771502560452018176.0000 - mae: 696763648.0000 - val_loss: 970833170637258752.0000 - val_mae: 560435968.0000\n",
      "Epoch 1211/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10504214022771965952.0000 - mae: 915844096.0000\n",
      "Epoch 1211: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5770749394986991616.0000 - mae: 697116288.0000 - val_loss: 971059395154673664.0000 - val_mae: 560627776.0000\n",
      "Epoch 1212/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15574823000493522944.0000 - mae: 1046037824.0000\n",
      "Epoch 1212: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 5770627349196308480.0000 - mae: 697468416.0000 - val_loss: 971419210334863360.0000 - val_mae: 560885120.0000\n",
      "Epoch 1213/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15515067842058780672.0000 - mae: 1036766464.0000\n",
      "Epoch 1213: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5770218880626589696.0000 - mae: 697773248.0000 - val_loss: 971652787836289024.0000 - val_mae: 561060224.0000\n",
      "Epoch 1214/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 378690839901110272.0000 - mae: 452357888.0000\n",
      "Epoch 1214: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 5769804364742918144.0000 - mae: 697766400.0000 - val_loss: 971285413513658368.0000 - val_mae: 560859648.0000\n",
      "Epoch 1215/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13960809200150904832.0000 - mae: 953021312.0000\n",
      "Epoch 1215: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 5769689465777815552.0000 - mae: 697859904.0000 - val_loss: 971652581677858816.0000 - val_mae: 561132416.0000\n",
      "Epoch 1216/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 377943515591606272.0000 - mae: 493504960.0000\n",
      "Epoch 1216: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 5768908812522094592.0000 - mae: 697980352.0000 - val_loss: 971532803629907968.0000 - val_mae: 561072512.0000\n",
      "Epoch 1217/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 379779596930777088.0000 - mae: 486645376.0000\n",
      "Epoch 1217: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5768625688277942272.0000 - mae: 697987584.0000 - val_loss: 971480095791251456.0000 - val_mae: 561074496.0000\n",
      "Epoch 1218/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7257336394740137984.0000 - mae: 843392896.0000\n",
      "Epoch 1218: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 5768262299684962304.0000 - mae: 698141312.0000 - val_loss: 971667150206926848.0000 - val_mae: 561199616.0000\n",
      "Epoch 1219/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8788707602604228608.0000 - mae: 733743872.0000\n",
      "Epoch 1219: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5768212821661712384.0000 - mae: 698384256.0000 - val_loss: 971933919215616000.0000 - val_mae: 561420544.0000\n",
      "Epoch 1220/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5640119717025808384.0000 - mae: 712185536.0000\n",
      "Epoch 1220: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 5767694951685029888.0000 - mae: 698659648.0000 - val_loss: 972140421243207680.0000 - val_mae: 561573568.0000\n",
      "Epoch 1221/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 385365940633600000.0000 - mae: 475108288.0000\n",
      "Epoch 1221: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 5767382140626927616.0000 - mae: 698715648.0000 - val_loss: 971904301121142784.0000 - val_mae: 561448640.0000\n",
      "Epoch 1222/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5560973571523608576.0000 - mae: 698106432.0000\n",
      "Epoch 1222: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 5767031396417667072.0000 - mae: 698793600.0000 - val_loss: 972230787355115520.0000 - val_mae: 561682816.0000\n",
      "Epoch 1223/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5573233675929124864.0000 - mae: 701318912.0000\n",
      "Epoch 1223: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 5766570701045628928.0000 - mae: 699080192.0000 - val_loss: 972385131299864576.0000 - val_mae: 561801024.0000\n",
      "Epoch 1224/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10346163624325677056.0000 - mae: 817147008.0000\n",
      "Epoch 1224: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 5766383784068907008.0000 - mae: 699294976.0000 - val_loss: 972549233410310144.0000 - val_mae: 561958720.0000\n",
      "Epoch 1225/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2022269339683520512.0000 - mae: 591606848.0000\n",
      "Epoch 1225: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 5765969268185235456.0000 - mae: 699468672.0000 - val_loss: 972676708039655424.0000 - val_mae: 562054336.0000\n",
      "Epoch 1226/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5567470585732136960.0000 - mae: 697034624.0000\n",
      "Epoch 1226: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5765706484906196992.0000 - mae: 699697728.0000 - val_loss: 972871802634108928.0000 - val_mae: 562200256.0000\n",
      "Epoch 1227/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5622303780365139968.0000 - mae: 731563648.0000\n",
      "Epoch 1227: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5765256784650436608.0000 - mae: 699852864.0000 - val_loss: 972853385814343680.0000 - val_mae: 562215232.0000\n",
      "Epoch 1228/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7182694948376936448.0000 - mae: 799664128.0000\n",
      "Epoch 1228: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 5764899993127223296.0000 - mae: 699922752.0000 - val_loss: 972928908519276544.0000 - val_mae: 562284800.0000\n",
      "Epoch 1229/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13927945897108307968.0000 - mae: 919697984.0000\n",
      "Epoch 1229: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 5765077014499295232.0000 - mae: 700191488.0000 - val_loss: 973250859267784704.0000 - val_mae: 562542464.0000\n",
      "Epoch 1230/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2042155656739291136.0000 - mae: 588089856.0000\n",
      "Epoch 1230: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 5764177613987774464.0000 - mae: 700314176.0000 - val_loss: 973138434203844608.0000 - val_mae: 562495296.0000\n",
      "Epoch 1231/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13932305460712439808.0000 - mae: 962532672.0000\n",
      "Epoch 1231: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 5764321650011013120.0000 - mae: 700485888.0000 - val_loss: 973372836338991104.0000 - val_mae: 562680192.0000\n",
      "Epoch 1232/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 330438153640869888.0000 - mae: 446608320.0000\n",
      "Epoch 1232: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 5763604768429703168.0000 - mae: 700539840.0000 - val_loss: 973190592286687232.0000 - val_mae: 562594112.0000\n",
      "Epoch 1233/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13929265311061639168.0000 - mae: 929022848.0000\n",
      "Epoch 1233: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 5763699876185505792.0000 - mae: 700725632.0000 - val_loss: 973452757090435072.0000 - val_mae: 562814080.0000\n",
      "Epoch 1234/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15530515980429033472.0000 - mae: 1031633024.0000\n",
      "Epoch 1234: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 5763247976906489856.0000 - mae: 700998848.0000 - val_loss: 973664000761921536.0000 - val_mae: 562974656.0000\n",
      "Epoch 1235/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10474168768031358976.0000 - mae: 873089536.0000\n",
      "Epoch 1235: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 5762733955220504576.0000 - mae: 701141120.0000 - val_loss: 973661045824421888.0000 - val_mae: 563001664.0000\n",
      "Epoch 1236/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10376231968810467328.0000 - mae: 824379456.0000\n",
      "Epoch 1236: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5762498659732160512.0000 - mae: 701299456.0000 - val_loss: 973702346229940224.0000 - val_mae: 563056064.0000\n",
      "Epoch 1237/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 467097037972701184.0000 - mae: 525971008.0000\n",
      "Epoch 1237: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5762185298918244352.0000 - mae: 701221888.0000 - val_loss: 973343355683471360.0000 - val_mae: 562858496.0000\n",
      "Epoch 1238/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 426688714341613568.0000 - mae: 522951776.0000\n",
      "Epoch 1238: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 5761907672232230912.0000 - mae: 701081216.0000 - val_loss: 973249072561389568.0000 - val_mae: 562807616.0000\n",
      "Epoch 1239/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5511311380076036096.0000 - mae: 670419392.0000\n",
      "Epoch 1239: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 5761539885592739840.0000 - mae: 701097472.0000 - val_loss: 973216224651509760.0000 - val_mae: 562785024.0000\n",
      "Epoch 1240/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 383739247540043776.0000 - mae: 501898624.0000\n",
      "Epoch 1240: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 5761230373069520896.0000 - mae: 701069952.0000 - val_loss: 973151353465470976.0000 - val_mae: 562777280.0000\n",
      "Epoch 1241/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8848915210073800704.0000 - mae: 787217984.0000\n",
      "Epoch 1241: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 5761176496999759872.0000 - mae: 701233152.0000 - val_loss: 973358336529399808.0000 - val_mae: 562960768.0000\n",
      "Epoch 1242/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8778485443000795136.0000 - mae: 767285632.0000\n",
      "Epoch 1242: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 5760761981116088320.0000 - mae: 701471360.0000 - val_loss: 973529997782286336.0000 - val_mae: 563100032.0000\n",
      "Epoch 1243/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10492162275819913216.0000 - mae: 892104576.0000\n",
      "Epoch 1243: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 5760422232023105536.0000 - mae: 701671360.0000 - val_loss: 973642835163086848.0000 - val_mae: 563212992.0000\n",
      "Epoch 1244/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8817275663472918528.0000 - mae: 782645248.0000\n",
      "Epoch 1244: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 5760173192639414272.0000 - mae: 701771648.0000 - val_loss: 973521339128217600.0000 - val_mae: 563181248.0000\n",
      "Epoch 1245/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 450001315787964416.0000 - mae: 535851584.0000\n",
      "Epoch 1245: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 5759818600139456512.0000 - mae: 701726528.0000 - val_loss: 973527042844786688.0000 - val_mae: 563185408.0000\n",
      "Epoch 1246/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8766265470769692672.0000 - mae: 751062656.0000\n",
      "Epoch 1246: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 5759608043662737408.0000 - mae: 701876928.0000 - val_loss: 973666612102037504.0000 - val_mae: 563314304.0000\n",
      "Epoch 1247/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13904754997855256576.0000 - mae: 962642496.0000\n",
      "Epoch 1247: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 5759438169116246016.0000 - mae: 702092160.0000 - val_loss: 973891668388347904.0000 - val_mae: 563506816.0000\n",
      "Epoch 1248/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13949699734663856128.0000 - mae: 972318336.0000\n",
      "Epoch 1248: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 5759131955127910400.0000 - mae: 702320000.0000 - val_loss: 974086762982801408.0000 - val_mae: 563688064.0000\n",
      "Epoch 1249/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8724287766088646656.0000 - mae: 748430976.0000\n",
      "Epoch 1249: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 5758654767081455616.0000 - mae: 702382656.0000 - val_loss: 973909604171776000.0000 - val_mae: 563650112.0000\n",
      "Epoch 1250/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5524150927109390336.0000 - mae: 698430528.0000\n",
      "Epoch 1250: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 5758411225255903232.0000 - mae: 702355840.0000 - val_loss: 973997427663044608.0000 - val_mae: 563749376.0000\n",
      "Epoch 1251/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5545635384316133376.0000 - mae: 692480896.0000\n",
      "Epoch 1251: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 5758101712732684288.0000 - mae: 702398784.0000 - val_loss: 973926852760436736.0000 - val_mae: 563749888.0000\n",
      "Epoch 1252/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 470140554877861888.0000 - mae: 514098624.0000\n",
      "Epoch 1252: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 5757916994779217920.0000 - mae: 702307712.0000 - val_loss: 973728871947960320.0000 - val_mae: 563681344.0000\n",
      "Epoch 1253/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5554433126605783040.0000 - mae: 706221056.0000\n",
      "Epoch 1253: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 5757735025604820992.0000 - mae: 702421568.0000 - val_loss: 974078447926116352.0000 - val_mae: 563956480.0000\n",
      "Epoch 1254/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 498973701128060928.0000 - mae: 532310912.0000\n",
      "Epoch 1254: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 5757276529256038400.0000 - mae: 702581504.0000 - val_loss: 974039483982807040.0000 - val_mae: 564005760.0000\n",
      "Epoch 1255/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10404041916411805696.0000 - mae: 864559104.0000\n",
      "Epoch 1255: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 5757075868383969280.0000 - mae: 702729344.0000 - val_loss: 974211351394123776.0000 - val_mae: 564181056.0000\n",
      "Epoch 1256/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13915095904714489856.0000 - mae: 952061248.0000\n",
      "Epoch 1256: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5757139640058380288.0000 - mae: 703073408.0000 - val_loss: 974662426039418880.0000 - val_mae: 564532352.0000\n",
      "Epoch 1257/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2075548649508569088.0000 - mae: 613655680.0000\n",
      "Epoch 1257: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 5756449146756136960.0000 - mae: 703261504.0000 - val_loss: 974709636319936512.0000 - val_mae: 564604992.0000\n",
      "Epoch 1258/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8738801869331103744.0000 - mae: 765743104.0000\n",
      "Epoch 1258: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5756241339058487296.0000 - mae: 703405440.0000 - val_loss: 974813677607714816.0000 - val_mae: 564735488.0000\n",
      "Epoch 1259/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2026054958217953280.0000 - mae: 594354624.0000\n",
      "Epoch 1259: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5755800434895749120.0000 - mae: 703447744.0000 - val_loss: 974685103466741760.0000 - val_mae: 564707008.0000\n",
      "Epoch 1260/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13870466727743062016.0000 - mae: 952690944.0000\n",
      "Epoch 1260: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 5755941722139918336.0000 - mae: 703623104.0000 - val_loss: 974994203673100288.0000 - val_mae: 564987776.0000\n",
      "Epoch 1261/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5581608106242080768.0000 - mae: 709279616.0000\n",
      "Epoch 1261: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 5755343587814408192.0000 - mae: 703754880.0000 - val_loss: 975018392928911360.0000 - val_mae: 565059904.0000\n",
      "Epoch 1262/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5530605610120249344.0000 - mae: 694128896.0000\n",
      "Epoch 1262: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 5755050567965605888.0000 - mae: 703839104.0000 - val_loss: 975079690702159872.0000 - val_mae: 565148544.0000\n",
      "Epoch 1263/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2007898447830581248.0000 - mae: 586670976.0000\n",
      "Epoch 1263: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 5754786685174939648.0000 - mae: 703894528.0000 - val_loss: 975019492440539136.0000 - val_mae: 565164480.0000\n",
      "Epoch 1264/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 395201724979085312.0000 - mae: 485935968.0000\n",
      "Epoch 1264: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 5754642099395887104.0000 - mae: 703860608.0000 - val_loss: 974852985148407808.0000 - val_mae: 565135488.0000\n",
      "Epoch 1265/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5510932598320267264.0000 - mae: 669026816.0000\n",
      "Epoch 1265: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 5754218787419193344.0000 - mae: 703849408.0000 - val_loss: 974785777500160000.0000 - val_mae: 565153728.0000\n",
      "Epoch 1266/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5698730833611849728.0000 - mae: 791427584.0000\n",
      "Epoch 1266: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5754073651884326912.0000 - mae: 703862848.0000 - val_loss: 974824535285039104.0000 - val_mae: 565247360.0000\n",
      "Epoch 1267/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7298100788339933184.0000 - mae: 872784000.0000\n",
      "Epoch 1267: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5753694870128558080.0000 - mae: 703953408.0000 - val_loss: 974834911926026240.0000 - val_mae: 565289920.0000\n",
      "Epoch 1268/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5620087164923543552.0000 - mae: 726476544.0000\n",
      "Epoch 1268: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 5753452977570447360.0000 - mae: 704041472.0000 - val_loss: 974913664446365696.0000 - val_mae: 565412352.0000\n",
      "Epoch 1269/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7162697580646760448.0000 - mae: 781211904.0000\n",
      "Epoch 1269: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 5753320486419300352.0000 - mae: 704232704.0000 - val_loss: 975191772168716288.0000 - val_mae: 565659328.0000\n",
      "Epoch 1270/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 376336441908658176.0000 - mae: 481384000.0000\n",
      "Epoch 1270: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 5752864738849587200.0000 - mae: 704343296.0000 - val_loss: 975065740648382464.0000 - val_mae: 565652096.0000\n",
      "Epoch 1271/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 471928223345672192.0000 - mae: 546334080.0000\n",
      "Epoch 1271: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 5752883430547259392.0000 - mae: 704286080.0000 - val_loss: 974798490603356160.0000 - val_mae: 565579904.0000\n",
      "Epoch 1272/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5477050047998722048.0000 - mae: 668492352.0000\n",
      "Epoch 1272: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5752566771198459904.0000 - mae: 704401536.0000 - val_loss: 975261178840219648.0000 - val_mae: 565927296.0000\n",
      "Epoch 1273/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8727999167588204544.0000 - mae: 758546496.0000\n",
      "Epoch 1273: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5752241315756638208.0000 - mae: 704746688.0000 - val_loss: 975368862260264960.0000 - val_mae: 566098816.0000\n",
      "Epoch 1274/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5641748643502358528.0000 - mae: 718357824.0000\n",
      "Epoch 1274: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 5751770724779950080.0000 - mae: 704929152.0000 - val_loss: 975591238486982656.0000 - val_mae: 566269376.0000\n",
      "Epoch 1275/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8749286262457761792.0000 - mae: 761820544.0000\n",
      "Epoch 1275: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 5751662972640428032.0000 - mae: 705165632.0000 - val_loss: 975885907603226624.0000 - val_mae: 566523200.0000\n",
      "Epoch 1276/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5608873795587670016.0000 - mae: 738235264.0000\n",
      "Epoch 1276: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 5751251205535825920.0000 - mae: 705396992.0000 - val_loss: 975954214763102208.0000 - val_mae: 566623744.0000\n",
      "Epoch 1277/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7294033694828789760.0000 - mae: 859632832.0000\n",
      "Epoch 1277: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 5750915304733540352.0000 - mae: 705532352.0000 - val_loss: 976194526773248000.0000 - val_mae: 566823872.0000\n",
      "Epoch 1278/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8772669576245673984.0000 - mae: 767905600.0000\n",
      "Epoch 1278: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 5750847135012618240.0000 - mae: 705751680.0000 - val_loss: 976293895136608256.0000 - val_mae: 566974784.0000\n",
      "Epoch 1279/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10374639875973447680.0000 - mae: 885557120.0000\n",
      "Epoch 1279: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5750556314187071488.0000 - mae: 706008832.0000 - val_loss: 976633369351684096.0000 - val_mae: 567229312.0000\n",
      "Epoch 1280/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 424497593825886208.0000 - mae: 495477760.0000\n",
      "Epoch 1280: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 5750228109966180352.0000 - mae: 706135232.0000 - val_loss: 976492150826991616.0000 - val_mae: 567197056.0000\n",
      "Epoch 1281/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 496738393988792320.0000 - mae: 559733312.0000\n",
      "Epoch 1281: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5749912550129008640.0000 - mae: 706097024.0000 - val_loss: 976487340463620096.0000 - val_mae: 567237568.0000\n",
      "Epoch 1282/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13876102824347041792.0000 - mae: 972179904.0000\n",
      "Epoch 1282: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 5750120357826658304.0000 - mae: 706464448.0000 - val_loss: 977176115778945024.0000 - val_mae: 567727936.0000\n",
      "Epoch 1283/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7214250382338293760.0000 - mae: 811091456.0000\n",
      "Epoch 1283: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 5749311117268615168.0000 - mae: 706857408.0000 - val_loss: 977443915579785216.0000 - val_mae: 567925888.0000\n",
      "Epoch 1284/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15522051939918413824.0000 - mae: 1075714816.0000\n",
      "Epoch 1284: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 5749496934733709312.0000 - mae: 707215616.0000 - val_loss: 977952508427108352.0000 - val_mae: 568288448.0000\n",
      "Epoch 1285/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 451554513401151488.0000 - mae: 504854112.0000\n",
      "Epoch 1285: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 5748861417012854784.0000 - mae: 707456448.0000 - val_loss: 978007209130590208.0000 - val_mae: 568382528.0000\n",
      "Epoch 1286/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7159023012786733056.0000 - mae: 794065664.0000\n",
      "Epoch 1286: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 5748605230803582976.0000 - mae: 707636608.0000 - val_loss: 978195431777370112.0000 - val_mae: 568517056.0000\n",
      "Epoch 1287/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 452397357783318528.0000 - mae: 523891712.0000\n",
      "Epoch 1287: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 5748357840687333376.0000 - mae: 707647552.0000 - val_loss: 978029749118959616.0000 - val_mae: 568485440.0000\n",
      "Epoch 1288/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15636548483765239808.0000 - mae: 1119583488.0000\n",
      "Epoch 1288: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 5748202259792003072.0000 - mae: 707751616.0000 - val_loss: 978169593254117376.0000 - val_mae: 568636864.0000\n",
      "Epoch 1289/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5552146692175822848.0000 - mae: 719468160.0000\n",
      "Epoch 1289: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 5747799288780423168.0000 - mae: 707801792.0000 - val_loss: 978022052537565184.0000 - val_mae: 568594176.0000\n",
      "Epoch 1290/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7243214267392983040.0000 - mae: 832943616.0000\n",
      "Epoch 1290: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 5747595329373470720.0000 - mae: 707814464.0000 - val_loss: 978165744963420160.0000 - val_mae: 568731008.0000\n",
      "Epoch 1291/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2041357411297525760.0000 - mae: 591414656.0000\n",
      "Epoch 1291: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 5747311655373504512.0000 - mae: 707866880.0000 - val_loss: 978045898195992576.0000 - val_mae: 568711552.0000\n",
      "Epoch 1292/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 371777866699898880.0000 - mae: 480535584.0000\n",
      "Epoch 1292: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 5747117591571202048.0000 - mae: 707815680.0000 - val_loss: 977787512963465216.0000 - val_mae: 568619328.0000\n",
      "Epoch 1293/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8705939665800134656.0000 - mae: 746333056.0000\n",
      "Epoch 1293: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 5746985650175868928.0000 - mae: 707863488.0000 - val_loss: 977936565508505600.0000 - val_mae: 568773824.0000\n",
      "Epoch 1294/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2061304201492824064.0000 - mae: 595760704.0000\n",
      "Epoch 1294: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 5746642052792188928.0000 - mae: 707936640.0000 - val_loss: 977925707831181312.0000 - val_mae: 568800960.0000\n",
      "Epoch 1295/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5494546026775707648.0000 - mae: 679900416.0000\n",
      "Epoch 1295: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5746408406571286528.0000 - mae: 707989120.0000 - val_loss: 977873137431478272.0000 - val_mae: 568802816.0000\n",
      "Epoch 1296/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10451980623382839296.0000 - mae: 909458944.0000\n",
      "Epoch 1296: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 5746241280803864576.0000 - mae: 708044800.0000 - val_loss: 978000268463439872.0000 - val_mae: 568956864.0000\n",
      "Epoch 1297/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7166490346006773760.0000 - mae: 788478720.0000\n",
      "Epoch 1297: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 5745933417548087296.0000 - mae: 708201088.0000 - val_loss: 978113793039007744.0000 - val_mae: 569070656.0000\n",
      "Epoch 1298/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 469871277608271872.0000 - mae: 515662688.0000\n",
      "Epoch 1298: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 5745732206920204288.0000 - mae: 708206016.0000 - val_loss: 977963366104432640.0000 - val_mae: 569034176.0000\n",
      "Epoch 1299/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8829136645157552128.0000 - mae: 816277696.0000\n",
      "Epoch 1299: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 5745525498734182400.0000 - mae: 708199168.0000 - val_loss: 977781396930035712.0000 - val_mae: 569026368.0000\n",
      "Epoch 1300/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5545760178885885952.0000 - mae: 707557376.0000\n",
      "Epoch 1300: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 5745224782303985664.0000 - mae: 708157504.0000 - val_loss: 977751160360271872.0000 - val_mae: 569040512.0000\n",
      "Epoch 1301/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 553709142305406976.0000 - mae: 574385216.0000\n",
      "Epoch 1301: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 5744991685838897152.0000 - mae: 708110528.0000 - val_loss: 977654678214934528.0000 - val_mae: 569039232.0000\n",
      "Epoch 1302/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8717036486903463936.0000 - mae: 734753792.0000\n",
      "Epoch 1302: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 5744916369292394496.0000 - mae: 708246272.0000 - val_loss: 977922959052111872.0000 - val_mae: 569273152.0000\n",
      "Epoch 1303/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 444565399019716608.0000 - mae: 510563904.0000\n",
      "Epoch 1303: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 5744587615315689472.0000 - mae: 708322304.0000 - val_loss: 977689587709116416.0000 - val_mae: 569194304.0000\n",
      "Epoch 1304/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7158153299089162240.0000 - mae: 784569024.0000\n",
      "Epoch 1304: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 5744213231606431744.0000 - mae: 708361792.0000 - val_loss: 977943231297748992.0000 - val_mae: 569404480.0000\n",
      "Epoch 1305/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7172336449331658752.0000 - mae: 788426624.0000\n",
      "Epoch 1305: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 5744102730687840256.0000 - mae: 708637824.0000 - val_loss: 978312804643635200.0000 - val_mae: 569713472.0000\n",
      "Epoch 1306/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 455617621182644224.0000 - mae: 512627712.0000\n",
      "Epoch 1306: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 5743712404059979776.0000 - mae: 708793984.0000 - val_loss: 978276452040441856.0000 - val_mae: 569770432.0000\n",
      "Epoch 1307/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 347744810259447808.0000 - mae: 463079264.0000\n",
      "Epoch 1307: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 5743438625664663552.0000 - mae: 708841728.0000 - val_loss: 978295968371834880.0000 - val_mae: 569856448.0000\n",
      "Epoch 1308/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8756175802317406208.0000 - mae: 769852032.0000\n",
      "Epoch 1308: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 5743469961746055168.0000 - mae: 709050304.0000 - val_loss: 978578405421219840.0000 - val_mae: 570152512.0000\n",
      "Epoch 1309/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 431260964726243328.0000 - mae: 494057088.0000\n",
      "Epoch 1309: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 5742976830780997632.0000 - mae: 709196288.0000 - val_loss: 978526178618900480.0000 - val_mae: 570195456.0000\n",
      "Epoch 1310/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2165369678526939136.0000 - mae: 662235136.0000\n",
      "Epoch 1310: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 5742761326501953536.0000 - mae: 709241664.0000 - val_loss: 978516764050587648.0000 - val_mae: 570267840.0000\n",
      "Epoch 1311/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 493312796792979456.0000 - mae: 538092288.0000\n",
      "Epoch 1311: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 5742563964164767744.0000 - mae: 709305856.0000 - val_loss: 978554903360176128.0000 - val_mae: 570380672.0000\n",
      "Epoch 1312/30000\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13779707540428292096.0000 - mae: 938994688.0000\n",
      "Epoch 1312: val_loss did not improve from 956405104179675136.00000\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 5742704151897309184.0000 - mae: 709623232.0000 - val_loss: 979082256624648192.0000 - val_mae: 570752192.0000\n"
     ]
    }
   ],
   "source": [
    "# # Model Train\n",
    "\n",
    "history = model.fit(train_input, train_target,\n",
    "                    epochs = n_epochs, \n",
    "                    batch_size = batch_size,\n",
    "                    validation_split=0.25,\n",
    "                    callbacks = callbacks,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053a1a5-9a9a-4ac8-af8c-231bd8d18ffc",
   "metadata": {},
   "source": [
    "Model Train 이후 코드는 제 것이 아닌 작성하신 코드 사용하시면 될 것 같습니다.\n",
    "시각자료 plot 코드만 참고하셔서 학습 진행 후 결과 데이터 톡방에 올려주시면 될 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "36689b51-87ed-4ba3-817c-fd5dec9a3973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 6ms/step - loss: 527434971929903104.0000 - mae: 528379872.0000\n",
      "Test_Loss :  5.274349719299031e+17\n",
      "Test_MAE :  528379872.0\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluate\n",
    "\n",
    "scores = model.evaluate(test_input, test_target)\n",
    "print(f'Test_Loss : ', scores[0])\n",
    "print(f'Test_MAE : ', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "71725818-f927-423d-b107-547a96a8c081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 0s/step\n",
      "RMSE : 726247172.4572866, MAPE : 162.25042283994182\n"
     ]
    }
   ],
   "source": [
    "# Model Test setting\n",
    "\n",
    "prediction = model.predict(test_input)\n",
    "test_result = np.concatenate((prediction, test_target), axis = 1)\n",
    "# test_result = test_result[test_result[:, 1].argsort()]\n",
    "\n",
    "# 성능 평가 지표 값 확인\n",
    "\n",
    "RMSE_value = RMSE(test_target, prediction[:, 0])\n",
    "MAPE_value = MAPE(test_target, prediction[:, 0])\n",
    "print(f'RMSE : {RMSE_value}, MAPE : {MAPE_value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "77d11761-542f-4c7f-a981-3aef24215667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOydeZgU1dX/v9XV6+wMA8ywCQgioAiKRoxBFPd9jUYTcc1Po9GIRl807hqNwbi87soSNa5BfV3RKJsbKgoRRVCUTZhhWGfvparu74+qe+tWdVUvM93Ty9zP88wD3V3dfXurOvU933OORAghEAgEAoFAICgSPLlegEAgEAgEAkEmEcGNQCAQCASCokIENwKBQCAQCIoKEdwIBAKBQCAoKkRwIxAIBAKBoKgQwY1AIBAIBIKiQgQ3AoFAIBAIigoR3AgEAoFAICgqRHAjEAgEAoGgqBDBTQ9izpw5kCSJ/Xm9XtTV1eGss87CDz/8ELf95MmTIUkShg0bBqdG1osXL2aPNWfOHMttn332GU455RQMHjwYgUAA/fr1w8SJE3H11Vc7PofT35AhQzL22hcuXOj6PJIk4ZJLLmHbzp8/HxdccAH23HNPlJaWYsCAATjppJPw5ZdfOj52LBbDP/7xD+y9994IhUKoqqrCQQcdhE8++YRt8/333+Oaa67Bfvvth6qqKlRXV+OXv/wl/v3vfydd+1/+8hdIkoS99trLcv26desSvqajjz467nGOP/54DBgwAJIk4bzzznN8viFDhrg+ZjAYZNvV19fjL3/5CyZOnIiamhpUVFRgv/32wxNPPAFVVZO+LjsrV65EIBCAJElYunSp5Tb7d5f/a2ho6NR7snHjRpxyyikYNmwYSktLUVlZifHjx+Ohhx6Coihprz8T8K9z4cKFcbcTQjB8+HBIkoTJkyc7Psa2bdtc30fKeeedZ3lfAoEARo4ciZtvvhnhcJhtd8sttyR8P9etW5eBV63z0Ucf4aKLLsJ+++3H1p/o8devX48LLrgA/fv3RyAQwIABA3DKKadYtnnllVfwm9/8BsOHD0coFMKQIUNwzjnnOO7vnHjqqadw8sknY8iQIQiFQhg+fDguvfRS1NfXx23r9rvh9y2U1tZW/OlPf0L//v0RDAYxbtw4vPDCCwnXQgjBpEmTIEkSLr/88pTW35Px5noBgu5n9uzZ2HPPPREOh/Hxxx/jzjvvxIIFC7Bq1Sr06tXLsm15eTnWrl2L+fPnY8qUKZbbZs2ahYqKCjQ3N1uuf+utt3DiiSdi8uTJuOeee1BXV4f6+nosXboUL7zwAu69917L9sOGDcO//vWvuHUGAoEMvWJg3333xaeffhp3/aOPPoqnn37aslN89NFHsX37dlx55ZUYPXo0tm7dinvvvRcHHngg3n33XRx22GFsW1VVccopp+Cjjz7Ctddei4MOOghtbW348ssv0dbWxrZ777338NZbb+F3v/sd9t9/fyiKghdffBFnnHEGbr31Vtx0002O616+fDlmzJiBfv36xd1WV1fn+Jpee+01/O1vf4vb0d93330YO3YsTjzxRMyaNcv1vXr11VcRiUQs123YsAFnnnmm5TG//PJLPP300zj33HNx4403wufz4Z133sGll16KJUuWJHwOO6qq4oILLkBNTQ02b97suh397vL07t2b/T+d96StrQ0VFRW48cYbMXjwYESjUbz99tv44x//iOXLl+Opp55Kef2Zpry8HDNnzowLYBYtWoQff/wR5eXlrvd95plnEI1GAQAzZ87EhAkTHLcLhUKYP38+AGDnzp14/vnncdttt2HVqlV48cUXLdvOmzcPlZWVcY9RV1eXzstKyAcffID3338f48ePR0VFhWNwR/nmm28wefJkDBs2DDNmzMDAgQNRX1+Pd99917Ld3/72N9TW1uKGG27AsGHDsHHjRvz1r3/FvvvuiyVLlmDMmDEJ13TzzTfj0EMPxV//+lcMGDAAq1evxu23347/+7//w7Jly+J+l7/85S8xY8YMy3VOv91TTz0VX3zxBe6++27sscceeO655/Cb3/wGmqbh7LPPdlzLww8/jDVr1iRcr4CDCHoMs2fPJgDIF198Ybn+1ltvJQDIrFmzLNcfcsghZMyYMeTAAw8kZ599tuW25uZmUlJSQi6++GICgMyePZvdNmnSJLL77ruTWCwWtwZVVR2fIxdomkaGDRtGdtttN8u6tmzZErdtS0sL6devH5kyZYrl+vvuu494PB7y6aefJnyurVu3Ek3T4q4/7rjjSElJCQmHw3G3xWIxMm7cOHLFFVek9T5NnjyZlJSUkKamJsv1/GssLS0lU6dOTenxCCHklltuIQDI+++/z67bsWMHiUajcdtedtllBADZsGFDyo//97//nQwYMIA88MADjt9Rt+9uqri9J078+te/Jl6v1/EzyTb0dV500UUkFArFrfe3v/0tmThxIhkzZgw55JBDHB9jr732In379iX7778/qaysJO3t7XHbTJ06lZSWlsZd/6tf/YoAID///DMhhJCbb76ZACBbt27t+otLAv/9/Pvf/04AkLVr18Ztp2kaGTduHBk3blzSz8jpt7xp0ybi8/nIhRdemHRNTvf/4osvCABy++23W67fbbfdyHHHHZf0Md966y0CgDz33HOW64844gjSv39/oihK3H3Wrl1LysrKyCuvvEIAkMsuuyzp8/R0RFpKwM7stmzZ4nj7BRdcgFdeeQW7du1i11EJ9ayzzorbfvv27aipqYHXGy8Mejz585VbsGABfvrpJ5x//vmWdfXt2zdu27KyMowePRobN260XP/AAw9g0qRJOPDAAxM+V01NDSRJirv+gAMOQHt7O3bs2BF32913340dO3bgzjvvTPUl4ccff8SiRYvw61//GhUVFZbbOvveE0Iwe/ZsDBs2zKJa9erVCz6fL277Aw44AADw888/p/T4P/zwA2666SY88sgjcWvOBIneEyf69OkDj8cDWZYzvpZU+c1vfgMAeP7559l1TU1NmDt3Li644ALX+3322Wf45ptv8Lvf/Q4XX3wxu0+q0O/x+vXrO7nyzpPq93Px4sVYvnw5/vSnPyVVd51+y/3798fAgQPjfsup3n+//faDLMsp3d+JV199FWVlZTjjjDMs159//vnYvHkzPvvss7j7/P73v8cRRxwRp8YK3MmfI40gZ6xduxYAsMceezjeftZZZ0GWZcuOdubMmTj99NMdDxYTJ07EZ599hiuuuAKfffYZYrFY0jUoihL3p2la0vtR70Bncv8zZ86Ex+PB+eefn3TbpqYmfPXVVxYZe+PGjVi3bh323ntvXH/99ejXrx+8Xi/GjBmDf/7znymtYcGCBejTp0/cTnTlypW444478Oijj6KsrCzl1zRr1iwQQnDRRRelfJ9kvP/++8zf4BSg2Zk/fz68Xq/r94mHrvX444/HiSeemHT7448/HrIso7q6Gqeeeiq++eabpPdJ9p4QQqAoCnbu3IkXX3wRc+bMwdVXX+0YnHcXFRUVOP300y2pveeffx4ejwdnnnmm6/1mzpwJQD8hOeuss1BSUsKuSwWa9ujTp4/lelVV436fqfiqqIfI7snrCosXLwagp+6OPfZYBINBlJWV4fjjj8eqVauS3v+nn37C+vXrk6ak3Fi0aBFUVXW8/+LFi1FeXg6fz4fRo0fj3nvvjXufvvnmG4waNSru+zV27Fh2O89TTz2Fzz//HA899FCn1ttjyaluJOhWqOS9ZMkSEovFSEtLC5k3bx6pra0lkyZNiksj8amQqVOnkgkTJhBCCPn2228JALJw4UIm0fJpqW3btpGDDz6YACAAiM/nIwcddBC56667SEtLS9xz0O3sf6nIxhdccAGRZZmsW7curfdi586dJBgMkqOOOiql7c855xzi9XrJ0qVL2XWffvopAUAqKirI6NGjyUsvvUTeffddcvrppxMA5Iknnkj4mE8++SQBQB544AHL9aqqkl/84hfkN7/5DbsulbSUoihkwIABZM8990z6etJJS5155plElmWWqkjEu+++SzweD7nqqqtSeuz//d//Jb169SINDQ2EEPf00zvvvENuuOEG8sYbb5BFixaRhx56iAwcOJCUlpaS5cuXuz5+Ku/JXXfdxb5zkiSRG264IaW1ZwP+9S9YsIAAIN988w0hhJD999+fnHfeeYQQ4piWamtrIxUVFeTAAw9k102dOpVIkkTWrFlj2ZampWKxGInFYmTr1q3kgQceIJIkkf33359tR9NSTn+777570tfzz3/+k8iyTP75z3+m9T4kSkv9v//3/9jv7sILLyTvv/8+eeaZZ8huu+1GampqyObNm10fNxaLkcmTJ5OKioq00qaU5uZmMmrUKDJo0KC4fdkf/vAHMmvWLLJo0SLy2muvkXPOOYcAIL/97W8t240YMcJxv7N582YCgPz1r39l1/3888+ksrKSPP744+w6iLRUSvTo4GbRokXk+OOPJ3V1dQQAefXVV9N+jBdffJHss88+JBQKkcGDB5N77rkn8wvNEHTHaf8bNWoU2blzZ9z2/AF10aJFBAD5+uuvybRp08juu+9ONE1zDG4oX3zxBbn77rvJ6aefTmpqaggAMmTIEEv+/pBDDiG77747+eKLL+L+0g1Y0uGhhx4iAMjLL7+cdNu//OUvBAD53//9X8v1H3/8MQFA/H6/Za2appF9992XDBw40PUx3377beL3+8npp58e58X5+9//Tqqrqy35/lSCmzfffJMAIH//+9+TvqZUg5vt27eTQCCQkpfgyy+/JJWVleSggw5Kya+ybt06UlZWRp566il2XTreGupDOPHEE123SeU9qa+vJ1988QV59913yXXXXUf8fj+5/PLLkz5/NuBfv6ZpZPfddyfTpk0jX3/9NQFAFi9eTAhxDm7offmgmv5u7QHb1KlT4/YDkiSRY4891hLE0uDm/fffj/t9rlixImvvQ6Lghvr87AHCsmXLHF8rRdM0cu655xJZlslrr72W9po6OjrI4YcfTkpKSsiSJUtSus/ll19OAJCvvvqKXTdixAhy9NFHx21Lg5u77rqLXXf88ceTSZMmWfYRIrhJjR4d3Lz99tvkhhtuIHPnzu1UcPP2228Tr9dLHn30UfLjjz+SN998k9TW1sYdBPMFuvN7+umnyRdffEHmz5/PzoKcfmz2A+qIESPIZZddRvr06UPuvPNOQghJGNzwRKNRctVVVxEA5M9//rPrc3QX48ePJ3369HE0xPJQIy19vTyrVq0iAMjYsWPjbps+fToB4GhInDdvHgkGg+S4444jkUjEctv69etJKBQiDzzwANm5cyf7++Uvf8mCUCeDKCGEnHLKKcTn8zk+p51Ugxtq8E322/jqq69IdXU1mTBhAtm1a1fSxyVEN1MfeOCBltf58MMPEwBkwYIFKT3O0UcfTfr27et6ezrvCeXuu++OOyB1F/bg7o477iB9+/Ylf/jDH8gee+zBtnMKbg4++GASDAbJhg0bLO/pkCFDyIABAyxG1alTp5JQKMQCla+//trRbN2dhmKeRMHN//zP/xAA5B//+EfcbXV1deSYY46Ju17TNHLBBRcQj8dDnnnmmbTXEw6HydFHH02CwaDFVJ+MJUuWEADkkUceYdcdeOCBFnWM8s033xAATKV5+eWXidfrJUuWLLF8ngDIxRdfTHbu3Jl0/9WT6dHBDY/TDjwSiZA///nPpH///qSkpIQccMABZMGCBez23/zmN+T000+33Oe+++4jAwcOdKyMyTVuZ8UXXXSRo4phDzz++te/Eo/HY0lRpBrcEELIrl27CADLzicXwc1XX31FAJCrr7464XY0sLnlllscb4/FYqSkpMQxuKE7YPtBgQY2Rx11lKO6QVMRif6uvPLKuPtt2bKF+Hw+ctpppyV8TZRUg5uxY8eSfv36OVa+UWhgM378eLJjx46Unp8Qvbok0eusrKxM+hhHHXUUqa2tdbwt3feEMn/+fAKAvPDCC2ndLxPYf6M///wz8Xg8xOPxWM7o7cHN6tWrk35v3nrrLba9W7WUnXwMbp577jnX4Ka2tjZOZaSBjSRJcRWhqUADm0AgQObNm5fWfWnq+rHHHmPXXXzxxaSsrCzuN/X8888TAOTjjz8mhCROCdK/zmQbegqiz00Czj//fKxbtw4vvPAC+vfvj1dffRVHH300VqxYgREjRiASiaCkpMRyn1AohJ9//hnr16/PaBO6bHLPPfdg7ty5uOmmm3Dqqae6Vi1MnToVn332GUaNGoUBAwa4Pl59fb1j/4vvvvsOgF6tkEuowfLCCy903eb222/HLbfcgr/85S+4+eabHbfxer046aST8O9//xvr1q1jnzchBPPmzcPuu++Ompoatv17772Hk08+GQcffDBee+01x0qPcePGYcGCBXHX/+lPf0JTUxNmz56NgQMHxt3+9NNPIxaLJXxN6bJ06VJ8/fXXuPbaa13NtcuXL8fhhx+OgQMH4j//+U9cn6REvPDCC5aGcYDeT+Vvf/sbHnvssaSGz7Vr1+Ljjz/G4Ycf7nh7Z98T+v4PHz48rftlgwEDBuDPf/4zVq1ahalTp7puR7/TTz75ZNy6Ozo6cNJJJ2HWrFk49thjs7re7uCYY45BSUkJ3nnnHVx11VXs+q+++goNDQ2WykVCCC6++GLMnj0bjz/+eErFAzyRSASnnHIK5s+fj1deeQVHHXVUWvd/+umnAcCyplNOOQVPPvkk5s6dazGH//Of/0T//v3xi1/8AoBeLOHUqPHQQw/FySefjCuvvDKusaeAI9fRVb4AWxS8Zs0aIkkS2bRpk2W7KVOmkOnTpxNCCHn88cdJSUkJef/994mqqmT16tVkzz33JADIJ5980p3LT4lEfoZ77rmHALBItqmoKk7Kzd57702OOeYY8sgjj5D58+eT999/n8yYMYPU1dWRsrIy8vXXX1ueY9iwYeTTTz91/EtGuobijo4O0qtXL3LQQQe5bjNjxgyWqku2pjVr1pCqqioycuRI8vzzz5O33nqLnHLKKUSSJIsS9uGHH5JQKESGDBlC5s+fH/eYyfqvJPss9txzTzJo0KC4PkI8CxcuJC+//DJ5+eWXSTAYJJMnT2aXGxsb47a/5JJLCACyevVqx8dbtWoV6d27N6muriZvvPFG3GviH3PhwoVElmVy6623Jnydbt/RKVOmkFtvvZW8+uqr5IMPPiD3338/6d+/PykvL3f1fiR7T2666Sby//7f/yP/+te/yMKFC8lrr71GLrnkEiLLMjnjjDMSrjNbpOo54pWbWCxGamtryahRo1y3P/XUU4nP52OfSbrKzbx58xx/C8m+t+kYihsbG9n38dxzz2XpnJdffpksXLjQsi39jU6dOpXMmzePzJkzhwwaNIgMHjyYbN++nW1HPS8XXHBB3NrtacfDDjuMyLJsue74449nPh77/b/99lu23b/+9S9y2mmnkVmzZpEPPviAzJ07l5x11lkEADOB8xxxxBGkV69e5IknniDz589nPqJnn3026fsE4blJCRHcGNiDm5deeokAIKWlpZY/r9dLfv3rXxNCdLnz2muvJcFgkMiyTHr16sVSGZ999lmOXok7iXacHR0dZPDgwWTEiBEsN9/Z4ObFF18kZ599NhkxYgQpKysjPp+PDB48mPzud78jK1eutNw/UbUUgITpEEJMY6STfO3Ev/71LwLENyxMZ012VqxYQY477jhSXl5OgsEgOfDAA8kbb7xh2SaZxMynO93W5PZZUGPzTTfdlPQxUn3+9vZ2UllZSSZNmuT6eG4GdfrHfydouu3mm29OuEa37+if/vQnMnr0aFJeXk68Xi/p378/+e1vf+saeKXynrz++uvk8MMPJ/369SNer5eUlZWRAw44gDz44INJv3fZojPBzWuvvUYAkPvvv991+3nz5hEA5N577yWEpB/cuP395z//Sen1pJK2TpSSdWpY+OSTT5K99tqL+P1+0rt3b3LOOeeQjRs3WrZJlPrcbbfdLNvS3wdPotfOr+nTTz8lU6ZMIbW1tcTn85GSkhKy//77k0ceecQxuG5paSFXXHEFqa2tJX6/n4wdO5Y8//zzSd8juiYR3CRHIsRhaFAPRJIkvPrqqzj55JMBAC+++CLOOeccfPvtt3HNvMrKylBbW8suq6qKhoYG9OnTBx988AGOPfZYbNmyxbEBlEAgEAgEguwiPDcujB8/HqqqorGxEb/61a8SbivLMvOgPP/885g4caIIbAQCgUAgyBE9OrhpbW21DCJbu3Ytli9fjurqauyxxx4455xzcO655+Lee+/F+PHjsW3bNsyfPx977703jj32WGzbtg3//ve/MXnyZITDYcyePRsvv/wyFi1alMNXJRAIBAJBz6ZHp6UWLlyIQw89NO76qVOnYs6cOYjFYrjjjjvw9NNPY9OmTejduzcmTpyIW2+9FXvvvTe2bduGE044AStWrAAhBBMnTsSdd97J3O4CgUAgEAi6nx4d3AgEAoFAICg+xOBMgUAgEAgERYUIbgQCgUAgEBQVPc5QrGkaNm/ejPLyckiSlOvlCAQCgUAgSAFCCFpaWtC/f3/XTvqUHhfcbN68GYMGDcr1MgQCgUAgEHSCjRs3Oo6h4elxwU15eTkA/c2pqKjI8WoEAoFAIBCkQnNzMwYNGsSO44noccENTUVVVFSI4EYgEAgEggIjFUuJMBQLBAKBQCAoKkRwIxAIBAKBoKgQwY1AIBAIBIKiosd5blJFVVXEYrFcL0PQSXw+X9w0d4FAIBD0DERwY4MQgoaGBuzatSvXSxF0kaqqKtTW1op+RgKBQNDDEMGNDRrY9O3bFyUlJeLAWIAQQtDe3o7GxkYAQF1dXY5XJBAIBILuRAQ3HKqqssCmd+/euV6OoAuEQiEAQGNjI/r27StSVAKBQNCDEIZiDuqxKSkpyfFKBJmAfo7COyUQCAQ9CxHcOCBSUcWB+BwFAoGgZyKCG4FAIBAIBEWFCG4EaXHLLbdg3Lhx7PJ5552Hk08+udvXsW7dOkiShOXLl3f7cwsEAoEgvxHBTZFw3nnnQZIkSJIEn8+HYcOG4ZprrkFbW1tWn/eBBx7AnDlzUtpWBCQCgUAg6A5EtVQRcfTRR2P27NmIxWL48MMPcdFFF6GtrQ2PPvqoZbtYLAafz5eR56ysrMzI4xQ7hBBEFA1Bn6jaEggEgmwjlJsiIhAIoLa2FoMGDcLZZ5+Nc845B6+99hpLJc2aNQvDhg1DIBAAIQRNTU34/e9/j759+6KiogKHHXYY/vvf/1oe8+6770a/fv1QXl6OCy+8EOFw2HK7PS2laRr+9re/Yfjw4QgEAhg8eDDuvPNOAMDQoUMBAOPHj4ckSZg8eTK73+zZszFq1CgEg0HsueeeeOSRRyzP8/nnn2P8+PEIBoOYMGECli1blsF3LvtcN/dr7Hv7f1Df1JHrpQgEAkHRI5SbJBBC0BFTc/LcIZ/cpYqfUCjEyqDXrFmDl156CXPnzmU9X4477jhUV1fj7bffRmVlJR5//HFMmTIF33//Paqrq/HSSy/h5ptvxsMPP4xf/epXeOaZZ/Dggw9i2LBhrs85ffp0PPnkk7jvvvtw8MEHo76+HqtWrQKgBygHHHAA3n//fYwZMwZ+vx8A8OSTT+Lmm2/GQw89hPHjx2PZsmW4+OKLUVpaiqlTp6KtrQ3HH388DjvsMDz77LNYu3Ytrrzyyk6/L7lg2YZdaI+q+H5LK+oqQ7lejkAgEBQ1IrhJQkdMxeib3s3Jc6+87SiU+Dv3EX3++ed47rnnMGXKFABANBrFM888gz59+gAA5s+fjxUrVqCxsRGBQAAAMGPGDLz22mv497//jd///ve4//77ccEFF+Ciiy4CANxxxx14//3349QbSktLCx544AE89NBDmDp1KgBg9913x8EHHwwA7Ll79+6N2tpadr/bb78d9957L0499VQAusKzcuVKPP7445g6dSr+9a9/QVVVzJo1CyUlJRgzZgx+/vlnXHrppZ16b3KBqhEAQCRHgbJAIBD0JERaqoh48803UVZWhmAwiIkTJ2LSpEn43//9XwDAbrvtxoILAPjyyy/R2tqK3r17o6ysjP2tXbsWP/74IwDgu+++w8SJEy3PYb/M89133yESibCAKhW2bt2KjRs34sILL7Ss44477rCsY5999rE0V0y0jnxEMYKbqKrleCUCgUBQ/OSNcnPXXXfh+uuvx5VXXon777/fdbtFixZh2rRp+Pbbb9G/f39ce+21uOSSS7K2rpBPxsrbjsra4yd77nQ49NBD8eijj8Ln86F///4W03BpaallW03TUFdXh4ULF8Y9TlVVVWeWy0YepIOm6Qf7J598Er/4xS8st9H0GSGkU+vJJ6hyE1VEcCMQCATZJi+Cmy+++AJPPPEExo4dm3C7tWvX4thjj8XFF1+MZ599Fh9//DH+8Ic/oE+fPjjttNOysjZJkjqdGupuSktLMXz48JS23XfffdHQ0ACv14shQ4Y4bjNq1CgsWbIE5557LrtuyZIlro85YsQIhEIhfPDBByyVxUM9Nqpqpmb69euHAQMG4KeffsI555zj+LijR4/GM888g46ODhZAJVpHPiKCG4FAIOg+cp6Wam1txTnnnIMnn3wSvXr1SrjtY489hsGDB+P+++/HqFGjcNFFF+GCCy7AjBkzumm1xcPhhx+OiRMn4uSTT8a7776LdevW4ZNPPsFf/vIXLF26FABw5ZVXYtasWZg1axa+//573Hzzzfj2229dHzMYDOK6667Dtddei6effho//vgjlixZgpkzZwIA+vbti1AohHnz5mHLli1oamoCoDcGvOuuu/DAAw/g+++/x4oVKzB79mz84x//AACcffbZ8Hg8uPDCC7Fy5Uq8/fbbBfeZ07RURAQ3AoFAkHVyHtxcdtllOO6443D44Ycn3fbTTz/FkUceabnuqKOOwtKlS12HI0YiETQ3N1v+BLoi9fbbb2PSpEm44IILsMcee+Css87CunXr0K9fPwDAmWeeiZtuugnXXXcd9ttvP6xfvz6piffGG2/E1VdfjZtuugmjRo3CmWeeicbGRgCA1+vFgw8+iMcffxz9+/fHSSedBAC46KKL8NRTT2HOnDnYe++9ccghh2DOnDmsdLysrAxvvPEGVq5cifHjx+OGG27A3/72tyy+O5lHNdJvQrkRCASC7CORHBoaXnjhBdx555344osvEAwGMXnyZIwbN87Vc7PHHnvgvPPOw/XXX8+u++STT/DLX/4SmzdvRl1dXdx9brnlFtx6661x1zc1NaGiosJyXTgcxtq1azF06FAEg8GuvThBzsmnz3PvW95FS1jBn48aicsOTS11KBAIBAKT5uZmVFZWOh6/7eRMudm4cSOuvPJKPPvss2kdeOx9X2hs5tYPZvr06WhqamJ/Gzdu7PyiBYJOooq0lEAgEHQbOXPKfvnll2hsbMR+++3HrlNVFYsXL8ZDDz2ESCTCqmUotbW1aGhosFzX2NgIr9eL3r17Oz5PIBBgfVwEglxhem5EnxuBQCDINjkLbqZMmYIVK1ZYrjv//POx55574rrrrosLbAC9t8kbb7xhue69997DhAkTMjYrSSDIBqJaSiAQCLqPnAU35eXl2GuvvSzXlZaWonfv3uz66dOnY9OmTXj66acBAJdccgkeeughTJs2DRdffDE+/fRTzJw5E88//3y3r18gSBVCiAhuBAKBoBvJebVUIurr67FhwwZ2eejQoXj77bexcOFCjBs3DrfffjsefPDBrPW4EQgyAQ1sABHcCAQCQXeQV93p7N1y58yZE7fNIYccgq+++qp7FiQQZACVK0gUhmKBQCDIPnmt3AgExYBQbgQCgaB7EcGNQJBlFD64EYMzBQKBIOuI4EYgyDKqKpQbgUAg6E5EcCPIOpIk4bXXXsv1MnIGr9yIPjcCgUCQfURwU2R88sknkGUZRx99dFr3GzJkiOvYC0HXEJ4bgUAg6F5EcFNkzJo1C3/84x/x0UcfWcroBblD0cyARlRLCQQCQfYRwU0R0dbWhpdeegmXXnopjj/++LhS+tdffx0TJkxAMBhETU0NTj31VADA5MmTsX79elx11VWQJInN6brlllswbtw4y2Pcf//9GDJkCLv8xRdf4IgjjkBNTQ0qKytFqb4DQrkRCASC7kUEN8kgBIi25eYvzYHtL774IkaOHImRI0fit7/9LWbPns0Gi7711ls49dRTcdxxx2HZsmX44IMPMGHCBADAK6+8goEDB+K2225DfX096uvrU37OlpYWTJ06FR9++CGWLFmCESNG4Nhjj0VLS0taay9mrJ4bEdwIBAJBtsmrJn55Sawd+Gv/3Dz39ZsBf2nKm8+cORO/OfscrGpoxviDJqO1tRUffPABDj/8cNx5550466yzcOutt7Lt99lnHwBAdXU1ZFlGeXk5amtr01riYYcdZrn8+OOPo1evXli0aBGOP/74tB6rWNFEKbhAIBB0K0K5KRJWr16Nzz//HCeecjqiioa2GMGZZ56JWbNmAQCWL1+OKVOmZPx5Gxsbcckll2CPPfZAZWUlKisr0draKvw+HIpISwkEAkG3IpSbZPhKdAUlV8+dIjNnzoSiKNhz+BB2HSEEPp8PO3fuRCgUSvvpPR4PS2tRYrGY5fJ5552HrVu34v7778duu+2GQCCAiRMnIhqNpv18xYoqSsEFAoGgWxHBTTIkKa3UUC5QFAVPP/007r33Xvzi4EPR0NwBv1fG0JpSnHbaafjXv/6FsWPH4oMPPsD555/v+Bh+vx+qaj3w9unTBw0NDSCEMJPx8uXLLdt8+OGHeOSRR3DssccCADZu3Iht27Zl/kUWMEK5EQgEgu5FBDdFwJtvvomdO3fiwgsvhOINonxnBwJeGSNry3H66adj5syZuO+++zBlyhTsvvvuOOuss6AoCt555x1ce+21APQ+N4sXL8ZZZ52FQCCAmpoaTJ48GVu3bsU999yD008/HfPmzcM777yDiooK9tzDhw/HM888gwkTJqC5uRl//vOfO6USFTMqVwquEUBRNXhlkREWCASCbCH2sEXAzJkzcfjhh6OyspIVWBHo/znttNOwfPlyVFRU4OWXX8brr7+OcePG4bDDDsNnn33GHuO2227DunXrsPvuu6NPnz4AgFGjRuGRRx7Bww8/jH322Qeff/45rrnmGstzz5o1Czt37sT48ePxu9/9DldccQX69u3bPS+8QFBUa2pPmIoFAoEgu0jEbqoocpqbm1FZWYmmpiaLAgEA4XAYa9euxdChQxEMBnO0wq7R2BJGQ1MYftmDPesqkt+hiMmXz/OTNdtw9lNmILnsxiPQq9Sfs/UIBAJBIZLo+G1HKDdFhqncCPIF3nMDCOVGIBAIso0IbooMzYhuRHCTP6g2cVSYigUCgSC7iOCmyGDHURHd5A2qzXMjuhQLBAJBdhHBTZFhKjciuskX7Gkp0etGIBAIsosIbhwoZI91AS894+TL56jaPTdCuREIBIKsIoIbDp/PBwBob2/P8Uo6D1Nu8uO4nlPo50g/11yhaNZgRgQ3AoGgWFnT2Ipf/PV9nPjQRzldh2jixyHLMqqqqtDY2AgAKCkpYZ15CwUlGgFRFGiShHA4nOvl5ARCCNrb29HY2IiqqirIspzT9cQpN6JaSiAQFCnhmIotzRF4cnzsFMGNDToVmwY4hca2lgjCigZJAnztPbtTcFVVVdpTzrNBnOcmJoIbgUBQnNCTN1+Ou7CL4MaGJEmoq6tD375944ZEFgL3Pb8M325ugiQB70+bnOvl5Ayfz5dzxYYilBuBQNBTiCk0uBHKTV4iy3LeHBzTYVOLgk0tejVOIBAouLRaMRLXxE94bgQCQZESM1pf5Fq5EYbiIiMcM8uM7YqBIDdoIrgRCAQ9hJihTPu9IrgRZJAw5+ewd8YV5AbR50YgEPQU8sVzI4KbIoPvfqsJgSAvUG0fhOhQLBAIipWYmh+eGxHcFBkRPi0llJu8QAzOFAgEPYWoIpQbQRYIK8Jzk2/EzZYSpeACgaBIYZ6bnhzcPProoxg7diwqKipQUVGBiRMn4p133nHdfuHChZAkKe5v1apV3bjq/EXVCHOq08uC3COUG4FA0FOI5km1VE5LwQcOHIi7774bw4cPBwD885//xEknnYRly5ZhzJgxrvdbvXo1Kioq2OU+ffpkfa2FgN2oKoKb/EDMlhIIBD0F1ucmx9VSOQ1uTjjhBMvlO++8E48++iiWLFmSMLjp27cvqqqqsry6wiNsS3downOTF4g+NwKBoKcgDMU2VFXFCy+8gLa2NkycODHhtuPHj0ddXR2mTJmCBQsWJNw2EomgubnZ8les8D1uAKHc5Avx1VKiFFwgEBQnwnNjsGLFCpSVlSEQCOCSSy7Bq6++itGjRztuW1dXhyeeeAJz587FK6+8gpEjR2LKlClYvHix6+PfddddqKysZH+DBg3K1kvJOfYSYxHc5AfUYkN/7EK5EQgExQr13OS6iV/Oxy+MHDkSy5cvx65duzB37lxMnToVixYtcgxwRo4ciZEjR7LLEydOxMaNGzFjxgxMmjTJ8fGnT5+OadOmscvNzc1FG+AI5SY/ocpNSUBGtF0ThmKBQFC0xEQTPx2/34/hw4djwoQJuOuuu7DPPvvggQceSPn+Bx54IH744QfX2wOBAKvGon/FSlxwIzw3eQH13JT49FllQrkR8EQUFUt+2i6+F4KiICb63DhDCEEkEkl5+2XLlqGuri6LKyoc4gzFQrnJC6iCFvLrwY3oUCzgmf3xOpz1xBI8/em6XC9FIOgypuemB08Fv/7663HMMcdg0KBBaGlpwQsvvICFCxdi3rx5APSU0qZNm/D0008DAO6//34MGTIEY8aMQTQaxbPPPou5c+di7ty5uXwZeUNcKbhQbvICptz49Z+bCG4EPGu3tgEAGprCOV6JQNB1RJ8bAFu2bMHvfvc71NfXo7KyEmPHjsW8efNwxBFHAADq6+uxYcMGtn00GsU111yDTZs2IRQKYcyYMXjrrbdw7LHH5uol5BV25UZ4bvIDlQU3Ii0liGdXRxRAfMsAgaAQYZ6bnmwonjlzZsLb58yZY7l87bXX4tprr83iigob0cQvP1FEcCNIQFNHDACgiEm3giJAGIoFGUdUS+UnrFqKpaVEnxuBSVOHAgBQVPF7FRQ++eK5EcFNEWH3cogOxfkBPWgFabWUKAUXcDS162mpmAhuBEVAVMkPz40IboqIeOUmRwsRWKBBZmlApKUE8Yi0lKCYiIq0lCDThGMaytCOF/234bfyf0RaKk9QbKXgIrgRUGKqhraoflIi0lKCYiBfBmeK4KaIiCgq9vP8gF94VuFseb4IbvIEVi3lE6XgAitUtQGEciMoDoTnRpBxwjENPujmRC8U0ecmT6Bn5CItJbCzq50LboRyIygCRLWUIOOEYyq80CVuGZroUJwn2DsUKxoRqpoAgFW5iYnvhKAIyJcmfiK4KSLCMQ1+ptyo4gCaJyisFFxm1wn1RgAATUYDPwBQRAWAoAgQyo0g40QUFV4juJElTaSl8gSm3PjMnpkiuBEANs+NSEsJigDmuRGGYkGmCMc0eCU9LeWDIpSbPIFWSwV9HkiGxy6iikZ+ApvnRhiKBUUArZbyC+VGkCkiigof57kRwU1+QD8Hr8fDfvBCuREA9mop8XsVFD7Mc+MV1VKCDBGOqVy1lCo6FOcJNLiRPRKTakU5uACwKjeiQ7GgGBCeG0HGiSgaq5YShuL8gSk3soSAVyg3ApNmi+dGfCcEhY/Z50YEN4IMoSs3IrjJNxROuQl4Ra8bgckukZYSFBlCuRFknHBMM6ulhOcmbzA9N2ZaSgzPFAC2PjfiOyEocAghLL3qEx2KBZkiHFPNailJhSqqL/ICWgUjeyQm1UZi4rMRALvazT434mREUOjwvjExW6qHoGoEP21tBcmiyTeimE38AICoSoKtBd2FpVqKKTeiFFwANHWYv1FhKBYUOrz6KDw3PYTHF/+Iw+5dhJeX/py15+DHLwAiuMkXTM8NhKFYwCCEWA3FQmkVFDh8cCM8Nz2E+d81AgC+a2jOyuMTQizVUgBANBHc5AOqSoMbjygFFzA6YqrFeyU6FAsKHXrS5pH0NHwuEcFNNxBRVHy9qQkAsKMtmmTrzj6H4VDn0lKaGnPbXNCNKA6GYhHcCHgzMSAMxYLCJ5onlVKACG66hW82NbOINmvBTYwGN3xaSvg68gE640svBRdpKYEO38APEIZiQeFDfWO59tsAIrjpFr5cv4P9P1vBTVjRAxm/xAU0mlBu8gFrKbjocyPQocpNZcgHQFf4sllwIBBkG9bjJseVUoAIbrqFL9fvZP/PWnATM4IbjzAU5xOEEOv4BVn0uRHoUOWmpszPrhON/ASFDD1py3WPG0AEN1mHEGIJbra3RbNydkY9HAGJO2gK5Sbn8KkGvhRc9LkR0Eqp3mUBdp0wFQsKGTZ6QSg3xc/67e3Y1hqF13CORxUN7dHMe2GochPwiD43+QR/Ji7zs6WEH6rHs6tDV3F55SYmysEFBYzZnTj3oUXuV1DkUNVmn0FV7MCWjdRU2FAC/BblRgQ3ucaq3AhDscCEem56l5rKjSqUG0EBky9DMwER3GSdpUZwM2G3Xuhdqp+hbc9KcONkKBbBTa7hlRuPxM2WEsFNj4d6bnqV+iEZFgWh3AgKGVEK3oP4yghu9t2tF6oN+XlnFoIb1ueGD25En5ucY1du2GwpEdz0eKhyUxXywefRvxfCcyMoZGLCUNwzaOqI4fvGFgDAfrv1Qq+SblBu+NlSQrnJObSlviQBHo+EgE8oNwIdvhTcaxwMRHAjKGSE56aHsGzDThACDOldgpqyAEtL7WiLJLwfIQT/WbkF9U0dKT8XDW58Ii2VV9AsAzWUM+VGlIL3eJhyU+JjrepFWkpQyIhqKYNHH30UY8eORUVFBSoqKjBx4kS88847Ce+zaNEi7LfffggGgxg2bBgee+yxblpt+lAz8X67VQMAqg3jYDLl5ot1O3Hx00tx/SsrUn6usKEE8LOlRHCTe6hyQw9etImfKAUXUM9NZcjHznRFl2JBISM8NwYDBw7E3XffjaVLl2Lp0qU47LDDcNJJJ+Hbb7913H7t2rU49thj8atf/QrLli3D9ddfjyuuuAJz587t5pWnhhnc9AIAVJfqnUiTeW5+3tkOANjSnFjh4YlQ5YYLbiQR3OQcszux/lNjhmKh3PR4eOWGKntivpSgkGEdivPAc+PN5ZOfcMIJlst33nknHn30USxZsgRjxoyJ2/6xxx7D4MGDcf/99wMARo0ahaVLl2LGjBk47bTTumPJKaOoGpZv3AUAmDCEBje6cpOsFJw296IjFVIhwpQbLqARwU3OUbjuxAC4UnDR56Yno2kEzWH9d17BKTfCcyMoZExDcQ9XbnhUVcULL7yAtrY2TJw40XGbTz/9FEceeaTluqOOOgpLly5FLOZcGRSJRNDc3Gz56w5WNbSgPaqiPOjF8D5lAIDqFEvBW8J6UJJO6oJ6brxEGIrzCX6uFABRCi4AoP/GaaNyi6FYeG4EBUxU9LkxWbFiBcrKyhAIBHDJJZfg1VdfxejRox23bWhoQL9+/SzX9evXD4qiYNu2bY73ueuuu1BZWcn+Bg0alPHX4MTabW0AgFG1FfAYB7beKZaC0zM6GrCkAlVuZC4t5RHBTc6hZ+KyLbgRpeA9G9qdOOSTEfDKXFpKKDeCwkVUS3GMHDkSy5cvx5IlS3DppZdi6tSpWLlypev2kmTN5dE5TfbrKdOnT0dTUxP727hxY+YWnwB6BkZLfwGkXApOlZt0ghu6rUz4tJRIfeQa1Z6Wkruu3LRHFbRFROBayPB+G8D0ZAlDsaCQYYMzvT3ccwMAfr8fw4cPBwBMmDABX3zxBR544AE8/vjjcdvW1taioaHBcl1jYyO8Xi969+7t+PiBQACBQMDxtmxCfYH0oAaAlYK3hBVEFc21XI4FN2kcAM3gxkzPCUNx7rFXS7E+N500jqoawZH3LYamESy+9lB48+AMSZA+fI8bACwtJQzFgkImJqql3CGEIBJxrhKaOHEi/vOf/1iue++99zBhwgT4fL7uWF7KqMZBzcsFN5UhH+jFne3u6g1NS6kaSXlnR2dLyURUS+UTcZ4bWS8F76xy0xpW8PPODmxuCqNVqDcFC18GDoAFqcJQLChkxGwpg+uvvx4ffvgh1q1bhxUrVuCGG27AwoULcc455wDQU0rnnnsu2/6SSy7B+vXrMW3aNHz33XeYNWsWZs6ciWuuuSZXL8EVe5UMoHeopampRBVTzWHzoJVqaipiVN94+LQUEQe/XGNPS3XVc9MaNT9TYUouXOzKjc8jDMWCwiefPDc5TUtt2bIFv/vd71BfX4/KykqMHTsW8+bNwxFHHAEAqK+vx4YNG9j2Q4cOxdtvv42rrroKDz/8MPr3748HH3ww78rAgfj+JpTqUj+2t0UTBjctYTO1FI5pKA8mfz5duSGW4EYiwnOTa+zfg65OBW/n1BphSi5c4jw3sjAUCwqfaB51KM5pcDNz5syEt8+ZMyfuukMOOQRfffVVllaUOai87PFYjVWplIM3d6Sv3IRjqqWBHwB4NDE4M9fYFbyuloK3WoIbEbwWKnGeG2EoFhQBos9ND8DutaDQ4CZROTiv3KR6AAsrmrWBH4Rykw+w74FsC25UjVX6pUN71PxMhXJTuOwyPHdVRppaGIoFxUA+dSgWwU2WcPLcAMmVm4iiWg5a4RQb+UUclBthKM49bsoN0LngpFWkpYoCqtxU2JQbRSg3ggKGplXzIS2V+xUUKU7VUgCSTgZvCVsDktQNxZp1aCYASRiKc479exDgfvSdKQdvF4biooBWS1VRQzHtUCyUG0EBIwZn9gCSKTduhuL44CbVUnAVPltayiOa+OUc+j3wSLQUnAtuOqXciLRUMRDf50b/XghDsaCQEX1uegBunpteSYIbOjSTko5y45Psyo0IbnKN3XMjSRILcDoT3PDVUkK5KVziDcX690MYigWFjPDc9ADM/ibWt7h3ksngccpNqobimBpnKPaItFTOcfoedKXXTZvFUCyC10IlfvyCYSgWfW4EBUxMMTw3QrkpXuxn7JRkaanmsF25Sb6zI4QYwY2tFFwENzlHcVDwutLrpk0oNwVPVNFY1ZvoUCwoJoTnpgeQzHOzsz0GzUGCbokLbpKfncdUAo0AflEKnnfYOxQDXet1wxuKheemMKGqjSQB5UFhKBYUD+bgzNyHFrlfQZHi7rnxsdvtKg3QuWopmp4Qyk3+4aTcmGmp9INP3lAslJvCpKlDV23LA14W9NJS8Jjw3AgKGOG56QHQGTG0SoYS8MooD+iNoZ163dgNxamcndPUld1Q7BHKTc5RVetUcAAZMxQLz01hYvpt/Ow6mr4WhmJBISMGZ/YA3JQbIHHFVLNNuYmkoNxQdadEth4sRXCTexw9Nz5DuelECsLSxC/FNgGC/MI+ERzgDMUiLSUoYPJpcGbuV1CkUGOg7CDPJTIV01QVFXzCKZzd0zN4EdzkHzTI9WRKueGqpTrTBFCQe3a2WyulAGEoFhQHwlDcA0ik3PROENxQzw0tGU/Fc0PTUiVe68FOFp6bnJPYc9O1ailhKC5M6FypXlxaymd8PxRRCi4oYFhayis8N0WL4tLnBkis3NBqqT7lqQc3VLkJydazPg+EcpNrnPvcyAA6WQouxi8UPDuN4IbuBwDRoVhQHNCp4H5ZzvFKRHCTNRIpNwnTUh36wcsMblI3FIdEWirvcPoedK3PjWjiV+g4pqVEh2JBEcA8N0K5KV6c+ptQEio3EX3H1zcN5YZuE/KItFS+kbjPTXrBCSHEotyItFRhsrMtPi1Fq6WEoVhQqBBChOemJ+DktaDQ4MapFJx6bvqU6dukZih2Vm5kkZbKOY7VUnLnPDfhmAbCndiL4KYwoWkpYSgWFBMKpzqK4KaIUbX4/iYUU7mJWK4nhKAlrKAO2/HHZcfhKu+/01JuAh57Wkoc/HIN+x7I8aXg6aal+DLwztxfkB/QUnDecyMMxYJCh1cdRZ+bIkZxmS0FcCMY2qwN+9qjKlSNYB/PjyiJbsdkz/IU+9zoX6qgPS0FkZbKNY7VUrQUPM0UBD96ARDKTaGywzEtJQzFgsKGDs0ERIfioob1N5GcSsF1P812m3JDe9yEPPpBzI9YSoZiaiwNGvcjHr0DcqGkpQghmPHuarz1dX2ul5JxEk0F76pyk0rgK8gvCCFMuRGGYkExQU/WJMk5Y9HdeHO9gGLFPGN3KAWnfpqYhvaoghK//jFQv02FTwOMQZjhFEynNACiaSniDUKKtkIukGqpVQ0teGjBGvSvDOK4sXW5Xk5GYS0BpK73ueEb+AGiiV8h0h5V2ecmDMWCYiLGmYklh5P67kYoN1kiUbVUqV9mqQm+Yor2uKn0GR4aKdYpzw3xhgDED9LMV2j1SHsRKhGqGp+eDBh9btINbuKVG3EgLDSomdjv9aDEb/YCoSdBilBuBAVKPs2VAkRwkzUSVUtJkuRYDk573JQZnYb9UFLrc2OoO346ONOnBzcyVBCS/zvLFuOgXYwGWSVhKXiayo3R44aeFAnlpvCgPrteJT7L2S31KCjiMxUUKHR/lg9+G0AEN1nDqUqGp6ZcD262NJu+G+q5KZeNYAWpKTf0DD5AgxsvDW60gsjh03RcMUryGsmcoZiOXqADF0UTv8Jjp8PoBUAYigWFTz71uAFEcJM1aL8KJ+UGAAb1KgEAbNzRzq6jE8HLvDS4URBRtKTqS8RFufFBgVoIyo0R1MVUAq0AgrF0SKTcpGsIpg38qo0DYzEqXcWOa3AjDMWCAiefJoIDIrjJGvSM3c01PqjaCG52msENPciXcMoNkNybQVNXZnCjP7YMDYXQNqM1bHpJYoWw4DSgCp7j+IW0S8H1z7eXkdIUpeCFB+tOXOqzXE+/H8X2/Rf0HMyhmfkRVuTHKoqQRNVSADCol66ubNzRwa6j6ZlSo6TbK2nwQEuamqK3+1hwEzTurxaGchMp3mGQVMHLZCl4L6HcFCzmXCnntJToUCwoVGLCc9MzSFQtBXDKDZ+W6tB3fLRfDZBarxt6Bu8zqqMkQ7nxQmXVOvkMVayA4vMcZHJwZrsR3FSXUs+NCG4KjV0sLWVVboShWFDoCM9NDyGp54ZLS1FPDVVuQlxwE0jBVExv9xodiSWuWqoglJtwESs3iTw3aSs31rSUqhFxMCwwqHIT77kxDMXCcyMoUOiJqUhLFTnJlJsBVSFIku6joAM0abVUUDKVDD9iSRv5mcENVW6MtFSBVUsBxVcx5fQ9oH1u0lZubIZiQJSDFxru1VJCuREUNjGh3Jjcdddd2H///VFeXo6+ffvi5JNPxurVqxPeZ+HChZAkKe5v1apV3bTq1HA6Y+cJ+mT0K9eDEJqaogf5ADcTKiAl73VDb6dpKdNQrDJjcz7Dp6WKLdWiOAxQ9XfSUMw8N9zARdHIr7BgwY2LoVg08RMUKqKJH8eiRYtw2WWXYcmSJfjPf/4DRVFw5JFHoq2tLel9V69ejfr6evY3YsSIblhx6jhVydgZzFJTuqmYHuRplRT9f7K0FC0F99L7sVJwVSg3OcbJc0N//OmWgtNqqYqgjwVLQrkpLGgTP7uh2CcMxYICJ9+a+OV0ttS8efMsl2fPno2+ffviyy+/xKRJkxLet2/fvqiqqsri6rpGMuUGAAZWh/D5OlO5oR2KfZbgRknBc2OoAyS+Q3EhBDetRVwt5ZiW8nWtiV9pQB/f0aGpQrkpMKihuNotLSVKwQUFStH0uVm0aBFOOOEEDB8+HCNGjMCJJ56IDz/8sEuLaWpqAgBUV1cn3Xb8+PGoq6vDlClTsGDBAtftIpEImpubLX/dgZqkFByIb+RHlRsfsSs3yaql9KBGJkaQ4KOzpYTnJtew74HsoNykGcjRJn4lfi8XIIkuxYVCRFHRRnsVuRmKVVIQI1MEAjvMc1PIhuJnn30Whx9+OEpKSnDFFVfg8ssvRygUwpQpU/Dcc891aiGEEEybNg0HH3ww9tprL9ft6urq8MQTT2Du3Ll45ZVXMHLkSEyZMgWLFy923P6uu+5CZWUl+xs0aFCn1pcuTLlJINHRtNSGHe1QVI3t+LzEnDflRyxpm3169i7bPDceiUBVFbe75QWqRopauTEVvK73uWkzqqXKAl4WIKUye0yQH+wyKqU8ElAetIrmfNqyEE5IBAI7+ea56VRa6s4778Q999yDq666il135ZVX4h//+Aduv/12nH322Wk/5uWXX46vv/4aH330UcLtRo4ciZEjR7LLEydOxMaNGzFjxgzHVNb06dMxbdo0drm5ublbAhwtweBMCl8Ozh/gZY0LbqQU0lJUudEMxccbZLcRNeZ0l7yBqhGUYvOQOHpuuFJwQohlgGIiaFqqxC93OrUlyB18pZTHtl/glT1FI/DKEAgKCrPPTX54bjoVYv3000844YQT4q4/8cQTsXbt2rQf749//CNef/11LFiwAAMHDkz7/gceeCB++OEHx9sCgQAqKiosf9mGEJKS54YqN5t3hVn/i5BPhqSYwzR15cb9AKZqhOU6Pba0FIC8V274lBRQzMpNfCk4kHpwoqga+x6UBbzsMYTnpnDYYbR8qLI18AOsPgVRMSUoRGJKEXhuBg0ahA8++CDu+g8++CAtVYQQgssvvxyvvPIK5s+fj6FDh3ZmOVi2bBnq6uo6dd9swO+bEik3fcsD8Hs9UDWC1Q0tAAy5WjWDm0ASQzGfsnIMbmJR+13yCr4MHCjeDsV8cBPymcFNqmklmrIEgBLDUAwI5aaQ2OXSwA+w7idErxtBIZJvfW46lZa6+uqrccUVV2D58uU46KCDIEkSPvroI8yZMwcPPPBAyo9z2WWX4bnnnsP//d//oby8HA0NDQCAyspKhEL6AXr69OnYtGkTnn76aQDA/fffjyFDhmDMmDGIRqN49tlnMXfuXMydO7czLyUr8BUPiZQbj0fCwKoQftrWhpWbdTN1edALKFbPTaIDIH/mLmnWUnAAIFp+KzetduWmyAyybn1uvB4JikbQEVVRGYo/k7dDG/j5ZAkBr5mWSrecXJA7aFrKXgYOWL8fxRbgC3oG0TwbnNmp4ObSSy9FbW0t7r33Xrz00ksAgFGjRuHFF1/ESSedlPLjPProowCAyZMnW66fPXs2zjvvPABAfX09NmzYwG6LRqO45pprsGnTJoRCIYwZMwZvvfUWjj322M68lKzAGwITBTeA7rv5aVsbvt2sV3FVhHxAc5jdnqwUPML1FpBoICMHoEGCBwSakt+eG3taikqbxYLqMoYj5JPRElHQkWJwYvpt9J+sUG4KD6rcVJfGB7OSJLGAV5SDCwqRoulzc8opp+CUU07p0pOnUvI4Z84cy+Vrr70W1157bZeeN9soaQU3uspCg5vyoA/YaSo3ASmxckMDn4BXBlTjfrIPCmT4oeS9objZlpaKFNnB2s17FfQbwU001eDGrJQCgIBPeG4KDeq5cUpLAbqpWNGIaOQnKEjyLS2VH6soMvhJ3In63ACmqbjBUGsqgl7AZihONFuKKjcBrweg5mGPFyr0g5+W52meeOWmuA7WdPyF/XtQ4tc/n45YamlDvlIK6HyvHEHuSJSWAgCf8R0RhmJBIZJvwU3Kyk11dTW+//571NTUoFevXgnLV3fs2JGRxRUq/M4piXDDGvlRKgMSQMyAxI8YmlMwFAe8HoB6bmQ/NBbc5LehmC+BB4ovzeKm3FBTcUc0PUNxCVNuaK+c/A5eBSaJ0lKAGJ4pKGzYVPBCC27uu+8+lJeXs/+n2pujJ8L3Nkn2PtFeN5RefutZmx9KwtQDU258MkBTULIPiiQDBCB5Xwpuq5YqMiXCzXMTpMFNmp6bsoB+v4BQbgoOsxTcLS1ldikWCAqNfOtzk3JwM3XqVPZ/avYVOONUIeOGPbipClgPVn4p8eBMGvgEvB4gagQKXFoq/4Obnqnc0PRSezTFtFTUaig2lZvier+KmV3tSTw3bDK4+EwFhQc9MfXnSQfKTulHsiyjsbEx7vrt27dDlvPjheUSum9K1OOGUhny6T4betlnC26gJPHcGGkpn8ylpXzQpMIIbmgpOBW4ii24cZotBZhpqWTdpyntdkMxbeIngpuCYSfrc5MkLSU8N4ICJJZnyk2nghu3KqdIJAK/3/mspCeRjnIDAIN7m+pNhS24CSTrc2MxFJueG9UQ5fK/WkoPbqqMXi/FpkSw74IUXy0FIOVqqVa7odgrSsELCUXVWGVgr9IkhuIMpqUIIWwUjECQTZjnphD73Dz44IMA9J4MTz31FMrKythtqqpi8eLF2HPPPTO7wgLEPFtP7UMe1KsE32wy+tz4rAe7ZH1u6G1BGaYR2eODKlFDcX4HN9RzU13qx872WNangs9ftQXfbGrGHw8bnnXfmKYR1q06Li1lKDftqSo3Ueq5ocqNaOJXSDR1xEDPCatcmjZm2lBMCMHZT36Gpo4Y3vjjwSmfbAkEnSFaqNVSgG4kBvQfzWOPPWZJQfn9fgwZMgSPPfZYZldYgKQyV4qH992Ue+2G4iSeG0PpKOHvJ3tZtRTyvUOxoUj0Lg3gx61tWVdubn1jJdZvb8fho/phdP/szhlTiXtLgJChwIRTVm6MainRxK8goSmp8qDX9aSHfkdiGVJaFI3g05+2AwC2t0bQtyKY5B4CQecp2FJwAGwo5qGHHopXXnkFvXr1ysqiCh02TyhFZYAPbsq81mBEnwqeaPyCcdDjgxsP77nJd+VGf73VhlSf7UoRWnW0qyP7JfKWTtUunptUq6WoclNKq6XY+AUR3BQCyczEQOaVGz69JbxZgmyTb56bTnUoXrBgQabXUVSkrdz0MmdBlcr2tFTMMhzTDt1phWRu5yX7oUrGR5vnyg1NS/Uu03f62VZuaPBEO/5mEz64iRu/4O9cKXhpwKrciINWYcC6E7v4bQC+WiozAX6Mq7oS3xNBtqGjcwquz42dn3/+Ga+//jo2bNiAaNR6FvyPf/yjywsrZFRjp2KvkHFjMKfclNiCm1QNxSGZV27kgqiWIoQw5aa3sdPPdpqFnhWnWoLdpedKMIaDKjftaY5foIZiNn5BHLQKgl1JKqUA06OXKUOxVbkR3ixBdmHKTSEaiikffPABTjzxRAwdOhSrV6/GXnvthXXr1oEQgn333TfTayw46E4lHc/NsJpSBHwySqRmy23JPTfGQU82zcSQJBbcaDblZvH3W/H2inrcePxopgLkioiisQCApqWyrtwYz2fvjJwNLGkpyVm5SbUUvM1mKDaVG3HQKgR2ppCW8smZ7XPDp7dEECzINvlmKO7UKqZPn46rr74a33zzDYLBIObOnYuNGzfikEMOwRlnnJHpNRYcfIfiVPDJHrx31SS8fvkvIdnGJfglxRim57xzop6LkIeW5ehnhpqRlpJsnpuHF6zBC19sxEdrtqX2YrIILY2VJLNra7arpZhy0w1pKXqQ8kiAx61DccrKjWjiV8iYPW4SpaUy26GYNyYLb5Yg2+TbVPBOBTffffcd61js9XrR0dGBsrIy3Hbbbfjb3/6W0QUWIqbnJvW31yt79IhXsQU30HeKYZeDGG3wF+SVG4BVSxGbckOVglQPqtmEpqTKAl5W2pzNgzVfmt2dyo3T8FSzQ3HnpoILz01hsZNNBE+QlvJk1lDMjzIRCp8g29AT03zx3HRqFaWlpYhE9MnV/fv3x48//shu27Yt94pArlFJesqNBUWfDg45AAAIQD8Iu6Uv6BlZkBqKDeWGSM6l4FHjrDAfzvhpd+KKoI81fsqmcsMbLLvFc2O8104xbroditn4hYDVc5MPn6MgOWwieCJDcYY7FCvCUCzoRqjimC9pqU6ZLg488EB8/PHHGD16NI477jhcffXVWLFiBV555RUceOCBmV5jwaGm6bmx3tlQboIVQNtWBKQkwQ01FEvW4Eb1GB+tzVBMzwojedAfhVdufN2gRPAGy9ZurJZyUm7SKQUnhDCFJ66JnzgjLwjSMxRnSLnhvu8iCBZkm2gxGIr/8Y9/oLW1FQBwyy23oLW1FS+++CKGDx/OGv31ZJQ0PTfWOxvKTaDcCG6MtJRLzpzNlvJY01JUubGnpagykg87O1oGXh70dotywwc33Vkt5RTkhtJIS0UUjQVKYvxCYbLDUG6qExmKM1wKLvrcCLoLQkjh97lRVRUbN27E2LFjAQAlJSV45JFHMr6wQkZNs8+NBUVP9yGgd8/1J0tL0dlSNLiRjZlS1FAcF9zkT1qqxfC9lAdN5SabB2s+LdXWrZ4b9+AmlbQUv1ZmKPaKJn6FBG3iV5WwiV+mDcXCcyPoHlSNsPEiBeu5kWUZRx11FHbt2pWF5RQH6Q7OtMCnpWAGN247J3pwC7BqKX3nqbk08csv5YYGNz52sKaNoLIBfybbnU38HJWbNKql6FpDPpk9VkAoNwUDIcRMS5V2n6HYotyIIFiQRfiAPF8GZ3ZqFXvvvTd++umnTK+laOiackPTUjS4SS0tFTS8Ofa0VLxyYwQ3au7P5Ghaqqy7lBvusdu6IS2VinLTHlNBSOKArs02egEAAl6jiZ84aOU9rRGFpZqqQt1oKBZ9bgTdBL/fzhdDcadWceedd+Kaa67Bm2++ifr6ejQ3N1v+ejpd89wYyo0R3PhYcOMcjNCgx++hhmJdsdE8zsqNkk9pqbCZlmKeG4d1EUJYT5yuwB80uiMtxRQ8hxw0VW4ISX7gsY9eAMyzI5FuyH+oaVySgKDPfZdLjeeZauIXVUVaStA98CeOnTruZYFOGYqPPvpoAMCJJ54Iieu8SgiBJElQ80AVyCVqJ/rcMHhDMQAvVHiguR4A6U7LL9kMxR63UvD8SUvxpeDUhOZUxfXgB2vwwAff47mLD8SBw3p3+vn4M9nuTEslqpYC9MA1yF220xa1TgQHzLSURvTX5TZpWpB7WLsGr2zZX9phHYqzMH4hH37vguKF73GT6DvenYjBmVmgS8oN9dwYwQ2QeAQDDXpYcGN4buBiKKZrywevRkvESEsFrNVSNEimrNi0CxoBVm5u7lJww+eFuyMtlahayit74Jc9iKoa2qMqqkriNmFQ5aaMS0vxee2IIoKbfIZVNCZQbYDMG4pFnxtBd0G9kvlSKQV0Mrg55JBDMr2OokJV3dMRSaHVUoahGKDBjZtyY5TfwVotxQzFxAyKNI0wNSEfdnaWtJSxYydEDwr4Hwl97eEuSuuKrVrKHkRlGqbguTxH0KcHN8l63dhHLwDWioSooqE00NXVdo2ftrbixv/7BpcdOhwH7V6T28XkGfT7G0hitDSngme+z41ISwmySb71uAE66bkRJIbuU7rU58Zfyq4KQEnQoZimpWyGYg9VbkyvCl8amg8yNV8txSsR9l439LUnmo6eCvzOXkvB69JVEik3gGkqTlYxRXvh8IZir+xh3698CFTfW7kFH6/Zjrlfbsr1UvIOZvpPkHoEeM9N6srN2m1t+NMLy/D9lpa42yzKjTCeC7KI2eMmf0KK/FlJEaFmohTcG2QjGPyIuaoWTLlhaSmr50bSzPvlW8dSVi3FdSgG4tdGX3skxVEFbthLbLM9X4p+D7wuCh5VYpIpN3SdpX6r0OrvhnlcqULXkA/pznwjZeVGTr8U/JWvfsZryzfjxS82xt0WE038BN1Evs2VAkRwkxW6Vi1lpKXkAOA1ghtJcVQtCCEOaSmjj4ZDnxt+p5kPByE+LeX1SKDZG/va6Gvv6g7afkac7cngSpIxHKlOBm+PxldLAfk1goHu3Jyq3Xo6qSo3nTEU08DXKUAWhmJBd5Fv3YmBNIKb119/HbFY18txewLmbKnOVEsZwY2XC24Qc1Qt+IO9F9a0FIy0lIeYwQ0fNOTDzo7umCuCPkiSZPa6sSs3LC3VtYO4Pd2VbeVGSzJANWQYTJN7buLTUgBfDp77z5J+t7I5PqNQiaTsuTEMxWmkpehn76T2xEQpuKCbYCfZhajcnHLKKawrsSzLaGxszNaaCh7Ta9GJO6tccMPSUs6eG2twY09LxVdLZftMjhCStCGduRaNeUnKg8ZIAZdqkUwFN/Yz4mzPl0rmuWFpqSTKjZOhGOAa+eVBcEOrJfJBEcw3aFqVfl5udCYtRX8TThVW1uBGfC6C7JFvE8GBNIKbPn36YMmSJQCQ9SqTQidRf5OksLSUH/DqZd1u1VL0bEySAJkqNLR5Hw1uCO+5yV5aqjkcw0F3z8dVLy5PaXteNSkzghufy/DMzKWluttzk/h7EExxMjgtWy8LOHtu8uGsPCaUG1dYn5tkpeCdMBTTx3b6PfOPI4IbQTah6eiCrJa65JJLcNJJJ0GW9UZUtbW1kGXZ8S9V7rrrLuy///4oLy9H3759cfLJJ2P16tVJ77do0SLst99+CAaDGDZsGB577LGUn7M7SHbGnvjOVLnhDMWS4mgo5uVuptAYfW6clJtsGoq/b2hBfVMYry3fjM27OpJuT/02QZ+HRfv+rKel7MpNbj03qVZL0bQUnQhOCeSRodgMbrI3G6xQod/bbCg3NLB1uo8i0lKCbsI0FOeP6JFyn5tbbrkFZ511FtasWYMTTzwRs2fPRlVVVZeefNGiRbjsssuw//77Q1EU3HDDDTjyyCOxcuVKlJaWOt5n7dq1OPbYY3HxxRfj2Wefxccff4w//OEP6NOnD0477bQurSdTsCqZLlVLmcpNwKWJH5sI7pXN+8lWz41E+OAme54b/szxnW8acOHBQxNuz5eBU3xeKe6xFFVjwWJXS8Fzpdy4pqVSVW4iyZSb3Ac3wnPjDvudJlFufJ2YLUV/E85pKTE4U9A9sD43eZSWSjm4ef3113HMMcdgzz33xM0334wzzjgDJSUJ2qqmwLx58yyXZ8+ejb59++LLL7/EpEmTHO/z2GOPYfDgwbj//vsBAKNGjcLSpUsxY8aMvAluuqbcGH1uOOUmkCQtFfB6ANUwe9vSUh4uLaVksTSU35G+9fXmFIIbfb3lgfjGdHzgFVYyd/YZp9xkObjJRJ8bTSP4cWsrAKBvhbVTX34pN/kzsyzfsJyEJIAZijvluXFKS+VXdaSgeClozw1vKL7tttvQ2tqa8cU0NTUBAKqrq123+fTTT3HkkUdarjvqqKOwdOnSvKnm0jIxOFO2VkslUm6CPtks+Y5Tbsz7RbPoueEPal9t2JU0NcWXgVN8cvzOnX/dXVZubMFNW5bTUskUvFQ8N99ubsbO9hjKAl6MHVhluc2fR4ZiRSg3rphpqRQ7FKeR2qPpasfgRig3gm6CpaUK0XOTbUMxIQTTpk3DwQcfjL322st1u4aGBvTr189yXb9+/aAoCrZt2xa3fSQS6fap5UpXBmeyaik/88/4oVgUDIplp0mVGzpbyhjD4HHrc5Nx5cb6eG+vqE+4PU0J8WkpJyXCEtxkcPwCv4ZskbxaKnlws/iHrQCAA4f1Ns+KPnsCWPJonik3wnPjhuUkJAF0tlRnDMVO77uYCi7oLgq6iV82DMU8l19+Ob7++ms8//zzSbe1B1a0/Ngp4LrrrrtQWVnJ/gYNGtSp9aUDq5JJ11xFiDUtxZr4Je5zE/B5TM9NXJ+b7ulQnG5ww9JSSZWbzLWQ7+60lJpEwQul0MTvQyO4mbSHMa8p0gK8cy0wbzoqPPp3JR8OXFHjvRXKTTwpKzfMc5NGWiqJcjNaWofx0g95oe4Jipco63MjDMUW/vjHP+L111/H4sWLMXDgwITb1tbWoqGhwXJdY2MjvF4veveOnxg9ffp0TJs2jV1ubm7OeoDTac+NyqXVZJty4xTcsGopPi1FPTd6kGMpBc9iDp5+uUfXVeC7hmaWmupfFXLcvjkcb5J1Msjyr7urB/H48QvZTkslVvCCSTw37VEFX67fCQD41Yg++pUtWwDoj1su6cFNXig3ikhLuZGqcuOjpeDppKVoKbjDd0BVFTznvxNBRLF/7ImUH1MgSJd89NykNRV8zz33zKihmBCCP/7xj3j11VexcOFCDB2a2IQKABMnTsQbb7xhue69997DhAkT4PP54rYPBAIIBLp3ZHKyM3ZXqGoDWJWbtAzFRlBjBDkyXy3F7QBVY0J4p0zPDtBgaWCvEEoDMr5YtxNvr6jHRb8a5ri9U1rK59DEL6OeG/v4hW5q4uf2PUhWLfXZTzsQUwkG9gphSG/jt9ZqBvZlHl2ty4ezchrU5EOglW+kq9ykEyBSRdcplaUpUVRJbQCAMq0Viqqx1JdAkEnY+IVC9Nzw3HzzzVizZg3+/e9/Y+7cufj666879eSXXXYZnn32WTz33HMoLy9HQ0MDGhoa0NFhmlGnT5+Oc889l12+5JJLsH79ekybNg3fffcdZs2ahZkzZ+Kaa67p1BqyAd3ReNL1JdHUEhCv3Dj1uWFVGB6ATv+W3ccv2HeAmTwQ8U2cjtu7DkDi1JRTWsppECQf0GRq/AJNB3VbKbiLVJusWor6bX41osZMubZuYbeX5ZNyIzw3rqSs3HSmFDyBYiZxJ0shKSIqpgRZo6A9N5TPP/8ce++9N8aPH49f//rXOOOMMzB+/HiMHTsWX3zxRVqP9eijj6KpqQmTJ09GXV0d+3vxxRfZNvX19diwYQO7PHToULz99ttYuHAhxo0bh9tvvx0PPvhg3pSBA8mnQbtCG/h5fIDHo6s30D03yfvcWIMbyfA+ecB7bqw7t4wGN8ZBzS97cMzedZCkxFVTTtVS/iTVUopG0mpwZofK/ZUh/T3KehO/VD03LkHbhz/oBnmWkgKMtJROqaR/X4TnJr+xnIQkQE4zLaVphP2GnQaWSqoZ3AQRFRVTgqxh9rkpQM8NAKxcuRJTpkzBqFGj8Oyzz2LUqFEghOC7777DfffdhylTpmDJkiUYPXp0So+XyhyiOXPmxF13yCGH4Kuvvkpn6d1Kss607nfkzMQAS0sFoE8Ft1epUUk66ItPS9F/eUOxPZiJqCqA+FReZ4hykXu/iiAm7NYrYWrKMbhxUm5sB+6I0nlpnXqOqkp8aGgOs+Z42YIGuW4KXihBtVR9UwfWNLbCIwEH7c55ybi0VAg0uMn9QYsGNYpGoGkEngylO4sBlpZKOn4hPUMx/7k7DdvklZsgonnxPcklhBAs+n4r9hpQiZqy7rUqFDt0tlw+eW7SWsnNN9+MI444Ap999hl+85vfYNy4cRg/fjzOPvtsfP7555gyZQpuueWWLC21cOi054bvTgxwaSk9cLHvnCzKjWZXbro5LcVyrvprnjJKL9dftnGX4/atTh2K5fgOxXafTVdSUzTorDCUm7Yce24SVUtR1WbswCpUlfjNG1rNgbUlhnKTT2kpwGpcF3BpqSRN/OiBIVXlhlfsnNNSZpo7KEXzQuHLJa98tQnnzf4Cf337u1wvpehg+/88Cm7SUm4WLlyId955x7HkWpIkXH/99Tj22GMztrhCpdN9buiZltGZmDcUA3p1FJ+3j/BnhK3WDsUex1Lw7KWlzFJA/TUP7KVXSTU2hx23b07ZcxOv3HQWmtKqosFNtqul1BQ9Nw4BGw1uJo2osd7Qwik3hJaC5z6YKIntwjXeVzFXnYSYShBIa89S3ERSVW7SNBTzgb9TWgr2tFQefE9yyYtfbAQANDQ575MEnYfus/OpiV9au6CWlpa4Bno8tbW1aGlp6fKiCh2NdLZaiio3RnBjKDcBSVcYwoqKSi6NZMnlxzXx07eTLZ4bm3KTQX+E3VDWr0JPrW1pjjhu3+JQCp6sQ7HT5bTWqFk9N9lOS3VWudE0go8MM/HBvN8GsCg3NC2VD8rN4coiXO79P1SjGTHlXECo/oy0S8FTNBTzvwWntJSsmL+9IKJ58T3JFRt3tOPzdTsACF9YNojloecmrTBryJAh+Pzzz11v/+yzz7Dbbrt1eVGFTqc9N6w7sVW5CXmM4MZFxXBOSxnBTTcpN/Y+B7VGcNPQHHb0VpnVUmaw5qTc2M82u1IOzpSbEh977K4YlJNBg1w3BY9XbjTu4ERHLpT6ZYwfXGW9E+e5CZD8aeIXInrJcaXUJg4eNiIploLLbCp4qmkpTrlRtbjfmUfjqqXQs9NSry7bxP4fFRV9GScfB2emtZIzzzwT06ZNwzfffBN324oVK3DNNdfgrLPOytjiCpXO97mxBTeGChOiyo3twJ6ozw0bv8ApN/YDeUbTUrbZIn3KA+w5mjqsM786oipr4ke3A5JXSwFdO5Dbq6WA7M6XSlW5AawHqo/W6CmpibvXWHcWagxo384uBvIoLeXVdNUxhKgoObYR5k9CEuBL01DM/zYIMfc7FI9q89z00GopQghe+epndtkxhSfoEgXvuZk+fTref/99jBs3DkcccQRGjRoFQK+iev/993HAAQdg+vTpWVloIUF3TulXSxnBjc1zE3RTbmiHYp8nrkOxx0OVGzP1Yj9jyUqfG+PLHfTJqCrxYVd7DFuaIxZTbH2TXh5e6pdR4VAKzh8c7Smbrig3VLoP+b3wyRJiKkFbRLEEO5lETaLg8cFNR0xlSs767boKsveASusduJQUAAQ0/X3Mh+DGp0UBWQ9uRK8bK5aqxgTQKkCNIKWKM/v+IKYS8PGTrFnTUvnwPckFX23YhXXb29llEXxnHr4VSL6Q1kqCwSAWLFiAO++8E/X19Xjsscfw2GOPoaGhAXfccQcWLFiAYDCYrbUWDJ2eLWVPSxlBDvXcJKyWomdphtrjoR2KEyg3kQz+yJ36HPCpKZ56w9BXVxWymNN9LC3FdSi2KTVdGZ6pcGssNbw+2exSnEy58Xgklqrg17GtVf8sa8r91jtwDfwAwK/lRxM/Qgi8RF9zUIqKtJSNVJUbfn+RSsWZfZiu/T6yxik3PTgt9eoyXbWhXb7F9zPz2Ktl84G0gpvvv/8efr8f1113HZYvX4729na0t7dj+fLl+J//+Z9uH3OQr3S+WsruubEZil3MtU59biQvHb9gzcvzZKMUnHfL92WmYmtws8lo7FdXaQ2EnZSbuFRcVwzFKg02PCj16+9PNudLqSkoeFSt4T/bba369yCuF0dccJMfyo2iEQSMir4gIjkPtvIJRdXYyU4y5cbH7S9S8d3Yfwv2dIvMV0tJPVO5iSgq3viv3in91/vrMwVFWirz2Ktl84G0VjJ+/HiMGjUK1113HT799NNsrangYW330x2/YE9LUeXGOHAkNhRb01KSoeDI4GZLZTEtRdUWXpbsZ/hp7OXg9bv0ywNsQzWpcsPvfDJaCq7xyo0eVGRzMngqA1TNiinzdW1vo8GNTbnhysABwKvS4Ca3Z+QxVUNA0r+jelpKHDwo/Pc1mXLDf09SCW7ilBvbfbxxyk3P+1wWrGpEU0cMtRVBTDIqD4WhOPMU/PiF7du345577sH27dtxyimnoF+/frjwwgvx+uuvIxwWvQMonZ4KHqfc0D43tBQ8dUOx5IlPS3WHcsNH7rWVbmkpqtxYg5tACspNJpr4+WQPSphyk73gJhVjOVVuLGmpFiMtFafcGJ4bI+j1qbqPINdKSUwxlZuQFBGeGw7++5qsWopP6aZiKo733NjTUqbnJoRIjwxu5n6lV0mdPH4AK8UXwXfmYdWyedTnJm3PzQknnICnnnoK9fX1ePXVV9GnTx/8z//8D3r37o2TTjoJs2bNQmNjY/IHK2Iy5rmxNfFzVW58nnjPDU1Lwfwh288Gs9HnxueYlrL2utnMPDfWtBTN1/I7H7sq0SVDsfG4Xlli/XWyOV8qlfSkfb5UW0Rh/48Pbgzlpnqo/rhqfqSloqqGAMxqKXHwMKGfjV/2JDUIS5LETohS6XVj/9zt77uXmFWKQSnWpZRuIdISjmHhav1YdOq+AxyrMQWZoeCVGx5JknDQQQfh7rvvxsqVK7F8+XJMmjQJc+bMwaBBg/Dwww9ncp0FReerpawBCjtDN5Qb+86JVUs5pKVkL01LaYCxnu7oUOznAjr3tJR+UO5vU278xrDPiENaigYBXUnBsODG40GJP/uTwVNSbnxWz812w0wc9JlrZFDlplqf1SUr+vuYc+VG1ayeG3HwYIRT7HFDod+VVA7AcZ4b28mLT+vZHYq3tugqYnnAiz36lTuePAkyQ8H3uUnEiBEjcPXVV2Px4sXYvHkzjjzyyEw9dMFBFeX0+9zYB2fqAYqPKTfpp6X0BekHcHsX01ykpQghbFJ4nHLj0H6evmbaeK9LTfw0mpbilZvsBzeyRwI2fQksedT8chiYaSn9s9zKmYnjxpxQz00vQ7lR8sNzo6iEeW78kgol6tyVuidiqquJ/TaUdOZLJUtL+XjlpgempezvvdkBnaQ0tFmQOvnYobjTE2C+//57LFy4EI2NjdC4HbYkSbjxxhvRu3fvBPcubjqt3LC0lE25IakYiq0dij1erneLpgDwx1UJZPIMm5r0+GopOoJha0sEqkYgeyQ0hxXWOC9OuUkwW6oy5EN9U7hr4xdotZTsQUmAKjfujzfvm3rc+fZ3ePCs8Rg/uFfaz2dJT779Zz3A6T8eGHwg28aeltpuBDe9naYWM+VGD248Mb0fTlSJnxjfnUQ55QYAtKjw31HSVW7kNBr5JU9LRQHjK9ETxy9YxtPAeuIVUwn8eVS2XOjk41TwTgU3Tz75JC699FLU1NSgtrbWslOlwU1PxkxHdLIUXLaWgvuMHiJxPV9oKbhMAFrybfPcAGDKDd1h+mUPoqqW0TM5J+WmpiwAj6Q3JdveGkHfiiAzE1eV+JhqQXHKiXdwwQ2QmcGZPg/X5yZBWurNr+uxcUcHFqxq7FRwYwlyaWdhWzk3G8FgBHy0x00fe6UUIZznRk9LeRTdUKwRXZXK1VlTLC64acvJOvIRc65UavsC+hmm4rlxauJneSzCdyiO5Vzh626ituCGDzCjqpZXQx4LHadWILmmU8HNHXfcgTvvvBPXXXddptdTFHS9WspI1xhBjmzspOxTrNmZiYc74HusHYoBsOCGqislARnRdi3raSnZI6FPeQBbmiNoaA7rwY1RBm6vlALclBt7WqoL1VKaqdzQPjdtCdJStN+MfXxEqlg8NzE9qEO42bJNia3PDX3O3qU25aZjp2kaN5QbxLiuq4qWs7OmmKqhFOaBVIu2J9i6Z2FRV1OAnhCllpZyV24IIczkDfTMaikazPmdlBtFE8NdMwjdZ6dtxcgindob7ty5E2eccUam11I0qGpyI6nzHelUcOOs3UuDGxUS4mc0sbNCmdtpGWkpr+yg3Bg7P3pgz46h2PqVsk8H32woNwNsfhvAmhOnUNNkVUh/TzJVLUWVG3vAyENVlF2dDG4s1VI0uIlYgxtanko9NzQtFd+d2EhJBauAkK4iSZpims1zeODi+9wAAImJ4IbC0lIpKjdeB99Zssem8PeJqcSipgXR82ZLmeNp9N+Y7JFAd8nCVJxZaLf7VL1l3UGngpszzjgD7733XqbXUjR0XrmxGYpl8wDnh4Jm7iDLdz4NSNxOzlBsPLKEKDG+aNRQTIMbw28SVTMnU7u13+5n61LsqNwoUeDpk9B/2X0AbNVSxtmXOck7A31uPB6UGopJW4K0VKaUG9kDU2WxKTd2zw0bveBWBl5eC/hK2dUVHn2NufRTRBXrgRRCuWGwE5AUlRtmKO5UKbh5H0XTLMpNoAempeyeG8B8f0VFX+YghMSlAPOBTqWlhg8fjhtvvBFLlizB3nvvDZ/POnjwiiuuyMjiChW108GNrRTcax7gAoihOWweQPgdm58FNxLgMc9SVMgAVFZJRXd+VLWIKZmrGHBXbvTXQIMbx0qphhXATwtRHVgG4AAWKKkaYWuuzEi1lINy45KWiioadrXr7xv9N+3nowoeVLNUP5JiWsqtgV9ZX13Z8/gALYYqbxTbo6U5PXDZPTeICUMxJV3lRk6jFDwd5aYnp6X4A65f9iCiaKLRZAaxduEu8ODmiSeeQFlZGRYtWoRFixZZbpMkqccHN+wg2ulqKTp+warc8AqCY3Aj+wDD3C1LEhS4KDc0LZXRJn7Obvl+5VblhqalLJVS4V36miNN8EFBVImft0TTUl3rcxNfCu6WlqIjEICuKzd+rlNssrSUOVfKZfRCWa3+r78ECDehQtbXltO0lKLaghuh3FDSVW7oPkNNxVCcoFpKUTUEJev4hZ5bLWW+9z6vB4iItFQmSWfESHfSqeBm7dq1mV5H0aBpBHS/1OXxC5KkBzhqFH7E0Nxhqgz0IO+TJcjEuJ4zEXs8ZnBDNAUSzB80VQuyPTgTAPpVWj03bCI4PzQz3MT+W41mbFf1NgJ8cEOrpbo2fiG+iZ+bckNHIACpBTcbd7TjrCeWYOpBu+H3k3bXn4/OsuKaqcWlpWzVUtvb3NJSRpVVWV/9X1+pJbjJ5YFLUSLwSNzBWBHKDSWSpnKTTp+bRE38+GGmAPXcpNDTSdOAdKs88xSn955Wo/W0QC+b0PdSkvKrz02XvsXbtm3D9u3bM7WWokDlmkN1uRQcYP4bvxSzHGRpeibo0J1Yf24JivHxqjH9gKnY0lKZOtvXNMI1yHMzFIdBCGHBTX9+aCYX3NRIzVA0Ak0j7MzU7/Ug5Pd0ec0xru+MaSh2CW5aTbVlV3sUWpIz6c/W7sCmXR14a4U53JLexc8HNy5pqY6YiphqpsJcg5tyTrkBUCHr68xlWkqzNe2TFKHcUNJWbtIxFHMHFft9ooo1VShLBKoSRUIW/x24Zwiw9fuU1prvJPLcCOUmc/Dpv1z12nIi7eBm165duOyyy1BTU4N+/fqhb9++qKmpweWXX45du3ZlYYmFBS8ny+lGscxQzB3YjNSUH/rMIRolsy+UL747MaArN6qh3GisFNxuKM7MD5x/HHvkzntutrfp0rgkmUEPAFtw08Qek/Xx8XqY3JkJ5cYne7g+N86Pt5ULbjQCtCbpZLyrXT9w7GwzDyAsPZlIufGZyg0dvSB7JFSFrD42U7npp//r103FFR79PrlMS2m0EszAI5QbRtrKjScNQ7Hx2DTFqvBpKZtyAwCIWj+nONZ8oP8Wf/48pbXmO05pKb9DRaaga0Rc/Ja5Jq201I4dOzBx4kRs2rQJ55xzDkaNGgVCCL777jvMmTMHH3zwAT755BP06pV+w7NigQ9uOl8Kzis31uGZzeEYasoC1rlStqGZgNVzoym0FNxQblgpeGbO9vmzIHtaqtYIYna2x7B+u97crU9ZwLodF9z0hhnc0FRN0CezJmidNRTz6UKvR2IBXltUcezuyys3ANDUHkNF0BZwcFDFhQ9uaEsAXwqem46Yyp6zutQfP2SxxRbcGBVTZXI+BDfWYEaKJTmI9iDCaVaRdMZQXBH0oSWssD5WgB7olEq24EZJ8rlEW/V/I60prTXfcargceqlJega9pL7fCGt4Oa2226D3+/Hjz/+iH79+sXdduSRR+K2227Dfffdl9FFFhL8GVenPTdyvHJT5SdABGjuMIIb/ofrkJaSPRIUIgMSmBxtloJnts8Nfxbks6XiKkM++L0eRBUNyzbsAgDUVdka+PHBjaQf/GOKxtSpoE9mZ1+dTb/EuHb2XtmDoPHZaEQPmOzdknnPDaD7bgYlePxdHfr2LREFUUXvfsqaBiZQbkqMQJMPbuJSUoC1FBxgaakyKfel4KotuKHTygWmuhJMccdP01KpGIrpPoAqN/ZqqSCs32FPss+FBjXRlpTWmu84VUuJtFTmcXqf84G0VvPaa69hxowZcYENANTW1uKee+7Bq6++mrHFFSKWtFSi/OP6T/Qct8YdrO2GYu7/vfz6j5H6bizdN53SUpIE1fh4iWqtlmKG4gz9wM1p21Kc4iBJEktNLdu4CwDQnzcTA5bgpp+nma2N+Yp8ni4rN7xB0ydLCPlk5lVwmgxuV26SlYPzt9NAh3UoVt09N05pqbhKqVjYfI+YodgIbjzUc5O7nTWJWd8rjyrSUpRwLD3lpjODM8uCTmkpLS4tJSnWzykOqtwUyfgMZ8+NYSgWwU3GcHqf84G0VlNfX48xY8a43r7XXnuhoaHB9faeAPVZeCTEpxZ43r4WmH8HsP5j8zp7KThgKjcBfWfXHDY60vJSoG1oJqAHGjFDmNNYcKM/RlmGlRv6OG7t/2lqajlVbuyjF3jPjYcqN8T03HDKTWc9N/zBwuvxwOORUMLKsOODm60ttrRUkoop/vadbfr/WTNH/oxZCZv9jABmlE6o3FC/jRzQOxQDgL8MAFCSB8oNsSk3XhHcMHj1MRVoKjuWwuBMGjiVB2lrB/M7HlPMrtGarP/+knqhaFBTJGkpp3SJUG4yT9TB25QPpBXc1NTUYN26da63r127tkdPAwfSGJrZtMH4d5N5HUtLcWfuRqBT4dO/QM0d1r4mAa+H89zYDcX6GjTjdhp4lWQ6uEky7r6vEdxsMhr49bePXuDUjBqJKjcqp9zI7OAQMSZgpwt/sKDrpOm5RMoN7WRM1Rg3eOVmR1sC5QawvN4QTUtFVW6ulMvohbJ+ZmmMkZYqRe6rpYgi0lJupHtWm6pyQwhhn3m54QXjD9h8qpAEKwEk+Vw01exPFC2S4MapiZ9XBDeZhn3HUzTNdxdpreboo4/GDTfcgGg0fkcfiURw44034uijj87Y4goRulNKGNvEOky1gp8SbR+cCTD/TaXPmpZinU+9HkCN73MDgFVLEVU3zZrKTWb73Jg9bpwjd9rIj5JQuaGGYptyw09V7kwKhn4uskdi5mFWMRWNDwxooLF7X10hSZaW2tkejfs/mwqeKLjhgrbGFjpXym30ApcONtJSIUO5yeXcIGJLdwjlxsT8naZ2VpuqoTimmgZ55rnhfhdqxAxkiKH2edSI+4kBn4qKFIvnJkEpeAa7s/d08tVzk5ah+NZbb8WECRMwYsQIXHbZZdhzzz0BACtXrsQjjzyCSCSCZ555JisLLRRSUm5auNQdPSvXNDO9ZPHc6Gfx5VS5CVuVm6BLWgqAWS2lKhajc0mGOxTTHYXfRbmprbQerOvsyo2tiR9dG50rxZeCA/qBPFWZn62R8wVRaMWUXbmJqRp2GsHM8D5l+PrnJstcLyeaEik3mu2MORwf3ADAzzv17eKVG1ulFMBKwUuIHkjk1ENgU24sBuoeTrpntakaisOcUkfTUvxvXDMqozRIgKHcBBFFTCXwex1+p7xaU3SeGz4tJTw3mYaeWLmd3OaKtEKtgQMH4tNPP8Xo0aMxffp0nHzyyTj55JNxww03YPTo0fj4448xaFCimhIrixcvxgknnID+/ftDkiS89tprCbdfuHAhJEmK+1u1alU6LyOrpDQ00xLcGAculTv7tXhu9P+Xe50NxQEXQzEAqJKh3Cgxy5lgWYab+LG0lEvkbulpA2BAgmqpXmgCQBCzGIpl+GRzom+4EykYpyaDNMiz97qhwYnskTCkRg8iEik3MVVDCxcg7bQFN3FeB0654RWpn3fqaYE45cZeBg6w4CaI3Cs3ks1z4xPKDSNd5SbVPjf0cSXJeZyKFjWCXvgh+fTfWxAR94M677MptrSUz0G5EcFNxshXQ3Ha4xeGDh2Kd955Bzt37sQPP/wAQB+kWV1dnfaTt7W1YZ999sH555+P0047LeX7rV69GhUVFexynz590n7ubGEqNwmCm1aH4IaX9uV45aZU1n+odASD5axEdVZuNJaWilnKtTNdCp7MUNyXS0t5PZLVMKtELbOIfFBQgXZEFa6Jn0/vfBn0yWiPqp06kLPRC5y6VObSpZiaiatL/ehlqCiJPDd2VWdne8w6hsPudeCUG0nSK7c6YiobUVFT6jZ6IT4tFWTKTe48N5JqTUv5SJKqnB6EqbCmp9wkO/hGuCosn6HE8GkpavKOSX4E/EZwI8UQianse2+BD2iKzVBsG5wJiOAmkxRFWoqnV69eOOCAA7r05McccwyOOeaYtO/Xt29fVFVVdem5s0VKE8Gd0lIqd/DkgxTDf1PGghtDuYlxcrdLWooqN5qmWH7MfCm4UwO7dImpiYObWq70u19F0Pre8KXR/jIg2ooaqcnaodhI3QS8HrRH1U4pNzS449OFbvOl+KolOtMqUbXUrrjgJmoZw5FIuQH0+VIdXBVYTblLWqrcQbkxgptcKjdUdVThgQzNOkurh+OUGklEqoZi84AiswM2r/YQIy0Vk3jlJuqu1kaLUblx6FAsmvhlnHS/491FfoVaKTJ+/HjU1dVhypQpWLBgQa6XYyEl5cYpLcVGLwTNihiAVU6V0OAmnEZaik4FV2NsZ+n1SCzCJiS1Nu/JcBuaSenLpVniKqVoSspfzpSJ3mi2KTey5d/OlIOzIZZpKDc1ZX42BiFRWmpXu1XV2dEWtXgmPPbOsC4jGCi97cpNS72x4HjlJkD0x86lh8BjqI6tnnIAgF8oN4xwmuMX5BRLwfkeUDQg4r8DvHIDbyrBDeezKZLghgYwfgdDcVSMX8gY0WKolso1dXV1eOKJJzB37ly88sorGDlyJKZMmYLFixe73icSiaC5udnyl01YhUyiuVJ8hVR4l56Sor1PZNuBzfDfhGT9ANwUVwruPDgTADTJNBTz6gr/Y8/EGQwLblxec2nAi3IjkIivlNql/xusBEr19GJvqdnquTHWy5eDp79Gc2gmhXpu2mzVUtuMZnp9ygKoKklBuWmPV26URMGNg3JDqQh644PEpp/1fys5P5uh3NChnDn13BjKTbtHTxUHhHLD6OzgTDVF5Sbok81UFv+7MBorxjwBwFBuQlLEvWVApAjTUqJDcbdQdGmpXDBy5EiMHDmSXZ44cSI2btyIGTNmYNKkSY73ueuuu3Drrbd21xJTrJaqt15u3cIpN7aUhBHshCRbWsqi3MTPlgKspeAxznPCDziLKhrsQkG60LMgt7QUAPSrDKKlsdW9UipYCZTWANCHZ/LKTYBLSwGdVG5ocMd9LrQk3q7csLRUeYppKSO4KQ940RJRdOWGOzhJNLgx0m68gRqwKjdxZuJoG9C+Xf9/VXxwQ1NAOVVujOCmw1sBKIAfiXsC9STSVW5SNxRznhuHtJRm7E8Uyc+CmyBi7kEwP3JBi+knXN4u7hhyjGO1lIM/SdA1zGqp/Apu8ms1neDAAw9kxmYnpk+fjqamJva3cePGrK4ntWqpLdbLrY1cd2Lbwd8Idmi3UdqhmK8kcktLaZIeuxLNNBT7ZQ+8sodVHmXioJjMUAyYXYpdK6WClWy0QI3UpCs3fLk7zCCnMyMY2JwnXrlhaSm7csOnpfT3vz2qup71Us/N0D56wLGzLcoUPIAbJEnTSrY+IrxyE2cm3mV8XwOVrKQXAEtL+Qyzci6b+HmMwaARn6HciLQUI2L7DicjVUMxn7J1MsnS75zq8bN9SgBR99+7vfy7CNQbpzJ8YSjOPMJzkyWWLVuGuro619sDgQAqKiosf9kkNc+Nodz4dY+Crtw4qy9UuQlKZlpK707KVQIwQ7FViGOl4DblBsissS6ZoRgALvzVUBwxuh+O3qvWeoNFuTHSUjbPDVU2aHqqMwdys8+NucZSF88NDW76lAdQHvQyC5SbekM9N0ONsvG2qMoaA3okQKKqHB166TJfCnAwEzcZwQ2v2gCmcqPqlWa5NEjKRmAepcENIrqhq4ejacRxMnUiUjUUmylbMy1l+Q4Yn4niMZWbECLuyo09mCkC300k5p6WEp6bzCHSUg60trZizZo17PLatWuxfPlyVFdXY/DgwZg+fTo2bdqEp59+GgBw//33Y8iQIRgzZgyi0SieffZZzJ07F3Pnzs3VS4iDKgQetwqkWNj0mdTuDWz4RA9ujDNxN+XGbwzBUzVilENzcne7c4di6rmBFosLQPyyB+GYlpFeN/SxE325Dx3ZF4eO7Bt/AzXXcsFNjdSEzSqxlILr/3ZBuWGpM66Jn1u1VAsdYBmAxyOhIuhDU0cMzR0xS1k7haalBleXQPZIUDXCAiSvx2OWulPlJoGhOM5MvMsY01E12Hq9Edx41Q4AJGM9izqDrBkdmf1V+pqg6WqiPcXaw+BVklSVm1QNxXwPF0cfiUKDm6CZlpKi7icG9mCmGIIbxyZ+QrnJNEXT5yaTLF26FIceeii7PG3aNADA1KlTMWfOHNTX12PDhg3s9mg0imuuuQabNm1CKBTCmDFj8NZbb+HYY4/t9rW7oWrx/VQs0B43cgDos4cR3DQCFQP06108NzKJwSdLiKkETR0xW58bZ9VHMz5ewnUoZsGNVwagZOSM30xLdaKk3MFz01tqxjpFY2eZwUx4btjnkrpyQ/vxVJXowY1bxRRNS/Uq8aNXiQ/bWqPMlCx7JH3cBuCq3JTwaSn70Ewa3FTalBsjGJZAEEjkpegGZCMtpdKhnoAe0PXw4Ib/TFLd8VPFN2mH4pj5+3cqBadqoSabaakgYqmVggMFn5ZSVI29H05TwUVwkznMaqn8SkvlNLiZPHlywiGIc+bMsVy+9tprce2112Z5VV2Dn2HkCPXblPcDyoyDHW8odqmWkpQIKoI+bG+Lojkcs0qBLmkpjU9L2QIQ+oPPhOcmloKh2BVLcKMrO71hVEspzspNl6ql+PEL/njPjaJq2NFuKjcAUBXyYT3cy8FpWqqqxIdeJX5sa42ycnIvH9wYniK7chP0dz4tBQAlCCOSw501VW4kfxkU4oFX0vTXHKrK2ZryAfr99UhJ0tQcqaelzN+GU1qKVrCpnoA5hwwR7HBVbmyem2iL83YFAr9f4z03dL8ngpvMka/KTX6tpghI6rlhQxDrzINda6PpubFXKFA1Ro2alTvtMesXymVwJjUUQ1PYj516TjLpuWHKTWe+3Cy4qbCkpaKKho4onS1F+9x0XbnxWZSb+LTUjrYoCNEPSNVGd+KKJBVT9PqqEh/raEyDG1mWuLRUcs9NfFrKCG7syo1HZoFwCSJod5hs3l14if7d9fhD6ICxfq7rdE+FVx5TbZSZcodizqjslGrx8MqNz1BupKj77z3Oc1PY86V41YyvDmWeG1EtlTHy1XOTX6spAmhnWnflxghuyvqZHozWLVy1lLNyAyWCcuMg2xxWuA7F7oMzTc+NYnpOvKbnBsh0n5uuKjd6WqpSaocWC7MzX7MU3FBuOhHcOPW5cUpLbW01Ry/Qz7CqhI5gcA5u6BTwypAf1ca2W1v1g4tFuSl39tzwaak+rsqNzXMDmMMzpYjr2roDr6HceP0hhKGvX42K4CbciZ1+urOl3ErBmXIjB1kTv0CqHYqBgk9L0dfp9UiWVHQ6huLP1+7AN5uakm7X03Eac5EP5NdqioCkfW5ocFNeawY3LXyfmxSUm46YZWK26blxUW64aqmApAGtjaZyk4GZRMk6FCeED25CvViFlze8w9KFlf833ImATHEYv1BdagYtdNgl9crw3pcqppg592+h6SpdudG3paZkDxCv3MTaTLUNVrOpRblRImZlXaLgBmHsbIsmTPFmE5+h3HgDIXQQ/T1VIoV95p8J7J6xVGCG4mSl4FwTP1bezP0uaO8hzRNgyk0I0QR9boxghhY2FLih2Kk7MWCe3CXrc9PUHsM5Ty3B2U8uYT2yBM6IUvAeQnLPDR/c0LQUXwpuV26M6hwlgoqgHqw0d8Ssyo1rWsrw3GgKYkbQdWnr/wIz9sAIsg5AhtJSrBKri4ZiSUKHrxoAEIhsdx2/0Bnlxmn8Qk1ZAHvWloMQYMFqfcbXthazDJySqJGfompoMXoPVYV0zw1gKkAlHgWAEXSUc2XwXGrKtYkf7UzsDQElveNflHEgKpEiUDRimUzendC0lD9QwtJSalgENwnl+uZ6S4BLYR2Kkyg3ES7wZ54bTo2gvYeIbHpu9PELSToUu/RiKjTc3nt/imm/jTvbEVMJmsMKNuwQKmQiRFqqh5Cy56aMU27UCNBmDNCMS0v52TaVLC0Vc+lz4+65oWcqQ2M/AiAYrq0F0Dlzrp1U+ty4wgc3ADr8ZnDjXi3VlfEL1jUeMVr/DN7/Tjd62yulALARDE6pH9pUEdCDoGqb56ZE5tSeQIUZrHIHD9rEL+D1sPJ0AFYzsZNnw68ftKpkfV272nKTmvIbwY0vGEIHS0t1JLpLj4CvaLKw5VvgH6OAZ04GNGuwke7gzKBX5tJS5u+C9h4icsCslpJSmC1VXme9XKC4qQmploI3NJkjRNY0FraKlW2cmiXmA/m1miIgaYdiVi1Vq8vFtOssLfmNS0tRz03UYmxNZXAmPPoPW9JibMdXSvQfagX0nVdmS8G7HtxEaHAT3cEUoZBNuenMVHBz/IL1czl8lB7cLFq9FRFFtXQnplQmGJ5JK6XKg154ZY+p3BjBTanEfTayVw9wAItyQz03NWUBq/F0VwK/DaCPcwDQJ6gHWDtc0mbZhqalZH8QEarciLQUN//J9rv4eSkAAqz7EPj4ActN3jQHZwZ8Hse0FA1uNK/Z5yaUiueG+sIKPC3F9wHiSdVz09BsBjc/iOAmIVGRluoZJO1zQz0UNEVB1Rt6IHMpBYcSRkXQUG46FC5a5sYvuJSC69VS+o+5RLMFNxksBU/bUKzGdP8JABg9UiJBPf3iD29nm9GDQ4ClpTIzfgEA9h5Qib7lAbRFVXz643ZuInhqaamdnN8GMH08HUbqrEQyAg7qZQgawQ1nKqaPX1dpaxDo1uOGYjxmb78e3FDfUHfjJ/p7IPtCiEg0uBFSvqtyQ9ONALDgr0D9f9nF9EvBucGZ3H1oeT685uDMAFyqpQjhghtDuSl0Q7GLyTVV5WYLF9z8KIKbhIhS8B5Cwg7FSgTo2KH/n+5EWHBDlRt7Ez8nQ3HU2tadpaVsTfw8NC2lQlE1SNAQ0vRgohz6DzaTnpu0DcV81ZChaEQDenATjHLBDS0Fp2kpm3LTEo4lfR3m+AnrGj0eCYdzqSlHQ7GhxjgFN00dRo8bYwYVLQWnlLLgJmR5nbxyc9DuNbjq8D1w/XGjbA/u0uOGYqSlqn36unbkKrgBp9wYwQ0p8LRGJnBTDyxeKi0GvPJ7vXM5UjcU89PGTTVCY6Zy2lgRXq5aSlIQjTl8R2IdADGej+6PCrzPjVtayu9N7f3l01JCuUmMOeZCKDdFTULPTauRkpL9QKiX/n9mKja8OHHjF8xS8IqQHqxsbTV3UInSUoR5bvTxC2UIw2OYW8tI5oKbWGfTUnQMhb+MqU6KodyUxHbqN8keeIz30hycaQY3zeEYfnn3fJzz1JKET8VK4R0+lyOM1NT7KxtN5abcwXPjkPbZZVduSqzBTUlccGPME+MCO7/XgysPH4F9B/eyPTjtceOWltKrpaq8hnKTo7QUHQ3i9YcQlfTvrxYLJ7pLj8BduTE+18Nv1htXbl0FfHAbgNQNxfy0cV4xZfsfaij2mtVSAKA5eaH4FBTdHxV4cOpuKNY/i2TVUltazOGvaxpboSX5PHoyEZfKtFyTX6spAkzPjcNbS/02ZbWmQZSeKVHiBmfGKzdbOck04JUBzTC12tJSRKKeGxUxlaBSMndYZVoGg5vOVktR9YKbdq2E9EZ+ZYoe3PBnvebgTHPNaxpb0RxWsCJJP4qYw/gFysTde6PEL6OhOYzvG/UzVifPTVNHLG4nZwY3VLmxBpglkrGTtKelbI38HGlymStF8enBTaWsP0cughtVI5bgJibp7wMRfW7cPTdUuanbBzjpYf3/Sx4G1n/i2LPGiTB3tsynWplxntC0VIgpNwBAog5BJw1u/GVm8F3oaSkXk6vPSyvLkgQ3nHLTEVOxuUkY5J1wG3ORD+TXaoqAhMoN89twAQ09U6IkUm4Mzw0tM/ZIRkBB+9zYlRt6WdP73FCfDWB6bzLhuel8WspqJgYANaQrN1XaLv0mrkzaaXAmLd0Ox7SEZ1eKQxM//nEnjdCDKtoqpo+D50YjQKttyCatoKK9cMoCXstnH4pTbozXGk4cjEFVgKZN+v+TpKXKjYqsHTmoloqpGgI0uAmGEPXo318iOhQ7p0Y0DWg2PtfKgcAeRwJ7/1q/vPJ101Cccodij0UxZZ3I6UgMXwDweKB6jKBTcfhcIlxwY5jUC95Q7KKapdqhmBqK6QFbpKaccRtzkQ/k12qKABrcyLIENHxjPQNq5SqlKHblxq1aSouhMmhIqiqNlI227qqz58ZUbmJQVIIKydyxlai6QpGZtFQnDcUOwQ0xuhT3gn5b0GEuDN/nZhuXoutI0P/GrJZyXiP13QC6qFbNeWeCPpmto8lWMcXPldLvK1l8N0FQ5cYIblJVblrqAaLqAWtZrfM2hnJTZgRQTmmzbBOLRfVZUtANxTGP8X0VpeCW1BGjbat+MiJ5TN9d/3H6v61bOmUo5hVT+j23KDcwOhUDZrdsHqbclBaRcuOWlqKGYvf3NxxTmb/ugKF69aYwFTvDHz861aE+i+TXaooAKtEN6fgWeOyXulmQ0sL1uKHYlRt7WoozGFf4rT9IttN0GZwJw1AsERUxVUMlp9yEjOAmE31uop3tc0ODG2qyBUBKdAWlN5oBEGYmBpwHZ9LSbcA6I8pOzDYV3c6hI/uACi7VJf649JVbxRRNS9Hb6f0pIXu1FH2t4STBDfVlVA4E3LpdG56bkJH6yoWhOBYxD5a+QAgxDz2ICuWGN/0ymo2UVFmt2ZeKnuC0bWWGYiXVUnCvB5IkcdOuje85nfdlnCxpNLhRnNJSxn4hUGYOZC105catQ3EK1VLUTFzilzF+UBUA4Icthf1+ZAu3MRf5QH6tpgigpeD9Iuv1K1a/rXcjBbjuxHxayq7c2NJSXGl4hdeqTLCzErcOxaxaSkFMJajgPDdBqtxkpBTcCG4ykJai86V8kooKtLmkpXjlxgxu6KBNJ9gZrYsvqHdZAPvtpht6+UopCq2Gsve6YWkpLqDhfTch2NJSTLlJUo2yK0mlFMDSUiGi74xz4blROIVG9gWhsIOoUG4clRvqt6kcaF5nDIxFayMLUpJ5biLc+AXAHCvCfotE/11IhplYM/YrklNwQ7+LxZSWcilP5t9ftzQ2TUnVVgQxvJ+uZP3QWNjVY9kiX+dKASK4yTh0p0RLrgECfDNX/y8/EZwSF9zYlRvzQOsliqWDLcsnu8yWglEtpRuKNVTAPJv2ahH3vhdpYjbxS9NQ7BDc+AIhNBP9oN1HanJMS7kFN22RRMENVW7c10i7Fdfa+80AqGRdiq0BBJ031auEU24c01I25SZZWor1uHExEwMsLRUgeiCx06HJYLZRwvpzR4kX8HigePQgTnJKf/QwHJUbp+CGG6BLg5SkHYpt3bt9trECtGu05Nc/D2IEnR7HtJSxr/KX6eoNoO9TlNxU32UC1w7F3EHYrVEi7XHTtyKAEX3192NNY2vOZrflMzTIzrdKKUAENxlHNXZKQcLtRFa8pP/rlJYq6Q1I3A/QrtxIElcxFWFdigEuWnbpc0PTUh6iQNE0i+cG0Bv55XQquFNwI3uwjegBQG80Oys3fFqqhffcJE9LuQ40BXDuxCG4dPLuuObIkXG3uaWl7E38ALAuxQAQJC6em6RpKVoplVy58WuGcpOD4ZlUuYlK+utXDaVREsoNN/8tiXJDU9PhXcwrk+rgTLoPoAcXMy1lNlYEAGL861ETVEsFOOWGv74ASea5Adx9NzQtVVsRxNCaUngkfczKVq48XKCTr0MzARHcZBym3KhcIFH/X2DrauvQTIpHNmVpID5AAbgRDBGLt4PtNFlaylYKLlPlRkFUIZZqKQColDIV3BiG4gykpQJeD7ZBv9xbsgc3+uOrGmFpptSVm+TqUtAn47qj98TeAyvjbqtyGcFATbyVIfNzc1ZubH1ukio3SUYvAMwf4VX1QCIXwzNVo59NFPr7o8jGQTSLwc3yjbuwsQCGGdoDEACcl4oLWoNV7LcbiOpNPhOlpVSNsN+cY1qKEATAVUsBzFhMp4VbiHCGYtln7m8KObhxCixh9dy59bqhaal+lUEEfTIGV+snEelUTLVGlIRp8mLBtVFlHpB/KypwaLVUQLMGElj+L6B9m/7/clv1C28qtldLAdzwzCgrBwc4udtlcCZLSxHVUG6sa6pAe0ZLwTttKLYpN9sN5aZGanJUbgBTveHPptoT7EzcBmemClVmeOVG1QgbnOmm3ATi0lK0FDxVQ3EC5cZIS3libWz+VncPz6QDMqPGwEyNBTfZaeLX2BLGaY9+gqmzP8/K42cSe+oIgLNy4/HozfwA+Dr0fYSqEVcVjp/sTQN+H995VzF/Ex4jLUUb+clqomopI/CmqakCrphyUxRkj8RM2277vi2c5wYAhvfV35dUB2jGVA2H37sIR92/OGkzxkInX0cvACK4yTisoZFmnFkOmKD/u3SO/q/HB4SqrXfifTf2tBTAKTdha1qKKTdufW5M5cbuuQEyqdxkLrjxe/ngppk17gOsknI4piIcUy1KRaK0FK0+cZ3WngSWluKUm2Yu0OEVNd5QHCBuhuIEwY2mpWUoRrSdqUXdPTxTM5rCxYy0lGYoBI4H0QywaWcHVI1g8678T3s5pkacghsAKNPVWxrcAO5pE77PEz14+/gSZy6w9BrBjWQE12wsAw9fCg4UhanYLS0FmOqt276PT0sBwHDDd5OqqXhnexQNzWFs2NGOtdsKu9NzMkRaqgdBq6UCqvGlHvtrfWcRMQ7kZf3iS3v54MYpLUWVGyXKRjAAvKGYdih2mQpOlLgOxQBQmSHPDX2MTHluttO0FKzKjccjsdRXRNEsKSkgcVqq0wGYQaWhxvCGYlopVR7wWh7XotwYlUzxhuIWPYhxom0roEb0XigVA9wXRR8z1s6Uo+4enknHLESNzsRmcJMd5YYqZeGYlvcGzzjlJhbWP1vAIbjR9wHejq3sKrezfmqo98mmCuHj01KGcqMRCbKhBFNjsdcpLcWXggNc6rRwK4QSKQrJysG3NOvvUT+jsIA3FacCn476rj6FTuQFjGO1VLQdeOZU4P1bzB5sOUAENxmGKTc0uCnrC4w6wdzAnpKi21Cc0lJUuVFtnps4Q7E1uJGMyx5iq5Yy5lpVSm2IZLQUPN1qqfjxC7pyo+9cq6WWuNb1Qa5iim/gByQuBY8l6FCcCk6eG1p6XVlifd95z03AzVAM4j6ckKakyvvHB6w89Aw71o7eJV7LmroLGtwoLLhJkP7IAHxaMBM9mrJJnHpAOxP7SszZchQjLSW3m8GNWzUPa+DHnS1bxgoYyk0EPviN34/H+P75tHB8UMiXggNcr5vCVR1Mz028opCokZ+mEYe0VHrBTXtPCm6cPDcNK4AfPwCWP5d4/5VlRHCTYWjvBB81FAfKgb3PMDdwDG74tFQCz41i9dwEvB59XkCSwZkeYqSlqHJTtRuAzCg3hJjmxkwoN37Zgyai71ztfW4A6/DMbbbqhURN/FifmwTVUolwqpZqcqiUAqzKjZ/YPDfeoPk5uZ0Z0zLwRCkpwExLAegb0l9fdzfy04zSYjpTSvJRhSA7yg3//ue7YTNucCafkpJsQbaRlpLbG9lVbuXg5twkLrjhOxtzwQ39vnuM70pQisYHhfxsKf7fok1LuSs329uiUDQCSQL6GMNzdzeCm22t0ZSU0Z4V3Dio9vXL9X/rxnX7enhEcJNhqHLjp8qNvxwYeogZwCRTbuR0lBsZ0FTAmPQdFyXLpqE4phJTuellBDdSG6JK1w4Q/NlPWk38VMVULoJV7Gq/14NmGMGN1B4X3FAlxyktlbCJH+tQ3EnlxsFQTFNUVSFrKpFXbvx25UaSkpeD71yr/5vITAxYBiL2DeivvbuVG2JTboixJq8WMQd1ZRDe55Ro3EY+EDc4081vA7D9g9S2lcU9iouqag7NNH9v1rSU/pmE4WdKpRzQP5cgnIIbe1qq8A3FUZcOxUDi4ZlUtakpC7AgqCzgxYAq/f1bs1V/T7a2RPDm15stPbco/H5oZQEHN+9+24AftiROTTp6bur/q/9Lx4rkCBHcZBiaJ/crdIdRrgcZ+07VL/cfH3+npMoNNzzTbihWuYNZnOeGKjcqNCWKMsk4m67igpsupqX4s5+0lBveUMtSNXo1QwsNbtAed+ZFpXg9LZW6ctPlaimHDsVs9IJNuSnxy2yn6meeGzMQSdjIb+MXwIf36f/vNybxojwepgjVBPXX3t2N/EhM/wwUOpjRWI8EYqnayRSFFNwkVG7ssC7FW81AxdVzYw7NpDhVS0WIjx2gaVoq5NS4M2I3FBueG7e0aQGQyOjKlBsH1dqekqJQ9eabTU14ZOEaHDpjIS5/bhmeXbI+7jHauf3QluZITsaidJU1jS34f898iStfWJ5wu4hTF+7Nxn1yrNx4k28iSAemEKhccAMAk6cDo08E+o6OvxOv5jh6bsxScF65CfpkoGOnfsHjtTbgotdBT0sFFO4szOidkom0FH//tMy6NCXlK40Lyjpk/XVUSm0I+e3KjTFfKqYxz41f9iCqaglLwc3BmV2rluqIqYgoKgJemQU3vWzBjSRJqC7xo6E5DL9mS0sB7srNpi+BZ0/VDypDfgUc8HskxV8KxNpR4zeUm27ekRJDJVCNgZm03T8Afb6Uz6H6rwvwypnTWXM+Ea/cJCjv57sUyxKiqtkQ1P1x49NSMZVAi3bAAz0tVWZ83yVLWsr2vkU5lRkwlZtC9twkMBQn8tywHje24GZE3zIs/n4rbntzpUWQpMEQjz3o/q6+Gb8cXpPeC8gxm3fpr6vB4fXx0JNj9j7HOoCtq/T/C+WmuFA1Ah8UyJpxkKE7Co8HqN2bVTBZKK/TdyyhaudScF65CfLVUh6gQ2/6hVCvuDy+RzaVG7+in4Up3lK9KzKAigyUglPlxiOBVW6kBPPbVMTdRIObCrQhaAuY+BEMWw3lZmAv/aw0lbRUZ5Wb8qCXvb30AMsmgofiK9zoZHCflqJys3k58Mwp+nWDDwLOftHiqXHFCJp6efW1dPtZohHcUOXG6/MjSmh1UOZNxc3hwglu0lJuaGq6rZG1K3A3FMcfuHkfiWq87xH4zO+7kS4MOKalDIUmYDMUF3BaqrOemy1NNLixnmTSiilCgLrKICYO0/ehTidU9usK0XdDf2et4cRNQSP273jDNwBRdSWSHzOUA4Ryk2EUjaAU3E6dng0lwl8CXPyBPobBKfihwY0ajR+/0E6Dm+q4uxHZVG7oFHDVXwFvqAqArty49dJIFRq5Z6I7MSUilwMqIEsEpZL1zIGfDE4NxYN7l+CnbW1oS1gtlXhwZjI8HgmVIR92tcfQ1B5D3/IgNzQzviKAqjk+J+WGTQY33oOmn4FnTtYvDzoQOOcl8wCTDGO7Kq8CwBvXQTmTtEYURBXN4imiKRCq3PhkD8IIwI/2rAQ3VkNx/lZLEULcPTdO5f00LRVuQqlHQTMSGYqdlBszLaVGO+CD7rlhHjNDQQshyg5IDKbcFFGfm0TVUlw7CTsNLmmpo8bUYtH3W7HXgEpc8MuheHbJenz60/aUgptC9N20GEFNVNWYUu1EnELGm4ntpvluRig3GUbVNJRJxk7dG2Km3qT0GQnUDHe+zW38glc2lZuS+OBGksxS8KCq76jUQCUz8GaiiR+bZZOBSimKJgcRIfr7VmYbGUEPFLxys5vRHr0jYbWUsc5OVksB5rTwRd/r5brMcxNyCG4SKTf2Rn5LZ+npxdqxwDkvm6nMVDCCpkqq3GTRUHzywx9j8t8XWDwFVLnRjNSpT/agw+hWjFjmRyQ0d5jPnc/KjaIRUMtMwCvrp/yJlJtQL1ZFV+PRvxdufVhYKTg9cKsxW1rKqJbiPDf0e6Ibirn3TYmavj0a1BR9nxvrkFGeBluPG0qvUj8e/e1+uOzQ4Qj5ZZQE9Pe+zWHcCd0P9Tb2ASs3F15ww3vbEqk3cYMzqd8mxykpQAQ3GUdRCcqpcpPOQSoRbPyCg6E4gXIDLi0VMoIbLVAJcMpNVO1aMzQ2NNMjAQvuAla9ldodEwQ3fp+MZug741LNevYY4A3FhnIzyAhuEs6W0rqm3ADAeQcNAQDc8+5qrG5o4ZSb+LTUgKoQPNDYIERn5aZZr3b77wv65V9Nc0zTJcQ42y6X9efJ1vBMRdWwprEVzWGFNTkDAMloCqcx5UZCBzGC8WwrN3kc3PCBV8Dn0YNXOm/LSbmRJJaa6msEN27zpdi0cZ8H+O5N4K8DcEDLBwBoWoqWgvvNjtxGujso2QzFvDoTVwpeyJ6brqWl7MqNnRLDC5hIudl3N72X0Y9bWzPSLLU7aeECmtYE8+rijNu0UirHZmJABDcZR+XTUpkKbjjlptQvM2+LxXPjpNwYRl0ZKko0/SyMBCqYclMiReCD0qWKKfqjHedZAyy6G3hpqj4kNBkJghuf7EGz0euGrptCXfktYYV1q92tt75tooOdqTB1Prg55xeDcejIPogqGq58YRkLruyGYgD4/aRhuO2YYeYVbsrN2kV6c7dgFbDHMekvyghuymB4X7I0PJMPHPmzVYl2wzW+o35vdpWbQglu+JRHwOsxzcSlfd1N1jS4kXYBMLud2zFLwWW9WZoawcj2LwHoFUBm7yEfJMkhLeUU3HiDpsrMmvgVcFrKoRcQxZ8guGFpqcpkwY3+XrU7qMU0uBnetwzlQS9iKkm5AWC+wHvbWhIpN/yA0lgY2PqdfkNPV24WL16ME044Af3794ckSXjttdeS3mfRokXYb7/9EAwGMWzYMDz22GPZX2gaKBox01KBssQbpwozFIchSRIzFQe8MtBuVEs5BDceLiVWQfSzQRKstAQUXa2YooFRf48RZGkx4M2rkvc4SaTcyB60GMoNTadRqBS/uUl/j32yhDpjR+QkEVO62sQP0Kug7jl9H/Qu9WNVQws2GfONnDw3NWUB/HY/vvM0t7PkRzAs+5f+/73P6FxlkaEI+bRwVodntnI7cUtwo1qDG+q5AZBx5UbTCFq4nW4kj4MbGoD4vR49wEiUkqIYXYproP82ks2WCvo8QJPe9bhC3WncRwOJmsoNw81QbC8DBwq+zw0hhO3TEik3Udv7G46pLHi2V0vZKWXBjXufm1K/jFF1+m+90EzFqSo3lmqpLd8CmgKU1CQeG9NN5DS4aWtrwz777IOHHnoope3Xrl2LY489Fr/61a+wbNkyXH/99bjiiiswd+7cLK80dTTCp6XSTDG4QVu1G2Xf1OMR9PHVUg5pKY8Z3FRqNJioAjyyruAAqJS6JpnSXhF9pCbzyvUf61PQE5FIufF60EyM4Eaxp6X0r+zPO/X3uHdpgEnECccvsGqprpnc+pQH8LfTxlquq3SoltKf1FAuvCHrPDGq3DT9DKx6U///uLM7t6BuGp7J5935HbrHCG4IF9x0kOwoN61RBXymphCUGzb4NZXgxuhSXGP8lpIZigNemY10KFOM4EYjcV2jATDlMK4UnJmJuROxLPe5iaka5n1Tn7W2BXzw5tzEz7nPDR2YGfLJlqpUJ5jnxkm5Mb6XIb8Xows0uEnZc8OriPXL9Cvr9sm5mRjIcXBzzDHH4I477sCpp56a0vaPPfYYBg8ejPvvvx+jRo3CRRddhAsuuAAzZszI8kpTR1G5Cp9MpaVKjR4JbfrE4LpKfUfVuywAtG/Xb0uQlgKASmKd4yTZfDedhZ5d9sEu4/H1x8V7NwJt293vGLGuhycge9BkNPILKNadAlVuaHBTU+43JeKY6uo3YX1uOlkKznP46H44+xeD2WUnQzEAU7ngU1KAGfSu/1g35PYd7dzcMRXYfKm2rA7P5M/e+B06C268nOcmS8pNk60SLJ+rpeKqdRL1uKEYvW6qyS4AKZSC+zwsaCqlwY2igRipQsUhuAkhYq2WYmXg3L4qy31uXlu2CZc8+xX+/l4K6etOwO/P0jEU8ykpKcnBmXluHHx+1FBc4pcxqk5/X79rKKzgJn3PjSevzMRAgXluPv30Uxx55JGW64466igsXboUsVjupo/yqBpBGVVu7E31OgstEzUmCv/11L1x/5njsN/gXgkNxRKXlqqC/uOSQkYwYQQhXe11Q3cQvWlw84v/B/QdoytK/7nR/Y4JlRuJeW5ofx4K7VBMU0I1ZQF2FqVqxLG8U+MqV7ydbOJn5y/HjcKhI/vg7F8Mdi+DpwZSn61fjV3RG3d258906GNH25hyk40RDJbghvu/R7MqN35LtVRmgxveBwAA4S6ODskmYbuhlSk3CeR6mpYylBu3NCtNeZVJUSC8S79rbAcAoqelDENxzMOnpQxDMWKuaanVDS14dOGPiHhC1tsyzLdG9RA9Qck0fPDm1DXdzXOzhTXwc2ikaiNRWopeV8KlpVZubs77KfY8Fs9NwuCG61CcJzOlKAXV56ahoQH9+vWzXNevXz8oioJt27ahri6+aVAkEkEkYlZ3NDdnN4JWNA3lUoYNxcZOD636UL2hNaUYWmPkyBMYij1cWqraCG48NMXFKzddCG7ojpKebaJiAHDC/cDMI/XU1D6/AYb+Kv6OSTw3tFrKF7MFN4ahmK65piyAEs402BFV4+ZR8WfAnW3iZ6fE78Xs8w9IvJGbcsNXREkysPevO78QLi1FB3Zmo5FfmyW44dNS+nMR4+Dpkz1oz5KhmDcTA50bnLluWxtKAjL6lme2c7KdCPPFpNDAj2IYiumJgtsoDarc1Gjb2HVeEkMF2vX0q/G9UyTuIM2Vgkdj3MGKS0v9bd4qzF/ViCGlQ3EMAKgRfShvhic7/7RNf07755kp+EopJwXGzXPTkGKlFGAqNx0xFapGLA1MaXAT8snYo185PJL+WW5pjiQ1KucLvHLTEnb/nFj6FTGgMX/MxECBKTcA4r6sNBp2kxHvuusuVFZWsr9Bg5IMI+wiWamWsqWlLCRQbryyBzGjW2wvST8L85RU6Tcy5abdUe1IFXr204sYxuayfsCgA4AJ5+uX37nOGO5pI8VqKTlqDUbtMnNNWQBe2cPUE6ccOO9d6Eq1VNrQg7vdKMwrNyOOBMqtAXta+IwgN9bGqra6U7mhnbglGtx4ec9NhpUb28Ew3T43zeEYjn5gMc58fEkml+WIu3KTPLjppe0CAOxyCVLpwbuX0mi5vrfUrPtIbF2jAbDvoEciUKJcY8yoqdxQ5WLNTu43koVeNz8ZwydbshbcuJuJAdOHYz+pcxu9wNi5Xu8LBKA0YJ442r1fHUy58SLok7F7H13BLyTfTeqeG2P/3/ajbiYOVScf+NtNFFRwU1tbi4aGBst1jY2N8Hq96N27t+N9pk+fjqamJva3cePGrK5R4dNSmaqWommpWJs1D66pZpDgqNxIUI2PuBr6TspjKDb2XjedhQU3Gg1uDJXpsBv1AKrxW2DZs/F3TKTceE3PjSfcZLnNrsrUlOk78ESmYj646Uq1VNrEXNJSvHLTWSMxhZXttrPGgfwZ/9J1O3DXO991uc8Gv4PjO0HLtAOzk+dGyXRwY93JpmsobmwOIxzTsHZbGzSXHjKZwqLcqDGgxdhvJdrxGwptuaqfsCRTbqpi1uCmBk16Wop6bjyccsNNkFej3OdCg5tAOVNS1jdFzfYTGfbdhGMqSylnTblJ0J0YcO9zsyVRcLPpK+CBscC/9ZM2XRXSb2q3pW3aDWWMzsVjqakCCW40jViqIxN7boxAu+lb/Yr+4/LCTAwUWHAzceJE/Oc//7Fc995772HChAnw+Zyl00AggIqKCstfNlEtpeAZeq5Aubmz4dWbjl0AjJ00TTdxyB5Agf4DK5H0HZ7Xptx0tUtxTNUgQWOlqGwAYEk1cMh1+v/n32E9A1z/KdC82dgufqCcXzarpZAkuOlTrr8vNAfuNIKBT0t1r3LjkpYq7QtU7w70GQXscXTXnsPv4LkxzvgJIbjm5f/i8UU/4f3vtnTpaXi1hu/t4aXKjaEMZNNzYz8YpqvcWHr1JOhmnQksTeRaGwEQvXrR4fvOME4MgmobAoiy2WVuj10RtX6mNVITYhqBxIaZcsqN7INm7AtUPl3IPDdlzLD98872rPW6Wb+9nXWJaOqIZcWHkqiBHwD4XQzFtDmlY+ro5y/0f1e9CWz6EpIkufpuOjjPDYC8LQdf09iKWR+tjdv/t0QUSyePRMoNPTGu2GEEN3nitwFyHNy0trZi+fLlWL58OQC91Hv58uXYsGEDAF11Offcc9n2l1xyCdavX49p06bhu+++w6xZszBz5kxcc801uVi+I7pyk+FqKa57qTW4MVJSgQrHvLhHklhwQ5FLMuu5iaoEVWiFF8YPnKpMALD/RUD1MKCtEfjofv26neuBF3+rD1cbfTLQa7e4x/R7Tc+NPbix77D6GCMRQqxjqHtaSvZISasgMgpLS9mUG9kLXP4F8PuFZvfpzuIzD0JVNs/N6i0tWLddX8OmLpo3+bM3/v+sAzPnuQmzDsXZ8dyUGSmBDvuMpCS0RZ19Q9nAoty0G7/Zkt7WlgB2gpWAMcaiBk2u6UX62GURm3IjNSGmaCy4sSg3koSYcZlEuM/FCF40Xwkzjm7c0ZG1Xjdrt5mPp2jE0ZDbVVzTUpoKfDMXlYYyZg9uthnjXOgJkwWaVgSAxXp1Lg1e7IFyuy242aOf/l6u255fHZ/vfuc73PbmSnxgO/Gxe2wSGoqN72Lpjm/0K+r2yewiu0BOg5ulS5di/PjxGD9eL4OdNm0axo8fj5tuugkAUF9fzwIdABg6dCjefvttLFy4EOPGjcPtt9+OBx98EKeddlpO1u+ERbnJVLUUwPluuB0a89vEqzaAfjC3BzcsDZQh5SaqaGaPm1C19WDt9QNH3K7//9OHgMZVwPO/0Xf2tWOBkx9xfEzec5NMualhyo17WooNzcxQpVTKuCk3gD4gtTNN++wYvVHQugXVRnBDZ17N+8ZM4VI/QWfhAxq+/JUqNx6fGdxku1qKVrOE0zww8utujWS3utKiHtATkkSqDWCcxOjKZx+pyT0tZTx2adj4fI2UU43UrH/XVeswU4pqqL+0mgoAC27CHjMAr2/qAGEjGDLruaFmYko2UlNxIwEoK/8P+PcFOGTtPwAAUcWqGlF1stypxw0f3Kx+G2hYwXw3fIBGCGHpUnrCRWfS7WjN3ty3zrDVWM/mJuu+gaZ/K9EKL5Qks6V05T6w83v9itq9s7PYTpDTaqnJkycnlCXnzJkTd90hhxyCr776Kour6hqKqnGemwwpN0BcOTgArlLK2W/k5Tw3AKASCTLdaVHlRmrDji56bvoY7eJZSopnz+OA3Q4G1n8EPHmofjZf2hf4zfOuk695zw0tdaXQ8QuUGpty45SWojN6MtHjJi2YcuMQ3GSKqiH6vy31qA7or5028ctkcNPm0ueGKjceHx2/kMU+N1z32B+3tqVdCs6vO1FL+UzAetHwwU1pkuAG0H/nTRtRIzXhBxflhqbjQh3G51s3Ftj4meG5IVzXaKsqqHj0AJTwn4uhzHTADLQ1AkTlUv1TzLDn5qet1sdrDsfQH5n9fbDGcrZ9BTZ+DgAoj+hKhV25oWoeTTdZF6o3S0RJb7232If3IuS70LgfP8xVYykd2n+rt+EL3GbMfetW9TgB1Ctk74vVEo6hH3ZgUeAqfKaNwozIXa6PEVFU1KBJ73cleYCqwa7bdjcF5bkpBDSCzHtuALMcnA9uqHLjYCYG9LRUjItfW6RSUxanyk0X01IxRUMfo108S53xSBJw1J0AJP1gLwf0wCZB1YiP99xEmi3VVvzZmOyRUGU00KM7EqfJ4Gz0Qnf6bQB3Q3EmKalmqakaTf9u7GyLYt22NqxqMM+6tzRlTrnhd+Y+Gtz49QNUNjsUN9ta46dbCs5vn/W0lMJN7m5PI7gxfkN9pF2uzRiZMtFer19hNICskXRDsYfN+7Iqg/SyxH8uRvDSCuu27fRyxtNSNuXGRZ3qCvT9ietxYwx1DBgjXfjgRtVMxYWmk6wLNYKbKTfr/377GvaQ9ev47xWfFqfjUHqX6sF+VNESmnO7G/o73m77njWHFezn+R5BKYb9PavR6lIKTojeV2ygZHy/y/tnvG1AVxDBTYZRNC3z1VKAczl4otEL0A/+KjE/4lZwSkkGm/glVG4A3UG//4W6ofKkh4GBExI+pt9rzpYCYHYzhtnnBgB6l/rhMVJNLP/tcNCiXZS7tVIKSJyWyhSSxM6WqiL6wU7RCOZ+pcvotGtxJtNS/HtMgxuJ99ykq9xoGrBhSVKVwD73J91qKV7Vy3ZayqrcGCckvB/NjTJzvlRzWGGBufWxVZSjHXLMCDwMEycLblTDUGwLblRuRh3DSEu1EOu2rfRyhg3FNLihB/5spKXYXCk+ha1pQMPXAAC/Eh/c8EEJX+YNAFAVoMUogNjjKGDkcQAIzmh/GYD1e0VTVAGvh/W+Cflltn/KRg+qzkLX7aTcjJD0wK1EisAb3uF4/5hKQAgwgAY3VflRAk4RwU2GUbUsp6VaHTw3bsqNzXPTKnHBDWcojnRlKrhKTM+Nk3JDOebvwLVrgbFnJH1MvywhCh8itAkZ57vhPTc0JQVYm2rZUTQ6eqG7lRsXQ3GmMUzZ/paN7KDx0lK95cGZ++s7nMbmSJcqUxzHL6gKM5LLfm62lOG5IakqN9++Asw6Sh/ZkQAzuDE8N2kaivmS3WynpSzKTaqeGyCuS7HTwT8c01AnGaNNglXs8++NZsRUYo7E8FjTUppsBNl8cGMoM82q1Z/TRC9nsM/NrvYoO7jvNUBXtbPrueEObzt+YoGazwhu+CZ+NCiRPVK8Ebm1ASAa4PHpn88kvYDlwPb5GCxtsQRGbuoPrWTclke+G7pu+yy65o4Yhns2scu9opvgBK2UMoOb/ElJASK4yTiyFoVXMna62fbc0LlSbsqNrVqqTYpXbsqkMGLRCJxoao/hicU/ss6dTuiG4l36BTflBtDTYcHU0nS0yVa7x1C+OnaZy+aDm3I+uDFKwR1kX6bc5CwtlUXlBjB3Krs2sJ0oLWv93YH6gS+qal06a+TVGmagVM3vjWykpfyyBx20WiqaonKzxSgj/fGDhJs1GwEJVW7SLgW3pKWyHdxwB1j6Oy119sZZMH5DdV5drXQyFYdjKuok48SmcqAlIIqpGjyGyVvzWgMW2kXaw/cfMg74TbbgZodiBEYZ9NxQM3FtRRC1xny87AQ3DqXgdDQAAJ/SBg80RJX470OJX473xFAzcUWdvh8bsC+w+xTI0HCq/KHjb6PE5tvpTU3FeaLcRBSV7Rfta2oJKxgumQFNP3VLnD8JML1NAyXjmJQnzfsoIrjJIJpGUEa4HYfP2TDbKWhVjFNaykW50Q3FXHDj4YItrnmeFHbuv/DsZ+vx17dX4YnFP7kuK6Zq5tDM8lr39adBlTFlO+I11sspN/wOizbwA7hBdo5N/AzlptvTUt2k3FQZ5fQ717M0FADsO7gKA3uVsPfJKTW1dltbSgcYx0F6Chfc0Gopr5S+ckPNmjvXAS3u/Xi6mpZqT7ExWSYIM1OrnGZaSt+mn0f/Tdp73VCfA1NuKgaw+5RJYXiUDshsmKmtGo8GN2p8WooGM3Qa9taIz3J7JlhrmImH9SlFZUh/nuYsKGiO1VKG34ZShnZ2cAfM/YajmZh1l+YO3sMO0f+R6i0+P/odu1J7Gph1DPDcWcCrl+CyyEyMktZje6vziWRn+X5LC/75yTrH9GUi+MrBuLRURweGSfXs8iBpq+PJAH2fB3mM72KepaUKarZUvqNoBGWSvkMn/jJImTyYOio3RuM8l1JwPS1lrqHdwwVbshdhTwmCWjs8kV2O9/+xUd+xbUvwg9Q9NymkpdLg+H3q0BpR0OvbPkD9Ote0VB8uLUXz5I4dirWep9wAwNF76cFmv4ogtrVGsaU5jDHkR0CNAoMPxE9bW3HEfYux3+BeeOmSiQmfgt+5RRUNMVWDz0hvxIgMn09/Xj0tlabnhjZ0BICNS4DRJ8VtEo6pzEtB01Kqpg+KTLUKjj/DTtS7IxNYlJtOpKXob8qu3NDHZcFN5QAgUAFN9sOjRlGm7GQjMegwUwoxgmwPp7hRZWZHzA8ggr0GVOKTH7ejIWwcGjJoKP7J6HEztKYUlUYhgH2kRiaI8FPTKbbgpkLqsKgRTLkJOJiJafBdwQ09rd4dALCbtAX/5fY5HVEVddiOX0dfBcwuJjgSQD/fMnzUZh383FVufeNbfLxmOwZXl+DQPVPf//LB/c72KDSNMP+it2kjApJ5+0BpK1rCCuujRaHfxQEekZYqevS5UvoOn2Syxw1gBjft28zqoSTKjb3PTYdsXVPYUEZkl+Bm4049UEt0lhtNxVCcJiV+Ly44eCgCZUbQxgc3FuXG3HlTr4lT51mzz00RGooBsxHirg1seCYAHDVGD27oIMDt27YBc44H/nki0LYNyzbsgqoRfL5uB9Y0uh/E+EoSSntEZd6NCHysMsXrkRAxlBsp1fELfA+RDZ85bkIPgh7J+rmno960W5r4daNyw9JSqVRL6b8hOl/KflZND9z9wSk3kgQlqD92pboDXs3YB9mUG2r6lunnoqlMXdwa1YONvQboiu6WiBHcZLDPDTUTD60pRUVQf75uSUsREhfclKPdUkhB9xtldjMx4DwXrHoYAGCo1IB2zpzeHlWxu8cI1isGAsffD0y+HgAwRlqH5iZr366usmGH/vmlWzDAK9wasX4OFa0/WrYdKG11PAbo7zPBANC0lAhuipasTASn0F42RAM6DMUmwdBMIL5DcbvHuqaIV/fAyFHnH9zGHfprSRTcaEoU1cZQzkwFNwzDF8T3uvHKZhVCTbl5IC8NJJ8tVbSGYnrG1NaIvkH99e9ZW47deutKXT+jnXzZ+v/o88nUCLDpS0vH1De/3gw3+M+f9kFsiyosLRWBDz6vfoMkSayfiqSE9SqVRBASr9w4QHe+FSEfAl4PW0c6jfz4HXqixmSZgJ7VlkiKWe2XUnCjn8SESDuCiMR1KaYH7v40FWAccFVDFapWubS1LbiRA/r3UKNBN5dyagzrwcZuvUtQ6pfRRoyAPJOeGyMttXufMqbcdEsTv13r9X2Ix8cOwOVotyk3KZSBV/LKzVAA+uBhth+Gvv8ZSlM6dfvoA4QnX4fWQF94JQ2lO77OwCvUIYSg0fDWpfs+2k8CeVNxrzbdhtAe0k+OBkmNzsFNTEMVWlFCO/InGgqbA0Rwk0H4ieBSJnvcAHr/ABrEtG3VDwopKDcqMX+sYZtyQ4MbbyTecxOOqexsINFZbiCi72RVyWsGI5mC+oLsXYqNMzKLcsNmSzn0udFon5siVW6CVayn0sF99Oc8d+IQdjNVbgY3cHPZNn3FRjMAwBv/3exaTUU/f7/sQYVxUGqPKhblhk8NWUqQk6k37dstxmTU/xeIxnt1WHAT9EGSJJae7Kxy012em3JifHc9Kf4+AhUsKKlx6FJMK8T6U0NxRX8AgFaiB0V9NbOaUrIZioMl+u+fxDr0waE0cPF4scM4PlWF/BhUXYK2DPe50TRiUW6yG9zYlBuq2vQbzUzd5ZLdc6N/H5wb+FFDMXfw9oXQFtRP5srbzWHM7VHF9Kv03p1d31StjyXo15S54KY5rLBAblea/YLs+3TeVNw3sl6/rv9kAHo1VGt7vDUhomhmpVRZv8x0XM8gIrjJIFmZCM7D+26ibbp3AnBVbrweCTFOuYl6rcpN1KcfEP2x+OBm064O1GE7/uF7BH071rgvKaoHN5FAkrk5ncEluKG53wFVZtBQ4nNXbsw+N0Wq3EgSMxUf0rcdX99yJM7+hSkR11YEUYoOjGjhUj6bv8I6rqHaj1utTf94aCBQGpDZzr81oprKDfFZGqapvNcjme/G8DNsJZXYKfcGNAXYHN+BnI5eoAdFmoZMpxzc4rnpJuWmXNmlX1FSk9q0ZEkyfTdoijMUh41UQC1vKAZAjOCmH9FTBBqRINk8N8HQ/2/vvePbqNL18WdGvbh3xyVxeg9xgBCSAKHDUhaWdoHQ2QABEi67dBb4wcJ+7y4E7qUsveyywBI6LBAIJQECIQXSC+mJHfcqWfX8/jhzZs5II1my5Vg45/l8/LE9Gkmjo5kzz3ne531fquTZiA8t3gDXNNOFFm58y3I4cpMiQ3FNWxd8wTDMsoSyHEffkptIzw0jNyUT1UVABiI9N4pyk2hYCoDHTa+57C7NXOMNhDFUUpTI/OHq9q7iagBAhWddDz6RMerbtVBUqze5LKzIemA8uSkNUHLjLZuBIMywSiEEWqOVXX8wnLaZUoAgNylFOEzUsJSU6rAUoK91w1Qbky1mGwNZ0rdf6DLpjylgpRe6xYDc7G7y4Hrz2zjLtBSndX0Y+5ACCrmxJ5AJkixikJu/nTsR/+/sCagq0AgkMwIat19gdW4GqHIDqKEpqWWX6mdgKMqyY5a8ClYE1D5EZO9K7FAMnkML6Pnz/k/GoSmN3JjV8J/Hxys3Vt3YmswW+IhyDN2RG0Xy30fysM40mm7bFR2aYjdBdlPsrXLT113BGelyBZUQciIhKQblJjpO3h4VluoKhJCNDjigbGcmVyWcVayQGxoq1J/vLF3fjgBNEmDExZqhjm+204KyHAc6WRG/FNW52VZP36sizwkzpwD2iaE4skIxT26UchRUuYk2FLsjDcUBr+aZ4sNSAHyZgwEAuT4tbdrrD6JKZsqNRm5QfhgAYGRgA5CiTugsJAX0ICzlC+IwaQPuM78AF7yatyscRkWYkjlTyVg0mSnRllp2Rr2GLxjSqhOnWaYUIMhNShHkDMUpbb3AwFcp5gv4xVgRRhqKfRb9MQUtlDzYgsbkZpZpFX2LUGwTnFshN357EpN3olAKDfJ1bgBgalUezlWK08HfCXx6J8p2vEV3jafcDMT2CwyqqTh6EirOtONkE+2rwypFS54GZPlpb6Jrjh4GAPjg5xrD0JQ28ZvV+h0dvqDagNEHi25srck0z1SUm1qSix9Cys1gd7SpmDXzy1RSiFml6mRaMHQeQM8Nu2lnhJVrJ0b/N0OM/g0A4LempYbZUqVMtXHma6EARe1h5s7I7wSAeh7aJT8a2jlyY3OrYY0shwXluU50ILWeGxaSqsp3q+8DHIAKxYQA+1bTB0omqQumaEOxcX0a1W9jcUWFFYNZgwEABVyRO39Xp2b25pQbZ+Vk+IkJuWgDad7R48/Go65dIzfJhqU8/iBusbyG2eZFuNi0SGvB0LYHDvjgJybYCoehxVoCADC37Y56DV1YKs0ypQBBblIKXUfwvghLsVTrzvpuWy8AgEmGjtwEIsJSIYWAWYPRqzP/ntUoluiq0006DYs4AUBGkB5HwHHglBsVHXU0++fb/0X5N7ehAC3GnptUtV/Y9DHwxYPdm2QZDkTjTAYuHTwSxfYQjpFXAwB8Y34HFI0FAEyQfkFplh2njC+Gw2LCriYPft4TPdaMCMzz/R23dTwIGWF4/CEE/caeG4uJb57ZTa0bhdzUkFx83aV4FHZ/HzXGkcoNa5SaTPNMvkJxX3tumMyfxchNIjVuGMb9DkSSMVneCmf7Dt1DXn9InwauQFbmhlLlZuODJfp8V4iQHX7Ud/hU4hK2OFW1I8tpQXmOQ+stlaKw1Dauxg0AVbnxBcNJF2PsDjrPTXsNzTCVTPS8t9E5JVPyqhV2Ad5zE6HctHEhqYhFJFHSwUtCGrlxduyELBF0mTN1hDY3KxPrCDUhe3/5LgWfEtjPZUglS246uoIYIdHPdpJpuarc+GvWAwC2kxJkOu1os1NPl63DiNyERFjqYIHec9OHYanO+m5bLwAsLMWRG0skuckGADhC0eSmoOZL9e9MyRPTVJypkJug8wCTm/pNwLPHqv4MKRzEOaavjIv48e0X9q0C1ryZ/LHsWwW8fhHw1UPAlk+73z8UoP4R4ACRGy0dPBKZe7+EQ/JjZ7gQtY7hwCAa/58gb8PgfBecVjOOHU1vjkZZUx2+ICqlWpzk/QCHe5dgqLQPHb4gQkoF4kjPjb55ZnfKDX2/GpKHtaEKELOTft8Nm3S78dlSAOe5SVC5IYTAw91E+5LceP0hNVzWo7BURhE6ymYCAKZ16qs217Z1adWJOYOrnEGvvyylzlYXsUZnByohSQd8tA2A4rkJmCnhkCXAbTWjLMep9ZYKdtHeSr3ENs5MDAAZNrPKFVIdmtLVGGIhqYKR9DpkYamIIn4xPTdGmVIK5DyaDj6IaAXvMjt30Kc5K3VkyGY2Ya00AgAQiFHuIFnwyk2yCpipY69qoZgk/4JwCyU6vtoNAICtZBBcVjM6XfQcc3qiWzD4AkK5OWjQZ32lGNSwVL2WDh6jgB9AlQq+iB/z2DAQhTw4DJSbkW3fqH9nwBPzZpAVoscRcqamgJ8OscjNzm+B546nN/KcIcDMPwIAzjctRiAY3WxQC0vJwFtXAwuvAGrXJn4cvnbgzcuBsDKBbPuy++fwisWBCEuxyaU5OiwlrX8HAPCf8OGobfMBpZMBABMlSm4A4LSJdIX2wc81NJOGQ4cviONkzeTL+umwlGKq3GgTub55ZjfKTaum3ARhRmfhJLo9wnfTxmVLAcl7broCYZ3VoSsQjlIjuwIhfL+tMelqr5Fo7KQ3HatJhtXHFiHJhW2DY88FAJwU/gqEU7H2NHsNlRtThr4MQ6SaBkAl2TaJeW7odR+Q6fmZ5bBAliWU5TrQCY6Qp6DWzXbF31WlnG+yLPVZrRtdKjjvtwE0Q7HkQShMEFLO9U7OV6aD2nohmtyY8ym5yUKnutjM8dLrr8M9JGr/X+xj6PP2/diTjxWF3pAbd+sW3f9VjV8AAEgdXVTsNpVDliX43JTcZHgNyI0ISx08CPJhqVQX8QMilBsWd4+j3MhAUClC7SNmdeXGQBTlxhmOmLw66jAyuFn9N1OKTW6yw5TcEFdfkpsW/fa3f08JT9lhwJWfATNuArFnoUKuxwx5jW6FDvDtFyRtJVa7JvHj+OiPtPGerJhkEyI3ynkgyYDJGn/fVIBNLt4mvQnU7wE2U6Xpo9BhNL1/ECU34+TtGJJLSchRIwqQYTOjprULK3Y16166M4LcVEp16PSFEFLCUn5Y1NpDAGAxJ++5qSFUwm/ImUS3R/huYhmKE82WMgpXRqqR/7t4C857ehneWmncKDBRNCrNEXNdVkjJFPDjYB9/OjqIHRVSHbzbvlW372nycMqNdsM1Z0aTm6jSBwq5KZfqcOjWx4DV/wIAdMl0OxvbTLsFTodDU9+4Oi49gS8Ywp5meh4MKdCSH9QqxV2pJjesgKIcTW445QbQCnx2dhuWig67OF2ZqCF0/g020IzSPCVzyptVFbX/Pvc4AICjaUNKvEx1XFiqwxeMaR0wQlY7Pd6Acn+Y1P41AEBupORmn4UqwcFMOq/k+GsiXwKkqw3ZkvI5RFhqYCMY4sNSfWEo5jw33RTwAxRDMaFfcRucUdkTzLDrDOvj6p51/wEA1BKqCmXCg84YE1AOUchNilov6MAMfAEPEFQMb52NWujlojfpTcPiACacDwD4L9PiKJMpa79gk4K0iB0A1G9M7BjWvAn89ColKee8CEAC6jcA7bXxn8engSeSAtxb2DM1FY8PTf3yORDoRJO5CD+TKhqnzx+JLtiQIXkxxkrrotgtJhw/lt4gF63X93cKdjbhUFkbr3KpDp2+IMKKoTggWXXNBq0mKbGwFFfArwb0PN7lnEAfi1Buojw3SSo3Hq5IG6t/EknYf6mj58a2ht7deJjfJs9t5fpKJUduHK4MfEpohk149Wvq9j3NXs1QzKUmS85chIj2HdAMtojzTlGTS6UmzGr4J7CHmszbbLRYWxZX3bo814FdRLmmm7cndeyR2NXoASHUkM63TGHm8JQrNwGDsFSUckPPS0YIYjW8jBeWctpM2EnoNeOvo2ShyE+9KYHsaHJDMstQS3Igk5Bmcu4F6jnlBkguvJfvpVWIl2edCAAYE1wHdNTB3kwVnXoHVZ6IQlqyg/XaHKzA1knHxmPK7BuPaS8hyE0KQQ3FLFuqL8NSDd0W8ANoV3DmuWkjriiZWlLIjTuC3AQ2UHLzqTwDAGCRQujsNJamc0kLgD4iNzxBZIUGmRcjq0Lf/HPK5QCA4+QV6GriyvlDm8AywN20GjajWzTvAD6YT/+e+QeaxVKi3Hy3fRX/uQcyDZzByFS87h0AwJb8WQAk1Lb6QGSTam4c6tfGYYJSen9Psz6UNKh+idbpHkCFVIdOfwjhAJ1cA5JembKYZDRC+W5+WRz7eLkCfvuVFfBGyygAEr2hdmgF6ViDxSjPTYLkhq3MnVYzMuxaxhcPlnadbM2QSLDMk1yXVesrlYyhWMFi6ywAgH3Le2pNod3NHpTwrRcYZBOaoV0vPmJgKC47DLuHXYT3Q1Pxnv10YNadwJlPYXUlvXYYcQSA8hwndhClEW5j7Ma5iYC1CKjIdepIcF9lTLGwlCvYoiiDElA8nj4YpdxEhqUilJsYNW4AGnbcpYxRqGEbQAhKg3T/YM6wqP3z3DasDCsZVAqx7A32R7RcaEliHAu7KGHdXzQTP4WrIIMAPz4Pc6AdISKh1UnnEktmMbzESh9v08+rdoXcsIyqdIMgNynEASvi5+/QVhRxlBu+cWYbXFGpoYzcuODR+lUFfXDtoTfun7NmqXVy/B0t0W/g00pvy6luvQAAJjNgjegMzhSXgpH6fQtHYbU0GmYpDPuaf+keYtlSmeBIXL3esGqIL/5MSVX5VNXXg6qj6e/uQlP9Qm4iTMXeZmDjBwCA/eWn0N9tXWjs9GNViJKb/DatqFh+Bl1VN7Trb+7Dm5fQ52bSG0SFotwQpfpw0IDcvBA8if7z07+AmhhVWZUbRz3JUuXxPV4LUEi9Cbx60xYVlqLnZaLkRs2GsZlUX0VkOjgjN8lmnkSiSfHc5LmsXPg4+VIJ21yTUENyYfa1Als+hS8Ywv42D4pZWCpCTWiUstW/u2CNTgU3mdF01AO4PnADHiSXUsI+6QI0BeiYZnPkpizHge2M3DTpew0li90KuSnP1V8LKrnp5XhHgoWlMluVazx3iLbYVBZMmVKMsBTvuSGEa5oZTW4kScJek3Jjb/oF6KiDCx5aQDE3WrnJc1s1crN7eY8/H0DJGEtfz1Oa5SZ83obDKPXTOcJcMhafhA4FAJBlTwIAdpIiOJWCj26HBXuIct+J8PM5vTRU1W4v7vkH6UMIcpNC6FPB+0C5sWXQon2ApmDEUW7MsqR6btqIU5fRAgCyMxsB1p7hmwX0Yt75DcxBD+pINvyFE+CRKUkLdBrE3Tto+KKD2GFx9MHnBTR1htW6YaQkktwA+NBKb6jZG1/VyBqAgGLIdPPeoubtQCBOszm/B9hAiQFOuJ8SLUBPbuIV4zpQ1Yl5RJqKf/43zXYpHAtz+RQANNtmR0MnfgrTNFZz7Sr16aydha4LfNCHkR10lbmlajYAGpby+vwI+xXlRo4mN6vJMOwsORkAAT69w3islJDUPqKlzNa1+YAyms3F+6KiPDfW2BWpjaD1DjKrzRGjlZuA8js1yk2BQ+L6SiVR50ZBlsuBd0NH0n9+fB71O9ajkDTDJgVBIAEZ+hVzs6QpmZEmbwaVwHb41JpGkcQRAMpzeeWmd+SG+W3KcvTXgqbcpDZzjSk3bkZulNIHALg6N/SYWK0bFrbUtV/oatFS4ZU2F5GoM1OCaWreDjTSkM4ekg+7I/q6z3VFKDeE0J8N7wNLHqYZlgmCmYmdVhNKlUrtCSuOLTtggw9dxAJX0VB8Sg4HAEiKt3ErGYRMRd3MsFmwm5GbiBpabsVk3GEXys2AB99bqk88N5Kk1bphK8KElRtnlExttdrweOgM+s/n9wEf/jew8SMAwOLQJJTlutR+VEGPQTq2EjaoJ1l9V/2XFfJjpmKV3IyK2nW5YzpaiAu2zn3AVi2Flik3Lj78RsLxV6Sb/0P9OTmDgbIp2vaKIyjBbN8HNGyJ+fT+VW520klz5Uv0/+pLUJRFj6O2tQvbGzrxE1FqytSuUWPpjNzU8+RmxxI4iAf7STaaK09AWDLBLgVg8TaAKBWKQ5K+zL9VaaK5Yvj1dKy2f22cPs8V8GPY394FFCiViutpWmowFFaJCJt0k/bc+NnNy2RIbgghaq2P3io3zFA8yKb1bupJ37UclwVvhabTf35ZjLJ/TMe3thsAAJK7iPab49DCKTeGdW6grfIDIaISxhauOjGDLizVW+VGCXOW5+ivhcy+MhQrnhtHk6LyFo3THlTmZbfkhYxwlHKja5zJ1HFnHmA1XqTUW6miY2nboc4H20ipYQPOfLcV68hguuDsrAdWvQI8fTQtMfH5vd2HujkwM3FRpl393hIO79Vp6d5uhx2trkpsCmvK1FYySP1uMuxmjdxEKDcZPuo79DiMiV9/Q5CbFCIUCsHNKhT3RbYUEG1MjFP5NMpzY9av5GxmGQuCv8NfcCkACfjxOWD5MwCAxeFDUJHrhE8p/Bf2tES9PlGUm3pk9x25iUwHj0NuLHYnFoZofRCs+be6nWVLuUhEQbJ4oSlWC2fc2XpDsMUBVEylf8cLTfWHcsNXKd67Eti/ljZhnHAuipXO4HXtlNzsIoXUCBjyA3U0NMXMnu1dQS3cs4n6rz4PTYbL4YDPSVdpGV171PYLQQPlBgCaraXA1Dl046d3RddLadNaLzDUtfmAQuW7rduoHg9DZg/bL7CwlNPGKTfc63b4gqrxvLceEGYoLjEpSmGifaUikOO0YjMpx7KyK4DCsQjKdsiSooApGW88WuRs9W8fMVZu7BaT6jliCl2kKgYoYakwJTekeWdSqkIkmHJTnqu/FvoqFZwV57MxcsPCnIDquQEAN2ghv1CYaO0y+LCUGpKKNhMztNgVcuNrBvbQFO9tpCTamAzqwfLBiq0mJWT13vVAzWpth9boGlWxwJSbggyb+r0lTMoVcrOZlMFpNSHPZcXH4cPUh7eEB6nniNtuxm7FWE4iyE2WQm68rvTqBs4gyE0KQfwd2uTTF2EpINqYGM9QLEv4MjQRO8JF+DQ8BZaIlRy7Cb0YOhk49yW1G7EfZiwNj0d5rlMt/EcMCumFlYyhepIVFfJKGXhy09VKFRMAKBgRtavDatZk3zatGF1AuWk5I4sVxjIVe5uBLUoH7XG/i348Ed9NfxuKV75I/x5zBuDIQWGGDZJEV+yrdrUAkNCUpcj1e1cAoNkr7Hts7PRT9UchN4vC1XBZzfBl0PfI9e3VlJsY5CYQCgMz/puqiw2bNCWJoVVTblgmeX27D4QR16ZtQNCnruydVpP62ky58SWcCs4pNwaG4uZO7cbQa+VGITf5snK+JZkpxZCjZC99VHA5cO23ePjQLzC163/xzIi/A2c9HbV/q6zVvOqK6PfFo0AJTdW365WqTB25cWI/cuAhNkgkZFgcMlEwz03ssFTqyE1QISsmhGBuNAhLmW1qaD8DHgSCRFcmQGcoblWq8hqYiRlkmwv7STYAgGylc8Y2UqJW0OaR56Lv+11YOR6TFZh6Lb1GAaB9f9RzYoGZiQt7Q27CZXDbzMhxWvGfEEduCEdubGbVcxOOaBuRE6Dzv98tlJuBDyU+G4Lcdzc1XT0ZSZcxFAlZkvADGY2j/Y/gq/DEqJWcVUmJ9YfC9AKb/R5IzmD8M3w8PLCjPMeJoNKPSvIZkJs2Rbkh2VGqUMrA17qpV8hIRqnh53ZZTWiCQipZ2A6acqORG3YnjZEOvuF9WrCvcCxQNCb6cUZudiyJXb31QLZeYGDkpquV+m0AYPIl9DBMsjq5sjo2/qJJdJ+91HcjSRJNXwZo76Han4G2vfDChm/DY+G2mxHOoupQnr9GzeAJyfqwlEpugmH6PR19G33giz9TLxMDV52Yrer9oTBaTXn0eSQENGwxVBaSTwWn35MjRliK99l4A6FetQRghuJcifltekZuWLiBeYF2t/hQizxa38lg8dRhylb/NqxzoyDSW6U2zeTH12rC4DyXmurcU99NqzegZrqVRYSl+oLcML/NYKkWUrCLKqc5EQX11OaZVLlhfhuzLOkXaWoaeGxy47Ka1PCdpCjZv5BS9fzkka9cW3/znY7wGU8A168ETnpQC8N2dFNeggNLAy/MSD4sReo55cZmQq7bio2kHDsLjsEW21hsIhWqqmYzy6iRlHsOT3ADXmQqBVwDbqHcDHhISvE0r9SHtU34idKRDcjRFxGDWdYfQ+Rkxy5ktVJnxeGou+x73Ou/GCZZQkm2HWElW0lmxkgOpF0jN32n3GTT312tsTOlFDisJjQTI3JDlRtHSPkMbCVXH0O5YSGp8WcbP14ykR6Xr422ZTACU27MB5DcWF1aVk7QS7sSV05THy7Oojc1ZqK0D6ZZEtj4gWre1d341r8HAPiGTIAPVrhtZpBceqMoCtdAYuTGFElu6HmnFhWbchkt8uVp0KeGc32l8t3aCrSuw8/5bjYakhtbko0zNeXGOCzVFGEi7k1LAOa5yWQNZ3uQKQVoyk2LcmwsRT+SJDC0mTXlxgcLLVppgIIY5IYfXwA4fEherzOm2DHnuaxR1X/VIn59QG5GSorqUjiGVjPlwWrdgHYG5/02fKp6ImEpp82MHWF9ttAulKoLRx45it+pI2xD64hztE7arLp0EsoNC0sVZtqQ7aCvmxC5CQVVb9BmUg6X1YxcpxWAhIXD/4I7cv6GAMzIUMiNJEloVlK9TZ56bXGiZDq2EwekOFXy+xOC3KQQkqLceOU+9FnwYak4ZmKAGop5RBIQ/gJkNzwmIZdk2ekKXFFIzH4DctNJDcUNyNJVqE0p+LCUSm6i/TYAvXE18eRGyZJiYSm1zUQ5zQ5A41ZdVhUAWpxvO63WiXExyI1sAoYo3p5Yoan+CEsBmu8GACbP1pHs4ky7+rckAbkTTgZKD6Gq2EunATU/K6tLgoLV/wcs+RsA4KMAzV5y28ww5VFyUxreT1fGAMIm47CUn/XuMVmAUbTTNQtz6Qv45cFtM6NQCZfofTcbtI7g9mjlJtHGmUy5cdqMlZuWCHKTTM0QHl2BkGpedoda6MaehqVcTLmhx7a7ydi7wtBh1uYDH7HGUW4UdS5SuXHqv8epQ3N7nTHFjtmIkPUNuaFjP9akkBsj5VVVbii5UTOlYrVeSFC5AYBOYkOb1fj7tphk9TOzFh0AALfy/CSUm7p2Zijmw1IJZEs1bYMU8qOT2LAPeXBYTCrpavL41RAwK7AIAMSRjTainHNMvVEyp/aSfNp9PQ0hyE0KwchNl9SHNzSe3MTx2zDwpCOy7gVPbpgMzwpulbP4OCM3Bv2nmAzbLOfoVzyphI7cxE4DB+jKqwWKkZuEACWUxsJSdraSLh5PFZWQjxbq47HubQCESv85g2MfV3e+m/4wFANaaEq2AJP+S/dQEUduSrMcNF314ndoI01vM/DSaThMWo+nLAswYdNjAAj8Ey/GO2GaseOymWHJp+SmXKoDUZSbcJRyw3luGEaeTH9v/pgSSl0Bvxxk2M0ozFTIDZ8xVbchqmkmoHUF75FyY+C5aerU32B76rthfhuLSdL6SvU4LEVvOs2dAXj9IZWMxFJuOizafNAFS3SdGwWqOtfuByEkIeWGtRdIFqraZEDIMvsiLKV4sEbLjNyMi94pQrlh50FUhlMC5MZpNWMHC92BdtM2MhMzsGy1hg6OiGQo5CYR5UZpp9LWSueywgw7spIJS9XRrt9bSBkcFtpLjB1TU6dfJZoZ3ELCbZQO3kLHdy/JN1Sp0gHpeVS/UshKg7ku2dXNnr2AO3HlBqAZUwyRBkOrSUaJkkXz6g+UkWurQzqBys5sAIAtGJFpBEBWSsu3yN0fR49hSG6MlRuH1QQ/LOhiypnSooJVIbUxgubMA/KVCqKRGVNqSMrASMyDkZvdy9QLXQdmwD7Qyk2eYqgedWrUTZVXblh3ZjiygYvfBsoOBbpacM2OG3GSaTmCkgU47VE0zvofhCHDLEuwmWXYC+i4FUotMPlaAADEZNe9jzUyLAXQ8Jgti4am9q5QbxydljxVBi/MYBldnHJTvyGu5yZRb4yXCz24ElFueljrponvK8WqE/cyLNXs8WNvCyUJGTZzFAlh8Fqy1b99sEYlEDDwtW46fEG1eSSfCg4ApdkOeN2ULAfq4pQ9iAOtxk1s5abTH0qqL1I8aGEpRWHgzcQMvOcmSNRMuiwr6Lm56WNg5StaUkK8sJRVa8EAxM6UYmCeNpZRRw9EITeddarabIgd3wBPTgNePQd/a78Zg1CPwgyb6pVKSG1U1O/N4TL1OsjhyA3LTGQlFwB6zkUV8lPM1ntIPm1QmoYQ5CaFkJlyc6DCUgkoN/z8FmkoliQJfzyJqiD/t3gr9rZ41ZoUFcpKy6SQG3tkplE4DJOXkptWLgU15WB1btr2aamSMZQbVoCr06QQIsV3wyZOW6BVe8185TUaOHLTtB3Y+yPtIzX2t/GPK28oMHgGEA7SAog8vC3A2oX072KDlWNfYuo1wDF3Aqf8Neohlg4OAIPzuXPUngVc9JYarttHcvFI2aNA9aVqWXq33QxJkmBy5aCVUGKU5aETHYkRlmKkEgANTQ0/jv696SP1xtFqoWbFTHtEWIopN03b4fHQc4+XypNvnKmFHjKMPDedqQlLsXBDrstGiRzQo9YLABQvBK3Rs1Xpe1UW0cKAh2y2oUX5bnyJKDcdPlWhsppldUx5FFbSsI6tc19Ub6FEsDtSCebA30BTFZryBUNww4NBUFp3FBqEpWx0fshUPTf03Lij48/AM7OAf50HvDeXJhWY7VHFEnm4bGZdWGobKTE0EzMwU38jX0vKVQhAonMJ5xVU4e+kzXtfPEXt8zUCu/CO7S6UtK3RlJtE1EZFudlENHLDq0ntyvWuU274WjeL7gb+XAYsfQQAC0ulJ41Iz6P6lcIUoOTGlyaeGwC6Ql5GqaFnThqEwwbnwhsI4f4P1nOl0ulnsLqoWcwZilBuuloghemF0GHOTuYTJAem3Cjpi3AVxiR1LFTRJiu1LJSJIqishqwBxTfkyNHUH95UzFKVhxylFUuMh6NuUZ73si71HN/9H1VuCkYBY87s/nVSCWcucNQf9AqfAh25yYtQF+2ZwMVvY/mUv+FU35+xIkRrcbCVHF+5dZ9EV6omopADc0RYymwQlgKAkbQFBDZ9rJo1m81U1XDbzGqKcl17Fx1/Rw4AAmsz9XvwikWP69zESAWPDEN1e6PwNAEvnwn8/IZuMzMT57v5vlI9U24y7GY1RX7tXkrMY4WkAHp9NxB6vdA6N915bvyGmVI8xowYgQ5ih4xwVIXaRBCrxg1AExyY/ylVoSlfMIwRkhJOyig1nivsWgsGfzCs+rFGBeiNH0XjgOEnAJMuBM5+VqtObgCn1QQP7Gg10ffZFjYu4MeQq4x9I0+mTWbtHGmP6L7tawf+fhTww9/p/5Nno+bCL7EuXIkCqQ2u185A8Q5q/G/xBtSq0zGh1I7aQsrUtHemELK5H4CaCg7Qa3N5WJkvg17A3w6QMHyw4LvwWLURbbohPY/qVwqTErrpMvUhueGL9jm7d6nzPl+jyU6SJNx7xliYZAn/WVur1EDRalJYXdkAABeJ6JTMQlLEBSni5pZSqCnfykUbQ7UBtBoVbZKe3FAFgcDCyI09W6uTw0zK/k7gxxfo34ddldixDZ4OVEyjhfC+eZRu66gHvnuC/n3MHXGz2Q40+LBUFLkBAKsLgVFnohmZaqopa1vAT3a1Jn12CEnEcwMAw44FJBOtPLxjKQCgXspXX78wkwtLSZKq3mS0U79HlpHnJtHGmd20X2Cm3VyXVfd/TGz8ENj2BfDFA7rNTUZNM3sYlpJlSfXd/KyQGyMFhMFqkrGV0BDKPpJnWMQP0FeijuW3YTi8Kk8Nu3TVJtBslgMhRFWCY5GyVKeD+wJhjJbjhKQAQ8+NA11wsfYsl30EXPhv4MwngNGnxX0/RmS+dx0Nr60A34XHGNa4YchXzq/GjojzSzUVR/hutn5O2zo486i6evr/Yp+1Euf4/4SvTYdDCvmR/fF1mCavRShMVBXKEEEfTaIAsClcrobP2DnPQnqRKp7bbsYn4UPx/JR3gGuX0RT2+etxrOkFrCFVIiwVC0888QSGDBkCu92O6upqLFmyJOa+X375JSRJivrZuDFGvZIDDLOi3PhNfdj+3WTRFJtEPDdxDMUMo0syMfsImmXDqnuysJQ9g76HG536m5XSeqGRZPZdGjgQXc8mht8GABwWerE2R9S6CYbCcMAHU1iZQB05XFhqC83cWf0qzRrKGQKMOCmxY5MkqpIAwIoXqSFw6SO0bUPJpG4nxgONIl1YytgXpvkx6OTboXZL1shNvVlftCshzw1Ax52lpisNPWtByXqG3YIitbicItkrvpu8Tqrc8NlSdlajKRhWPSPxoDbOjFHnhpES5kXqNizFzKbNO3SeK62vFOgKF+ixcgNoPhim3EQ2n+RhNkm4JXAVfue7Gz+SkTGzpZhC5g+GsVdRViL9NgzluU7UKv2T9m5bm9SxN3sCaubYoGzj42akuS2iiWlP4Q+FMCqe3wbQ+kuxbCl/CCWsGak1I27tsEgwgvCc62q8efRnqEd2fOWGkRs+Wwrg0sEjMqZYwsPQY+niADRs64Edj+XdDUykSQM3Wt4G0I1XrHErQELwmzOwHzlwKcfJsvIY+OsMgBrG3YcioHA0DclnDUJrkH4WodwY4PXXX8e8efNwxx13YNWqVZgxYwZOPvlk7NoVvxrmpk2bUFNTo/4MHz78AB1xfJgCVN3w96VyA2ihqTitFxh4chOPhMw/foS6onNYTKp07cik5CYTHtV/AYCa30DTwPus9QIQ3ZMnAeWmKaLWTTBMkAWuz4/VBeRWURXB305vVEpHXEy9Jjm1peoYasYNdgGf3A4sf5ZuP/auvqt11ENk2Mw4ZmQBJldkY3Ce8TnKzoFWbwD+oJZJ4ubITbNN70GQLMbKjT9oQDpY1pQS0tyr9JXilRtWfZUpN0U+6jMwUm4ALf03HlTlxqZlS3X6gqqMz8JSTNHqNizFyA1AizkqYAX8BlnZ+WZJ6mYZCRYyYOQrssovD4tJRivc+JGMAiDFrHNjt2gEb2s9XZDFUm4AIKx0uG7bG6ddiQFYmKMo02bo5+HfN5XKzUg1UyoWudGUG3+IVigukRSvS0Sn9e7A5hyPP6SqiPENxcxzE0u5iSA3LBTIlXhQWy9kOoBZdwKyBYdLGzBZ2hw/y08J7Te7hgKQ1AWLzWzSXd+8FwpAzEazvNKTjujXo3r44YdxxRVX4Morr8To0aOxYMEClJeX48knn4z7vMLCQhQXF6s/JlN6yGJmldz0YbYUQKsJZ5QC5Yd1u6ss8cpN7K87027BHafSlfLI4gzVtGhRQl82KYiOTs53o0juDSSz76oTA7RHl8Qddxzlhq2YGsKKcsaFpbIl5WbjyKGkw2ylKxAA+PZ/aZEyWxaNsycDSdK8N2vfpOnNlUfSlVaaQZIkvHDZYVh4zbSY50K2w6IS4sZOn2Yo5iY/1k9HhVmv3MQMSwEauVGwO0jPLzdnKPb4Q3QiVZSbQQG62Mly8sqNds3z6eCfrKvFSQu+xqZavQHeSLkJhAh8wTAIIWoRv6oCptx0E5Zq48nNUvVPta+UWblWnHm9Irk5EYpKPOUmcpER73pni5etdfQ4M+OQm6xB9HuQm7fFP9gIaCGp2ISMkhuC9s7OmPskA18gAeXGpmVLsTo3qnITo/t3LDAi4/EHVZUqXlgqz2XguQFiF/Jjyg1XloJvvYCsQcDE8wEA15jfj2/MVtqs1DnpvMf76JiiBAAZEecCWwy0c+QmHCZaDy9BbvTw+/1YsWIFTjjhBN32E044Ad9++23c5x5yyCEoKSnBscceiy+++CLuvj6fD21tbbqfvoI5eIDIzaw7gJvWJ3Qh8lWKY8XgGc6cNAgvXnYoHjv/EG2j1U3bSQDwtjdp25WwVENf9pUCaLoX32E9LrmhF2F9iJEberzBUFhTbnglKF/x3SjNQlF9CWDrQUhx2HG0GB7DrPRTbXjEq0nE171oaPdzYSltwu5wRqxuI8lNLEMxQBUz7jvcEcgGQMm1y2ZWpfK6Nq3WTUl4P+zw6W70siypK0bed/PWyj3YWNuOT9fpV8DsxuO0mXWTeocvCG8gpBaxZMpNt3VueOVmu6bcsHCe1leqZ5lSDJGF9eIrN5EVyWN/z0yhY+SGVbk1QsWw8QCAPN9ulSQmAtVMHMcEneWw4Cbzv3H+oqlqReykQIjWJgGAqX0PMiUv7bydF0PR55SbQJBWKC6BotzESfs2AltQefwhlWQ742VLKeMemZ0XU7kxIDdadWLlujvyRoQh4XjTCgRr18U+WKVf3lb3FAD6UHMOR25iKjdc6NDPXduiiF8EGhoaEAqFUFRUpNteVFSE2lrjSo0lJSV4+umnsXDhQrz11lsYOXIkjj32WHz99dcx3+fBBx9EVlaW+lNeXp7Sz8HDohiKg+Y+JjdAwjdPWUdu4n/dkiTh6JGFqOBDFrKMTtD/fe3N2nbFUNxA+jgsBWiyviM3rn+BTTR1QWX8ubBUtqSspFlqOaCFuEiYhqgO/33Pjk+SKKGBRCvxVh7Rs9dJE/CpwlpYSiMWfmcpgkT7ziVLgp4bBtXTJGG7T7nRKBOqzlTsLkDAlgdZIqh21qEqX088HQbp4IxcNHCptsFQWJXQXVYTZFlSSVRHV1C90VjNMkqy6fvHJTeE6MlN6y71JqQaisH6SnUfOo4HntDlOC06BS0SkddhrDo3gPYdM3UlXliqaAhNpy5FI1ZtS7xFQGTmpRFKzW34velDmnn33lz9uHYHQoB3rgEeGQO8cy0Q6IKrhfov91srqDprBFW5UVLBfVxYKmlyo4U4VQIdT7lxa4Z1nVfMSLkJhzQ/V3Z0WIopncgfjpXOGQCA8vXRDVUB0DIXjVsA2Yz1jskA9AuWXO48i/LcGGQXsusJEMpNTESuIgkhMVeWI0eOxFVXXYXJkyfjiCOOwBNPPIFTTz0Vf/1rdE0Phttuuw2tra3qz+7dBgXXUgRLSFFuDgS5SRCmJMhNLHiUooT+Dk65UchNI7L6PubKCEnBqLikjsnBtRHkJhAKI5MPSzHkc/6dMWfErUTaLYYdC8xbA/zu+Z6/RpqAmYrrO/iwlDYROux27CUayZRjeW5CMYy+itGa5AxGq59+n6yuhpYOTifwfVY6qZ9a3BLVTsSokB8jNXwFWA/3OLsZ8engjMjkOC2qxyWuB8TTSD1WkDTFTlFvGLnJCrfQ7b1UbvgVdTySABiQm3jKTQZ9XZY5HMtQDACSqwBe2QVZIti88efuDlnF7jgF/BiOangdNkkZ665W4K3fR7dEiYVVrwA//Yv+vfqfwAsno6juGwBAjX1o7OcxQzG88IeI3lDcC8+NFpaKTUBznFZIEh13XUYeq6XDKzdt+2itHdmiU+nrWFiKy35cUnwxAKBy33+0Qns8tn5Gf1ccgaagQzl2PiylXcMZUcoNPTd45Yb53GQpuodhuqDfyE1+fj5MJlOUSlNXVxel5sTD1KlTsWVL7OqZNpsNmZmZup++gkUJSwXSidzwnpsenoRema6Yg51cZ3BVuck8cMpNHDMxoMWQGyMNxSFiHJbiX++Iub0/zuzyqJovv0bwvYfYhObmJjyXzYRdhNYBChIZ5ogVsq4ruBHKpgDn/RMdpz2rbmKKhFbIj07gq7uoXH+Yqy7qZeyW6LBUg0KK6jnlRtf1WSHifJViRkhynFa13kuHLxhbeVKqs8JdpHmrdixBVyCkhfF62TSTIYcLS8VLAweiyUy8fm9MuWGIp9xAkuB1U5JZt2O98T5tNbS6d0gjhaz1Qszj7mzE+FpaEfyN/LmAxQXsXAp8+1jsY2Go20AL2wHYX3UWiCMH2LcSY/fSukPMV2IIznMTDAYU5aZ3nptgWGtjEU+5McmS+p3qTMVuLluKMU4Wksqu0CU51EcqNwA6c8fh69B4yAhRD2EktnxKfw87Tl2wuLjjzOUypiL9V0Z1oVibC5vZ1Hetd3qJfiM3VqsV1dXVWLRokW77okWLMG3atBjPisaqVatQUhK7guSBhFVRboLmPkwFTxL8arenCkuX8nlCXi4sxXluuvPy9BosHh3LIKiAreTVzuDeFiAURDAc5sJSnHJTPJ4W2Tt8DlBWndpj/hWjgOs9ZJQK7rSasVshNz5YVI8NQ1xDMcPo36AlW6l+a5bVc5O1YKhv92F3kwfLPXTSrwxFZ1DaI5Qbrz+k1vngw1J812cGvkoxW0HnOK3IdFhUcTCmesP3HBpCwwHYvgRNyntaTBJsXUqYI4VhqXgKCH1fvmCnFPemE0Vu4ig3AGAupP4VU8u26EJxHXXAc8cDC68APr4VADWcxivgBwBY9gQsIS/WhAfjHetvgJP/QrcvfgDYtyr2wQS8wL8vA4JebM08DFPXn4V3D/0nUKjND42uOBm0dm2BK/k6FOWGhaWSU2/5c4qdc/EMxQCXDs5XKWbkJuSnfd4Aw0wpfzCsmpF5cpPlsODJ0On0n1Wv6MNbAa/WDHj4Cdz1EEO5sRl7btq7tOuBhaXStTox0M9hqZtuugnPPvssnn/+eWzYsAHz58/Hrl27MGfOHAA0pDR79mx1/wULFuCdd97Bli1bsG7dOtx2221YuHAh5s5Nwao7BWDkJmRJH3LDqzU9VW78ZkoWQl7OjM2ypZAFa18XcTr6VuC4e6MaQUZCliU4LFzzTBCgqwUBXrnhPTeyCTj3JW1SFQAQy3PDKTdWTbnxIboSrtXcjedGAat+zJd6L8rUwlJfbq7H5jC92ViaotOQI5tn8oSGKTgADLs+q+ng/iCaucJ7JllSPQcxfTcquRlEG6zKFqB9HzpqaJG7HKcVEiujn0JDsVHzSR7892CO47cBklRuADiKaE+x0uA+/bgEuoDXL9LUrOXPAhveR0OHD/5gGLKkr4ytwtsC/ED9If8X/C1au4LAIRcBo0+noZiFV9Gic0b4+FZaCNJViPssN4JAxqubZeDKRVhdcBqWhUdjX9bk2B/GbENQouNqCrQj1NWOLEmpzpukcmMxyWpCBTv/4ik3QIyMKYtdU5VZIT8DM3EDR6B5VS/bacF34THYZhtDQ6bfcerNjqV0W2YZUDhaU250YanYyg3vuWHEloWl+jSZpJfo1yM777zzsGDBAtx3332YNGkSvv76a3z00UeorKRMtaamRlfzxu/34+abb8aECRMwY8YMLF26FB9++CHOOuus/voIGsIhWMNUSg+mEbnhU8EjV9iJImChKx2pq4Vu8HfSQnWgRfz6XLnJGwpMn0fr03QDp9WEEEwI2bT+UsFQDOVGwBDMj9HQYZwK7rKZOXJjVQ3EDIa9pQzAiBOfncF3Bv9yYx02E2Ul3bIL+OYx2oW9kxIHR0QLBj4U1dYVVCdgI+WGhTDbu4JoVm7WzHeSrXZZjpEOrpKbcsDqpHWOABCl3k2+06TdmFIalupOuem+YCdDQYY+lBir/YL62gVUCRkj78DuOoW4EQK8fyOw+3saOh5/Lt3+7lzU7qaVcEuyHMZh6x+eAXxt8GaPwKfhaqqSSRJw2qN0zBq36FLsVWz7khbMhARy1tNY1UiP+8edTWjwm/F22a04338XTLb4c4VfUaNNgXZk+GmIPWTN0Kk6icKp+G5YmKk7cpOv1rqJLOTHuoMrVg01LGVQ48Zt06nyWU4rAAlvuJQF4PLntArZSpYUhh8PSJLqDdIZiuN6buj/YaJda0K5SQDXXnstduzYAZ/PhxUrVmDmzJnqYy+++CK+/PJL9f8//vGP2Lp1K7xeL5qamrBkyRKccsop/XDUBvBpdTXCljTy3PCG4m5Wc7EQtirkxqcoN0pIKiDb0Al7WrF3NtEEbEo4oLMBgXAMz42AIYyUG1cEuVlLhiBEJOwl+QZGVmYo7k65oaSC9/OwsNTuJi+++aUBLciAP4N2psaiu4CXzwD+pwp44xI1LMXi/7xaA2g3G68/tnLT4dPCUuNCG4CFV2KiRel71ZlAWApQQ1POvd9CQhi3BB6nyoJsAUomxh2D7qAPSyWu3HTng0tWuWE1hybJ2zDq1cOBRX8CPr8P+Pk1mml4zkvAGY9Tg3VXC0oX3wAZYeO6PL4OYNnjAIDWQ6nyotZnceYCI06kf2//Kvq5696hvydfjIbCaar6FybA5xv2J1x7JaAsQC2BDmQFqFISdvfM3sCIMrvxs0rpscAyph75bAtuemM1Pl5bS1PsWWhKVW5YWGqw+lzmRSvI1KthjJx+FZ5Iv4OAB/jucUpAt3xCdxp+PACN7MdUbiKypZxWkxqqZR48VjohXVsvAGlAbgYMFHLjI+a0MpXKSdS5iYWwlYalTH6F3CgrAo8lD4DU94biJOBUJha/LZtuUJSbLKNsKQFDaOTGbxyWspmxixThVP+DuMp/U1SxuIQ8N+DDUjy5oe+9q8mDrkAYxZl2WGa/STudjz6dtscAgPXvoCpMVV12U2mIqPrKJPx4nptOH1VuBks1OGvjfGDNv3F/2x2okvbFbsEQSW4GU3KTV/8D7jG/hKM8i5Qb/gvUZN4L5LisyHNZkeO0xC3gB0SGpeJf60mTm5JJWFg8D7vDBbAGWoFvFgBLH6aPnfwXYOgxNPX67OcAqxv5jSsw1/SOMSFb9Qr1leQOhTzutwBogbgwS40echT9vS2C3BAC/PI5/XvUb7CtXt/M99N1+zWjazeKQkAJtcv+NuSHFYUjyTRwhkiPTXfKzanjS5DrsqLVG8BbK/dizj9W4IgHF6PTyppnRig3RjVuMoy/v1ZvAJiptIT54Rlgz4/0dWSLOq6sWjdf74lXCCOVG0mSNN+NMh+oyk2apoEDgtykDu4iPDb8eVzgvzOtUuN4PhMveyIeiJKtZFbJDVVuOs2UKKQVuVGUmy5LNgAg3NmIMIGx50bAEOzG1+zRukZHem4AYCOpQDOiw5LW7rKlFDDlJoOrocOUG4ZjRhVAKhhJe3id9wpw42pg5KkAgCM7qdyukRu9csP+9xhM5i6bFpbytrfgacvDsAU7AElGZrgFr1gfRKApRhfsSHJTdihgssHpb8Al5kUgkIDfPpWS3mIWk4wPbpiOD2+Y0e0qmf8eursmXTazGtZz28xxqxkDACQJe4ZdhKP8j+DFigeAIYrCPvU6faPZvKHAbx4BANxoXojx9nr96xAC/KiUSzjiWmS57OpmRnbV1675SS3ECQBo2kbDk7IFqDwS2xroNc2M1ku2Nqg+lu7GilkHSFcrSkDfQ+4hEXUlSW4Or8rDD7cfi9eunorLjxyCLIcFrd4A9oWUUHrHfhr6V+ZZ3lBc28pVJ+aghVIDwMhTgKLxtLXMwsvpDoOPVAuUdhoU5szjwlJG1aozIgr5+ZRrTpCbgwFmK3Zbh2ElGQFTD8M/fQFmLLSa5B6n7MkOetFZg0roTUkD71DITTr1FmETi9ecDQAIKyqT8NwkjlyXFbJSi4MVyIvMluIRGZa0qIbi+J4btgrkw1KZDrPufDpmZGH0ExVjeXXrpzAhZGgoBmi2F6ApN/wKWzUUd/lxef1DGCHvhc9RCMxZinpbJQZJjThxxe/VEKyKoE+rRcIyayx2XSuUL4bdDkw4N+5nTwYlWQ6Uxmg8ySMyW6o7sJpC3ao2CirznAhDxn8C1cAl7wO37wNO+nP0jhPOxSr7YTBJBDPqX9M/tvMboGEzTfsefy5sZpOa0t/qDSAYCuPer5qwz1IBgOh9N78spr8rpgI2t6rcHD+mCIPznPAHw/huG/UDdXfTDSlqtORrVzOlTNk9U24ir4fusqUA2hpjalUe7j5tDI4dRc/x/SSbPtheS0kcQL1M3Jy1eT+dg4cW6H2drMJ0pz9E60vNvJk+wF5nGA1J6QtaasedYTerY8a3YmCITAf3ibDUwQVWcTKdlBvGs7ozGMZ9DWc2AMDGyE0HJTftCoGINJT2J1i8u9NECRnpbISEMDKhZEMIz023MMlS1ATHS9WRVXLjeW6i0oY5GIWlJElSV6UWk4QjhxkYcoefADhykRlsxAx5DbqCxuSGGYxVAyU/mSufYUbtS5gWWAYfMWPXcX8Hisbi7XH/hz0kH7ldu4FXfkszghja9tHfJpu+WvYhF6FLsuNPgUtQM/y8mJ+5L6ELSyWgprJ6RomSG5bSvUupPBzL4O/xB/FUkKYlD97zrjpfAAB+fIH+nnCOat5l79/Q6cP1/1qFF77ZgU+9SosO3nfDyM3QWQCAbfVUuakqcOOEsdSM60/Q6BpSkiRM/nauxk3PyA2vgADxG2cagRHXvUFOuTEISQHA+hqqno8u0RufM+xmfQmD0aeD8K1qhtM2R51cHzYnd9yyLOHBs8bj9lNGoSgzOrvNzSmdgDAUH3QIKuSmp+GfvgA7lt6EjljzTLuS6s6UmzZT+oWl2ETTzsiNpxEZ8EKWlJusCEslBN6TYZIl3UrYGTGZx6pzA2jXhBHUsFSEgZGRm8OH5OkUIxVmq6qM/M70NbqYcqMoNcybonpulNUmf9xumwlXmD7Eb5peBADcGbwctsFT6efNKceF/tvpObR/rVbdFQDalD5GWWX6atkTz8eF+W/ipdCJOon/QEKXLZXAHMS+42SUGwCobevSVYXmsbOxE2c98S0+6RiCn8kwyGG/mvKNzgZg/bv07+rL1Oew9//vN37Cf9ZSVezbsFKzhvluQgGtVgsjN0pYami+CyeO1Rd+7U5RYD5Cc6Cdq3GTXBo4Q2RF4u7CUpFgLT92+JT6XO01hplSbV0BtXbQmAhyI3MlDFq9AUCWsabqagDAHqkEyKfZbuxasJikqDE6a3IZrp5pXPzQbdeKWwIiFfygQygNyQ1LBe9NurbFRUmMkyihHSUW3CIp5CYNw1JtknLxexqRpYSkiMWZVmbvdAZPbtw2sy6kGa3cGHtugPimYrYKjGzUV6VI7sePiVOpXAlNHS//CCglChiZGVWcqfxPyU6UcuNrxxEr/xt3Wf4JGQTPB0/Cv0NHI0fJGMl2WLCTFONHu1JMdPcy7X0j/TYcGjz087BsmAONZLKlAK3NRrzWCzzyXFa4rCYQojXF5PHlpjqc9r9LsbG2HQUZdjiPmU8fWP4M9ZCs/ietYVM6GSidpD6P3ZS3N3TCapYxosiNZeExCEOmKeFt+4A9ywF/B00TL54AfzCsKkhVBW4cUp6jO2e7C0sRpUqxi3RyrRd61n6F99xIUvI+lNIsSsY3exQlrH2/YabUxpp2ZX+7YdHFyBIGr3kPxfX+ubjKPx8sQuwxKOCXCDTPDV2QaMZtEZY6KBAM0y88nchNKpQbe0Y2AMBNmHJDfSytclavXzvVYBdtK09uFDOxJEJSCSOfu0FHkhmbWQZ/ikd5bjiyEwjGVm7U1g4Rr/+HE0fir+dMxEVTK42eRlE8AY3uEbBJQYxtpMoKC0ONLqYrYJYark7oNhNQtxF4+hgU7/kEfmLC/eRy3Be8GGZZywhhN4nVEu1Kjl08uVGK1WVFm0+bOrRigP0BfViq+zmoRAk/RGZOxYIkSVxoqlP32Dur9uKyF5ejrSuIyRXZ+OD66Rg28wJ6c/Y2A6v+oYWkplymey4bb7tFxguXHooTxhSjDS7sc4ygO2z7CtiqZEkNPQaQZexq8iAUJnBZTSjKpDVfeDLcnXLDyE2x1ITMHhbwY+CJgtOSfDsCptysb1d8VYFOqhgCOnKzIUZIioEpYC2eAAgh+HpLA94PT8OGUBlqWikZZZlS8RqwGkHtDK4oN4mm3Pcn0vfIfoVIR8+NOQXkxubOBQDY4VcMlVS5aZayAaSnobgZSqqntwnZIg08aUQqNzwkSdKFiyLPLZMsqRGbeLVujCoUA0BRph2/qy6Lv0iQJOwoOwMAMKX1P+gKhNTXG1XClBuWCk4n9GHty4FnjwUat8DvKsF5/rvxrO84ABJyXFb1psRutsuCys1132pawh7QVyfm4AuGVIN0Xr+Rm+RqWp13WDl+P7MKV0wfkvB7sNDUzkaPbvtzS7eDEODsyWX419VTqW9DNmk92z67F2jeTvs6jTtb99zfVZdhYnk2XrrsMBw5LB+DlOynnyxKjaDtXxn4bagaO6TApX5vfGiq2znJRueHERL9Pjsll7otWfCem3hNM2OBeW5qvGYQq2IU3ruC/uYypRIlN63eALY1dOrUtd1NjNxEl0VIBMxQrKaCBwS5OaiQzmGp3hiK3ZkaKQh6WtSwVLOkZFGlkaGYraIaw3SSkL1NIg28B8jnUk0jDZOA3pwbSW4kSUqo1k2b6rlJ/oYAALWVpyFATKjybUTn6rdwpelDPG/9K2Z9eTYuNX2Mlnb6vXt8QZwhL8VxK+fS0MbgGdh33idYRbT+Q3yxPNbyYENXDi2sFg4Ae1fSB2OEpVjBPzPnfTjQ4E3ELGMtHgoz7LjtlNEYnJ940dHKPLqvaioGVcaY0fXmE0foVZNJFwKOXLWiOSacF2VEPmlcCd697kgcXkULbw5SbvZLgrT3GLZ8qvWaivDbVOVrWUNHDM1TwyfdKhPKXFAhU/9gi7nnbTL47KhkSQNAw3LseAMO5TgCyvjmaMSTkZsxpcbkhp23LZ4AvtqkT8HfrXxfnQYFLRMBO75WpZo389yIbKmDBMw82RsikWowotUb45fLYUMboROOt6VebezWgGwA6RaWohdbPSM3gQ4USC30QaHcJAxeuTGaCHlzrtXgRmpNgNwYZUslA9ldiC/DkwAAeR9eiTst/8QseSXsjetwj+VlvB6aj+D6D3BM0xt41PoEZBIExv0OuOgtOHP0fh5dnx5lBdzWFQIppyZj1XcTg9w0dlKVKMdl1RXOPJDgr/Huekv1FGpYilNuftrdilCYoCTLjpKsiJR1qxM49Ert/4iQlBFY3ZpPO4aAmKyApxEAoY0xlRYFTLmpKtCIks1swsPnTcKNxw7H6JL4KowU0WahzWpQciBB8ES/J+QGAEqU/lteG58dKKnhz2AojI211HMTS7lh522LN4Cvt1BywxQsRkaNatwkAjbO767eh6117SJb6mCDptykz7CyibY3hMtiktEOenL76mhzQEgmNIed6uPpAnZT3t5uolViAQyWlLokwnOTMHjPjRH54FfGRjdSFiKJR25Y/L6n5MZuNeH50EkIQUbQ4sbnoUPwnPMKhE/6CxpIJqrkWpjfuBCz258BAOwaeTlw1jOA2aorHAjoyQ2fPeQpoX2jsGsZLfzD95XiwFo99FdICtCrNX3V761SITc7OeVm5S662JlcGWPxcPjvgYJRtPdU0VjjfTiwME2T34xg6aHaA0OPUf/k08B5HD+mCPOPH9Gt70VSCpMytNvimNe7AU9oEqlxY4QS5TO3mrgu8pmDaGYggB2NnfAFw3BaTep3EAl23ta1dWGZUu/nNxNoS4ndzUy56Zmh+LQJpZg+LB/eQAhzX12lFvcU2VIHCYJp6LkxqdlSvfuqOyVKbsINtCEeXPnwh1Pz2qnE0EJ6nFvrPSBO6hWqkmrogyIslTB0yo3BRMhP6EbZcmqtmxiGYkIIR256FsZxWEz4LjwWZ7r/ibeOW4IrAn/A0oLzIU+dg7PNj+Px4OkIm+jnuD9wIWqn3qUWfrJb9KboHK63jtkkq+GN5jylu/Tu76li6VcyBiNqojR19q+ZGEiuK3hPwTw3u5o8aruEFTspuamuiEFuXPnAdd8DZz+T0HvYLSb1/GsqPEJ7YNix6p9aWKpnffxkh1798Np7Tm70BS57Rm5KFeWmQeLGkDMTr1cypUYWZ8RUBplX7PONdegKhFGUacNxo+nnYsqNp4eGYlmW8PB5E5HvtmJjbTveWUVLIgjl5iBBOnpuVENxLyc7r0wnEblxC93gKlRX5elkKhuc54Is0a7QITslN6pyI8hNwijgPDfubpQbI5WgO8+Nxx9Sr5ceKzdKGmpT0I56D30fdlN0ZGTjf4LnY9lpi3GBeQGeDZ2qu/Hw/XIAvXIDQE21rXONoNV0u1q1jB1nHg23cGBl//MSzDzqC/DXeF+FxkuzHTDJEvzBMOrafQiHiarcVMdSbnoAFpraljmFbjA7gApKdFo8fpVM8mGpZGCKmAu6nD1rmgno1ZrummbGgmoqDnHHlUSmFKApN/VKluDM4QWoUFQe5rnp6KGhGKAerYfPnQRAW8gLz81BgnRUbhjLT8RgGA9eE5V/zS3b6QZXvloNNJ2UG7vFpPoCPEohv0GS0hhPeG4SBq9AGK3yeFnbSJpmsf5Y5Ib5bUyypPY4ShbseV2BkDqhMyM0I2f7QtlYG6QpvpHeIV4xiiQ3as2QLgKUVdONa/5NfxtUsm1SPDfpE5bqm2vSYpJRqqQu72zsxLaGTrR4ArBb5JhG156AZUytk0YAJ/6Zqj4Wuu0XJSRVkmVPOrzCYHbqw1Ihd8/SwIHUem52BTivUBKZUkB0McajRhagQlHaGjr88PiDalmEZJUbhpkjCjDnKK3QXzotbCORvkf2K0QoHevcsGypXio3PqWLrqNtG93g1pSbvorv9xSs70qLpHQzZ9WJhecmYVhMsppBZGQo5g2JRjdSdk7ESgXv8GkNOXva84yRG28gpKZ9M+VG62zu44r46W88/GfIiSAlrFdPi9evKgZqR2qDGjdb9tNwVXFWdOn6A4VkuoL3BpW5VC3Z2eTBSiUkNaEsO6WEqoy1JGjtAo64TteE1MhMnCysNgd8RCMDoYyekxtnL7OlAE25+cXLeYgMlJsxcYzS2RxBlyVg+rB8ZNotKunZ3eRFhxKW6ikpBID/PmEEJldkA9AM5ukIQW5SiGAo/cJScgrq3ABAQCE3Np9SqtxVoN640qlCMQAMVSa9+pDebCiUm+TACILRKi9eKji/LVbzzLZeZkoBUBsu6smNVfd7b7NXDX9Fmj31YSn9qpeFpVo8AdqoEQDCStfqiEypQCiMb3+h18WRQw16YR0g8GGpvrwmmRqwu8mj+W1SGJICNOVmr0ElZKM08GRhMctoA5fZ1cMCfoCe/PfYUKyQ4k2dHGFTyE1Tpx/723yQJGBkcWzlhq80PbE8WyU7FVxPMKbcJJstxcNikvHqVVPx9rXTMHN4/53v3SG97kq/cqSj54bdd3qrrgStEReVq0CtPptujnmm3Oz1R6wqhOcmKTDfQ75BOwFnop6bYPywVE/NxADNlgJoEtO+FtrcsiBCueGzeiJXq24+LBWl3HDkpuxQQOLO8Qhys3JnMzp8QeS5rBibwtBMstCFpfpwDmI3y52NHs1vE8tM3EOwc8+ozUMqlBuLSUI7oZ+jlThhc/b8e0ulcrPTzx2H0leKqTaVuc644SQ+LHXUCK1uD++7YRWKk61zEwm7xYRDKnJ6rLoeCPTuEwrooFUoTp+bPTuW3io3oUhyw4Wl0qlCMQAMK6TkZocnIkQglJukcPspo3HksHwcMyq6BohbWflZTJLhBNddnZv2XhbwA6Dz6uxroTdB5rlh5GZXI13l2y1y1KIjI46hWOvTE6CVa4vGAbU/0wcjyM1Xm2lNkRnD8/utxg2QfFfwnoKlIq/Z24rtiooSMw28hxiUTd9jb4sRuTFOA08GFllGO+h71JC8HntQgEhy03NzfK7LiqZOoGHStch3moAMmum0fl/3fhsgNrkpUxrJ7mry9LhC8a8RgtykECGSfspNKioUAwDs0cqNGpZKU+Vmu8cO8Pcs4blJCsOLMjC8yDjGzybxWN89UxFavAF4/SHYLbKOBLG+Uhm9uKlYTJSwmZSVgAAAIHFJREFUhMJENfOrnhuF5LCVv1E6Oy/N50aQmxy12ivNykHFETHJDSuYdtTInle5TQV4n01fFhJlYantXDp2qlPgWViq1RtAe1dAVfiCoTB2NPYuDRyg4foOldzkIqdX5IYLS/WikWRJlh1NnX78NHIejh2tpaYnYiYGKEG6emYVmjv9mFCWrW7nlZveGop/TUivu9KvHMxzk07ZUuze0+vQkU2fXUB02VLp83kBGmLIdVnV/lIqIgp3CfQcLlW5iUFulO23vbUGo+/+GENu+wi/+d8l8Crm3t5WJ2bgbyYmWVLDSSyUxkiP08Bj4FYK+clS9HFkcdVeAQAVh3MPauSmvt2HtXvpzWfG8P4lN7TtRWpKP8RDRYSJNNWqDUBvvkw949WbPc1eBEIENrOstmnoKTolTbmJNJsnA5Msqf6v3igirLrzvtYu3fb1CZIbgKqt/3PORN0CWyU3zR4uFVyQG4EkkI6em1RUKAYAyZmt+3+33w1fMAyLSUKeq/9qe8TC0AIXmohGbjplN23kJ5ASuFTlxvi8OnlccdREv3ZvmxrC0cJSvevDZOfITR7X+qAgot6MkXLD6vfkOKNbJvB9egAAFdMAk5WGNt3aqnqJotqMG5SZcHftvgQjlX2p3GTYLTqlZnKK/TYMjLzwpuJtDUrDzHxXr0OALUpvvN2kUOch6wkYWeipoRiAmmJfw5E5fzCMXxSPUXctJWKBNxT3tCv4rxED/xMeQKRjbyk2yRdm9C5FNbIuxNe0QCWmVOb26oLuKwwrdGPJTm0y8MgZ6LmILRAJZkiMpdycd2gFzju0AqEwQVcghL98vBEvf7cTi9bvx0njilOSLQUADqv2/jy5yHVZIUnUbAwYr6iZbyjbGU2wstVsKSUslVkCXPI+YLbrSDIja7zHoT9hTlF2ZHeoyHWqhfRSnSnFMCjbgXX72nTKzerdrQAQM1yaDP5pOhM1PjdeCx2NOb1UMpxWE5o6e6eIqIX8OOVma10HAiGCTLu5x0pVabYDsgR0BcLYH6SvbaRkDjQI5SaFYHVu0iksddHUSvz94mpcMX1I9zvHgUVpZQAAsGfhq1/oJDM9TVMBhxa40cwpNx5T7ydDAQ1soi3KjE+aTbIEl82MU8bTCrCLN+5HMBRW5XGj6sfJwM5VSOU7mZtNss4kbHTTYWEpI79IdmRYCqAp4aWT1H/DYYIlW2iByKNG9LzxYirBzP19HSpmakCGzYzhhT039sZDWQ59Dz5javHG/QCQkhTkOnMJHgmeg2Zk9nqBxoh1nkFmYaJg6eA8mVu6lZLnieXZPc5MsphkNeTFyL6RkjnQMPA/4QFEMA0bZ9otJpw4trjXr2NxZ6t/E1chlil1PWakMbnphB0+YoZNCsIryE1KMTjfhTfnHKEaP7vDlMocZDstaPYEsHJXS8rCUvxNKTJlPd9tVdUFI+VmYnkWbGYZRxjUpsnisqXCYWIYAlm7rxVNnX64bWYcohQ162+oYak+noNYj6lDKnP6LEMsstZNbWsX1u5tgyTBMIMvWTAiaDXJvc74fOC34/DT7lYcUp7d49fQlBuN3Hy8lraOOWFMz3tfAZSM8qSpN3Vufi1In7vwAEAoDdsvpAqOjGz17w5zDtp9QWQ7LRhbmp4mXZoxJammYq+5/+qPDFRMGZyrrgi7g9kkY9ZIekNatL5WNRRn9la54Tw3kT4bXfNPA4/B2NIs/HzPCbjp+BFRjzFDMSGa+TkSX22iq+ojh+WlTcYgO46+Vm7OO7QcR40owPWzhvXZezB1cI9yU/5iUx0AYFJ5dkr8TWysUhGiGVuahf86vKJXdV+YclPb2oVwmKCurQsrd7UAAI4f07sFanmu/joVhmKBpMCUm/6sddFXcDkc6CR0QqkJUsJw5ND8tDJP8xiU44DVLKuhKZ9JkJv+xnHK6nPR+v0py5biyU3kDY//P1YWS6zGfzazSX1Oi9dvuI+aAp4mISlA8/v1ZZ0bgIaMXrr8MBw6OLf7nXv8Hnrl5vMNNCR1bApUG0AjN+kSoinKtEOWaFXvhk4fPl1PP+8hFdm9buvBZ7g5LKa0nbdTCUFuUoiBrNy4rGa0KZbcXzz0QklXvw1AvR5V+S40KuSmyyLITX9j5ogCWE0ydjR6sKWuHYDme+kpHBbOUJwRGZaKr9x0B+a7YdWPebR6A+qqeuaI9LkOrGpY6tc/BzFy09DhQ4vHj6Vbqb9p1qjehWgYrAoRTJcQjcUkq4kf+1q68Mk6GpJKha2A7wGVLp+3ryHITYpACEnLVPBUIcNuRptSrnxjO71pTB+WPpO6EYYWutEAGjbzWbL792AE4LaZccTQPAA0cwNIbZ2bKOUmgzcUJz+hHzqEqhLPLNkW9dj7P+1DKExQVeBSja/pAC0s9euf2rMcFrX+zJsr9qArEEZJlr3HKdGRUMNSaaLcAECJkg6+saYN3ym+xtSTm/T5vH2JX/8VkCYIc/0BB8KqKRIumxltSkXPepKFwXnOtO4IC1DfzTPBU/Gv4DHYkHdCfx+OAIDjI4yRByos1ZPQw43HDodJlrB4Yx1+2N6kbm/q9OOvn24CAFx0eGXSr9uX0MJSv/45SJIk1VT8j2U7AQCzRhWmrJ+RGpZKIyWjVPGw/eP7nQiGCUYWZWBILyoxM/BhqXQic30JQW5ShGBY66EzEJUbi0nG9xgPD7Hhx/CIfq/GmgiGFriwjgzBbcGr0OVIH1/EwYzjRkeSm9QV8YskN7zBuCepvlUFbpx3aDkA4C8fbwRR8mj/55ONaPEEMKo4A7OPSC9yM3N4AXKcFkwYlN3fh5ISMFVsRyNtgBp5/vQGrHN6unhuAM1UzKpenzg2NZ83z2VV1cveVGP+NaHfyc0TTzyBIUOGwG63o7q6GkuWLIm7/1dffYXq6mrY7XZUVVXhqaeeOkBHGh8hTrpJp8aZqcQLlgswwfcMNpPytPbbMAzlGuulW4uIgxXFWXZMKNMy7HpbKZWRFlmKrlej99z0bEK/8djhsFtkrNjZjM821GHVrma8tnw3AOD/O3Ncnxt3k8X840dgxZ3Hq/2ffu3gC9fZLbIa1kwFNM9N+pCb0ohCfSeO631ICqAqWLlCFNPp8/Yl+vXKfP311zFv3jzccccdWLVqFWbMmIGTTz4Zu3btMtx/+/btOOWUUzBjxgysWrUKt99+O2644QYsXLjwAB95NIIcuRmIyg1AC64FYYZJllI6yfQVeHIzUAnnrxFs9e2y9j5rgxXxy3VZo15L77np2YRelGnHZUfSApj/88lG3PXuWhACnD25rE8zhXqDgZStyddRmj4sX6fU9Raa5yZ9lAzWggGghuoxCfSTShTMRpBOYbi+RL/O+A8//DCuuOIKXHnllRg9ejQWLFiA8vJyPPnkk4b7P/XUU6ioqMCCBQswevRoXHnllbj88svx17/+9QAfeTRCIV65GTiTCw8m304sy0JmL8MJBwIOq0ld+Q0ED8JAwSnji2GWJVQV9L6yLWu/YFT3hO951pvQw5yZQ5FpN2Pz/g6s3duGDLsZt50yqsevJ5A4yjhyk6osKQZWuC+dlAy+btRJY4tT5i8CNN9NOoXh+hL9Rm78fj9WrFiBE07QGz1POOEEfPvtt4bP+e6776L2P/HEE/Hjjz8iEAgYPsfn86GtrU330xdgyo0kDayVEw9WKn/6r8BvwzBUKQ0vwlLpg2GFGXh37pF4enZ1r1+LZUsZkRurWVaL8fWmvH6W04Jrj9GK1f3hxJFp0STzYAAflpqVovo2DOmo3JRwyk2qQlIMx40pRI7Tgplp0getr9FvFK6hoQGhUAhFRXo2XlRUhNraWsPn1NbWGu4fDAbR0NCAkpKSqOc8+OCDuPfee1N34DEQJgQOiwkpJNpph1PHl6C2tQtnHTKovw8lYZw9eRB2NHRimkGJfYH+Q6oqWx8xNB+D85w4fWKp4ePnVJfh218aey3vXzptMBZvrEOm3YIL0yxDaiBjbGkWpg3Nw5B8V68L2UXiuNFF+GF7E44emT7JBgVuG34zoQS+YDjl3danDc3HyruOT6kalM6QCEsBOMDYt28fBg0ahG+//RZHHHGEuv2BBx7AK6+8go0bN0Y9Z8SIEbjssstw2223qdu++eYbTJ8+HTU1NSgujma6Pp8PPp9P/b+trQ3l5eVobW1FZqYo7CYgICAgIPBrQFtbG7KyshK6f/ebcpOfnw+TyRSl0tTV1UWpMwzFxcWG+5vNZuTlGRtcbTYbbDYhIQsICAgICBws6DfPjdVqRXV1NRYtWqTbvmjRIkybNs3wOUcccUTU/p9++immTJkCiyX9Da4CAgICAgICfY9+zZa66aab8Oyzz+L555/Hhg0bMH/+fOzatQtz5swBANx2222YPXu2uv+cOXOwc+dO3HTTTdiwYQOef/55PPfcc7j55pv76yMICAgICAgIpBn6NSfsvPPOQ2NjI+677z7U1NRg3Lhx+Oijj1BZSQ17NTU1upo3Q4YMwUcffYT58+fj8ccfR2lpKR577DGcffbZ/fURBAQEBAQEBNIM/WYo7i8kY0gSEBAQEBAQSA8kc/8WZVsFBAQEBAQEBhQEuREQEBAQEBAYUBDkRkBAQEBAQGBAQZAbAQEBAQEBgQEFQW4EBAQEBAQEBhQEuREQEBAQEBAYUBDkRkBAQEBAQGBAQZAbAQEBAQEBgQEFQW4EBAQEBAQEBhT6tf1Cf4AVZG5ra+vnIxEQEBAQEBBIFOy+nUhjhYOO3LS3twMAysvL+/lIBAQEBAQEBJJFe3s7srKy4u5z0PWWCofD2LdvHzIyMiBJUkpfu62tDeXl5di9e7foW9UNxFglDjFWiUOMVXIQ45U4xFgljr4aK0II2tvbUVpaClmO76o56JQbWZZRVlbWp++RmZkpTv4EIcYqcYixShxirJKDGK/EIcYqcfTFWHWn2DAIQ7GAgICAgIDAgIIgNwICAgICAgIDCoLcpBA2mw1/+tOfYLPZ+vtQ0h5irBKHGKvEIcYqOYjxShxirBJHOozVQWcoFhAQEBAQEBjYEMqNgICAgICAwICCIDcCAgICAgICAwqC3AgICAgICAgMKAhyIyAgICAgIDCgIMhNivDEE09gyJAhsNvtqK6uxpIlS/r7kPodDz74IA499FBkZGSgsLAQZ555JjZt2qTbhxCCe+65B6WlpXA4HDj66KOxbt26fjri9MGDDz4ISZIwb948dZsYKz327t2Liy66CHl5eXA6nZg0aRJWrFihPi7GiyIYDOLOO+/EkCFD4HA4UFVVhfvuuw/hcFjd52Adq6+//hqnnXYaSktLIUkS3nnnHd3jiYyLz+fD9ddfj/z8fLhcLpx++unYs2fPAfwUBwbxxioQCOCWW27B+PHj4XK5UFpaitmzZ2Pfvn261zigY0UEeo3XXnuNWCwW8swzz5D169eTG2+8kbhcLrJz587+PrR+xYknnkheeOEFsnbtWrJ69Wpy6qmnkoqKCtLR0aHu89BDD5GMjAyycOFCsmbNGnLeeeeRkpIS0tbW1o9H3r/44YcfyODBg8mECRPIjTfeqG4XY6WhqamJVFZWkksvvZR8//33ZPv27eSzzz4jW7duVfcR40Vx//33k7y8PPLBBx+Q7du3k3//+9/E7XaTBQsWqPscrGP10UcfkTvuuIMsXLiQACBvv/227vFExmXOnDlk0KBBZNGiRWTlypXkmGOOIRMnTiTBYPAAf5q+RbyxamlpIccddxx5/fXXycaNG8l3331HDj/8cFJdXa17jQM5VoLcpACHHXYYmTNnjm7bqFGjyK233tpPR5SeqKurIwDIV199RQghJBwOk+LiYvLQQw+p+3R1dZGsrCzy1FNP9ddh9iva29vJ8OHDyaJFi8hRRx2lkhsxVnrccsstZPr06TEfF+Ol4dRTTyWXX365bttZZ51FLrroIkKIGCuGyBt2IuPS0tJCLBYLee2119R99u7dS2RZJh9//PEBO/YDDSMiGIkffviBAFAX+Qd6rERYqpfw+/1YsWIFTjjhBN32E044Ad9++20/HVV6orW1FQCQm5sLANi+fTtqa2t1Y2ez2XDUUUcdtGN33XXX4dRTT8Vxxx2n2y7GSo/33nsPU6ZMwTnnnIPCwkIccsgheOaZZ9THxXhpmD59Oj7//HNs3rwZAPDTTz9h6dKlOOWUUwCIsYqFRMZlxYoVCAQCun1KS0sxbty4g3rsADrfS5KE7OxsAAd+rA66xpmpRkNDA0KhEIqKinTbi4qKUFtb209HlX4ghOCmm27C9OnTMW7cOABQx8do7Hbu3HnAj7G/8dprr2HlypVYvnx51GNirPTYtm0bnnzySdx00024/fbb8cMPP+CGG26AzWbD7NmzxXhxuOWWW9Da2opRo0bBZDIhFArhgQcewAUXXABAnFuxkMi41NbWwmq1IicnJ2qfg3n+7+rqwq233or/+q//UhtnHuixEuQmRZAkSfc/ISRq28GMuXPn4ueff8bSpUujHhNjB+zevRs33ngjPv30U9jt9pj7ibGiCIfDmDJlCv785z8DAA455BCsW7cOTz75JGbPnq3uJ8YLeP311/GPf/wDr776KsaOHYvVq1dj3rx5KC0txSWXXKLuJ8bKGD0Zl4N57AKBAM4//3yEw2E88cQT3e7fV2MlwlK9RH5+PkwmUxTzrKuri2L8Byuuv/56vPfee/jiiy9QVlambi8uLgYAMXagkm1dXR2qq6thNpthNpvx1Vdf4bHHHoPZbFbHQ4wVRUlJCcaMGaPbNnr0aOzatQuAOLd4/OEPf8Ctt96K888/H+PHj8fFF1+M+fPn48EHHwQgxioWEhmX4uJi+P1+NDc3x9znYEIgEMC5556L7du3Y9GiRapqAxz4sRLkppewWq2orq7GokWLdNsXLVqEadOm9dNRpQcIIZg7dy7eeustLF68GEOGDNE9PmTIEBQXF+vGzu/346uvvjroxu7YY4/FmjVrsHr1avVnypQpuPDCC7F69WpUVVWJseJw5JFHRpUV2Lx5MyorKwGIc4uHx+OBLOunepPJpKaCi7EyRiLjUl1dDYvFotunpqYGa9euPejGjhGbLVu24LPPPkNeXp7u8QM+Vim3KB+EYKngzz33HFm/fj2ZN28ecblcZMeOHf19aP2Ka665hmRlZZEvv/yS1NTUqD8ej0fd56GHHiJZWVnkrbfeImvWrCEXXHDBQZGCmgj4bClCxFjx+OGHH4jZbCYPPPAA2bJlC/nnP/9JnE4n+cc//qHuI8aL4pJLLiGDBg1SU8Hfeustkp+fT/74xz+q+xysY9Xe3k5WrVpFVq1aRQCQhx9+mKxatUrN8ElkXObMmUPKysrIZ599RlauXElmzZo1IFPB441VIBAgp59+OikrKyOrV6/Wzfc+n099jQM5VoLcpAiPP/44qaysJFarlUyePFlNdz6YAcDw54UXXlD3CYfD5E9/+hMpLi4mNpuNzJw5k6xZs6b/DjqNEEluxFjp8f7775Nx48YRm81GRo0aRZ5++mnd42K8KNra2siNN95IKioqiN1uJ1VVVeSOO+7Q3XQO1rH64osvDOeoSy65hBCS2Lh4vV4yd+5ckpubSxwOB/nNb35Ddu3a1Q+fpm8Rb6y2b98ec77/4osv1Nc4kGMlEUJI6vUgAQEBAQEBAYH+gfDcCAgICAgICAwoCHIjICAgICAgMKAgyI2AgICAgIDAgIIgNwICAgICAgIDCoLcCAgICAgICAwoCHIjICAgICAgMKAgyI2AgICAgIDAgIIgNwICAgJpihdffBHZ2dn9fRgCAr86CHIjIPArgiRJcX8uvfTSHr/24MGDsWDBgoT2Y+/ncDgwePBgnHvuuVi8eHHS73nppZfizDPPTP5ge4lkP+uyZct02+fNm4ejjz66bw5OQECg1xDkRkDgV4Samhr1Z8GCBcjMzNRte/TRRw/Icdx3332oqanBpk2b8PLLLyM7OxvHHXccHnjggQPy/gcSdrsdt9xyS38fRkoRCAT6+xAEBPoUgtwICPyKUFxcrP5kZWVBkiTdtq+//hrV1dWw2+2oqqrCvffei2AwqD7/nnvuQUVFBWw2G0pLS3HDDTcAAI4++mjs3LkT8+fPV1WZeMjIyEBxcTEqKiowc+ZMPP3007jrrrtw9913q926Q6EQrrjiCgwZMgQOhwMjR47Uka977rkHL730Et599131Pb/88ksAwC233IIRI0bA6XSiqqoKd911l+6G/NNPP+GYY45BRkYGMjMzUV1djR9//FF9/Ntvv8XMmTPhcDhQXl6OG264AZ2dnT36rL///e+xbNkyfPTRRzH3OfroozFv3jzdtjPPPFOnpA0ePBj3338/Zs+eDbfbjcrKSrz77ruor6/HGWecAbfbjfHjx+s+B8M777yDESNGwG634/jjj8fu3bt1j7///vtxv3dJkvDUU0/hjDPOgMvlwv333x/3MwsI/NohyI2AwADBJ598gosuugg33HAD1q9fj7///e948cUXVTXlzTffxCOPPIK///3v2LJlC9555x2MHz8eAPDWW2+hrKxMVWRqamqSfv8bb7wRhBC8++67AIBwOIyysjK88cYbWL9+Pe6++27cfvvteOONNwAAN998M84991ycdNJJ6ntOmzYNACVPL774ItavX49HH30UzzzzDB555BH1vS688EKUlZVh+fLlWLFiBW699VZYLBYAwJo1a3DiiSfirLPOws8//4zXX38dS5cuxdy5c3v0WQcPHow5c+bgtttuQzgcTnpceDzyyCM48sgjsWrVKpx66qm4+OKLMXv2bFx00UVYuXIlhg0bhtmzZ4Nv+efxePDAAw/gpZdewjfffIO2tjacf/756uPdfe8Mf/rTn3DGGWdgzZo1uPzyy3v1OQQE0h590o5TQECgz/HCCy+QrKws9f8ZM2aQP//5z7p9XnnlFVJSUkIIIeRvf/sbGTFiBPH7/YavV1lZSR555JFu3zfefkVFReSaa66J+dxrr72WnH322er/l1xyCTnjjDO6fc//9//+H6murlb/z8jIIC+++KLhvhdffDG5+uqrdduWLFlCZFkmXq+328/Ag+1XV1dHMjIyyMsvv0wIIeTGG28kRx11lLpfZAd3Qgg544wz1O7S7LUuuugi9f+amhoCgNx1113qtu+++44AIDU1NYQQ+h0DIMuWLVP32bBhAwFAvv/+e0JI9987IYQAIPPmzev28woIDBQI5UZAYIBgxYoVuO++++B2u9Wfq666CjU1NfB4PDjnnHPg9XpRVVWFq666Cm+//bYudJEKEEJ0YZ6nnnoKU6ZMQUFBAdxuN5555hns2rWr29d58803MX36dBQXF8PtduOuu+7SPe+mm27ClVdeieOOOw4PPfQQfvnlF904vPjii7pxOPHEExEOh7F9+/Yefa6CggLcfPPNuPvuu+H3+3v0GgAwYcIE9e+ioiIAUNUzfltdXZ26zWw2Y8qUKer/o0aNQnZ2NjZs2ACg+++dgX8NAYGBDkFuBAQGCMLhMO69916sXr1a/VmzZg22bNkCu92O8vJybNq0CY8//jgcDgeuvfZazJw5M2Xm0sbGRtTX12PIkCEAgDfeeAPz58/H5Zdfjk8//RSrV6/GZZdd1i05WLZsGc4//3ycfPLJ+OCDD7Bq1Srccccduufdc889WLduHU499VQsXrwYY8aMwdtvv62Ow+9//3vdOPz000/YsmULhg4d2uPPd9NNN8Hr9eKJJ56IekyWZV0oCTA27bLQGQCVBBptiwx/GfmC+H3jfe8MLper288oIDBQYO7vAxAQEEgNJk+ejE2bNmHYsGEx93E4HDj99NNx+umn47rrrsOoUaOwZs0aTJ48GVarFaFQqMfv/+ijj0KWZTW1e8mSJZg2bRquvfZadR9eYQFg+J7ffPMNKisrcccdd6jbdu7cGfV+I0aMwIgRIzB//nxccMEFeOGFF/Db3/4WkydPxrp16+KOQ08+K1OQ7rnnHpx22mm6xwoKCnTenVAohLVr1+KYY45J6j2MEAwG8eOPP+Kwww4DAGzatAktLS0YNWoUgMS+dwGBgw1CuREQGCC4++678fLLL6uqxoYNG/D666/jzjvvBEALwj333HNYu3Yttm3bhldeeQUOhwOVlZUAqHH266+/xt69e9HQ0BD3vdrb21FbW4vdu3fj66+/xtVXX437778fDzzwgHqTHTZsGH788Ud88skn2Lx5M+666y4sX75c9zqDBw/Gzz//jE2bNqGhoQGBQADDhg3Drl278Nprr+GXX37BY489pqoyAOD1ejF37lx8+eWX2LlzJ7755hssX74co0ePBkAzrb777jtcd911WL16NbZs2YL33nsP119/ve59E/2sPK6++mpkZWXhX//6l277rFmz8OGHH+LDDz/Exo0bce2116KlpSXh140Hi8WC66+/Ht9//z1WrlyJyy67DFOnTlXJTnffu4DAQYn+Nv0ICAj0DJGGYkII+fjjj8m0adOIw+EgmZmZ5LDDDiNPP/00IYSQt99+mxx++OEkMzOTuFwuMnXqVPLZZ5+pz/3uu+/IhAkTiM1mI/GmhsrKSgKAACBWq5VUVFSQc889lyxevFi3X1dXF7n00ktJVlYWyc7OJtdccw259dZbycSJE9V96urqyPHHH0/cbjcBQL744gtCCCF/+MMfSF5eHnG73eS8884jjzzyiPpZfT4fOf/880l5eTmxWq2ktLSUzJ07VzULE0LIDz/8oL6uy+UiEyZMIA888ECPPmuk8fjVV18lAHSGYr/fT6655hqSm5tLCgsLyYMPPmhoKI58LQDk7bffVv/fvn07AUBWrVpFCNG+44ULF5KqqipitVrJrFmzyI4dO3SvE+97N3ofAYGBDomQiECxgICAgICAgMCvGCIsJSAgICAgIDCgIMiNgICAgICAwICCIDcCAgICAgICAwqC3AgICAgICAgMKAhyIyAgICAgIDCgIMiNgICAgICAwICCIDcCAgICAgICAwqC3AgICAgICAgMKAhyIyAgICAgIDCgIMiNgICAgICAwICCIDcCAgICAgICAwqC3AgICAgICAgMKPz/LNfcK5XX49AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test Results Visualization setting\n",
    "\n",
    "ParameterLabel = ['Threshold Voltage [V]', 'Saturation Mobility [cm^2/V-sec]', 'Subthreshold Slope [V/dec]', 'On/Off ratio']\n",
    "\n",
    "plt.figure(5)\n",
    "plt.title('RMSE : ' + str(np.round(RMSE_value, 4)) + '     ' +'MAPE : ' + str(np.round(MAPE_value, 4)))\n",
    "plt.plot(list(range(len(prediction))), test_result[:, 0], label='Predicted')\n",
    "plt.plot(list(range(len(prediction))), test_result[:, 1], label='Actual')\n",
    "plt.xlabel('Test Dataset Number')\n",
    "plt.ylabel(ParameterLabel[ParameterNum-301])\n",
    "plt.legend(loc = 'upper left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "aad17a85-89bc-410a-8624-3cdd4c9ab16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAHACAYAAADa59NNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABNnUlEQVR4nO3deViU9f4//ueIMCwCubGYqJSiiKEEmZTrl8S0cMnjafGUeTr+DmllkmXYqtWhjqbWcS/TlI62kGkfl4OpYBlmsuQCChqKeiBCUxBi2N6/PzxMDjPAfQ8zc98z83xcF9fl3MvMi1vk6fu+34tGCCFARETkhNopXQAREZFSGIJEROS0GIJEROS0GIJEROS0GIJEROS0GIJEROS0GIJEROS0GIJEROS0GIJEROS0GIJEROS0nDoEDxw4gLi4OHTr1g0ajQZfffWV7Pf47LPPMGjQIHh6eqJnz55YtGiR5QslIiKrcOoQrKysxMCBA7F8+XKzzt+1axemTp2K+Ph4HD9+HCtXrsSSJUvMfj8iIrItDSfQvk6j0WDr1q2YOHGifltNTQ1efvllfPLJJ7hy5QoGDBiAd955ByNHjgQAPPLII6itrcXnn3+uP2fZsmV49913UVRUBI1GY+PvgoiI5HDqlmBrpk+fjoMHD2LLli04evQopkyZgnvvvRcFBQUAAJ1OB3d3d4NzPDw8cOHCBZw7d06JkomISAaGYDPOnDmDzZs34/PPP8ewYcNw6623Yu7cuRg6dCjWr18PABgzZgy+/PJL7N27Fw0NDcjPz8eyZcsAAMXFxQpWT0REUrRXugC1ysrKghACISEhBtt1Oh06d+4MAJgxYwbOnDmD+++/H7W1tfDx8cHs2bPx+uuvw8XFRYmyiYhIBoZgMxoaGuDi4oLMzEyjQOvQoQOA688R33nnHfzjH/9ASUkJunbtir179wIAevXqZeuSiYhIJoZgMyIiIlBfX4/S0lIMGzasxWNdXFxw8803AwA2b96M6Oho+Pn52aJMIiJqA6cOwWvXruH06dP614WFhcjJyUGnTp0QEhKCqVOn4rHHHsO7776LiIgIlJWVYd++fbjtttswbtw4lJWV4YsvvsDIkSNRXV2N9evX4/PPP0d6erqC3xUREUnl1EMk0tLSMGrUKKPt06ZNw4YNG1BbW4s333wTGzduxMWLF9G5c2dER0djwYIFuO2221BWVoa4uDgcO3YMQghER0fjrbfewp133qnAd0NERHI5dQgSEZFz4xAJIiJyWgxBIiJyWk7XMaahoQH//e9/4e3tzWnNiIgckBACFRUV6NatG9q1a7mt53Qh+N///hdBQUFKl0FERFZ2/vx5dO/evcVjnC4Evb29AVy/OD4+PgpXQ0REUtReqcXxScdxLesa2ndsj9v+7zZ0GNDB5LHl5eUICgrS/75vidOFYOMtUB8fH4YgEZEdqL1Si4I/FUBkCfh29sWgfYPQIdx0AN5IyiMvdowhIiLVqr1Si6OxR1HxYwXad24vOQClYggSEZEqWTsAAYYgERGpkC0CEGAIEhGRytgqAAGGIBERqYgtAxBgCBIRkUrYOgABhiAREamAEgEIMASJiEhhSgUgwBAkIiIFKRmAAEOQiIgUonQAAgxBIiJSgBoCEHDCuUOJrKGqoAr1FfVG2128XeDZx1OBiojUSy0BCDAEidqsqqAKh0MON7t/cP5gBiHR/6gpAAHeDiVqM1MtQDn7iZyF2gIQYAgSEZENqDEAAYYgERFZmVoDEGAIEhGRFak5AAGGIBERWYnaAxBQUQgmJSVBo9Hg2WefbfG49PR0REZGwt3dHbfccgtWr15tmwKJmuHi7dKm/USOyB4CEFDJEIkff/wRa9euRXh4eIvHFRYWYty4cZgxYwaSk5Nx8OBBzJw5E127dsXkyZNtVC2RIc8+nhicP5jjBIn+x14CEFBBCF67dg1Tp07FBx98gDfffLPFY1evXo0ePXpg2bJlAIDQ0FAcOXIEixcvZgiSohh0RNfZUwACKrgdOmvWLNx333245557Wj02IyMDsbGxBtvGjBmDI0eOoLa21uQ5Op0O5eXlBl9ERGR59haAgMIhuGXLFmRlZSEpKUnS8SUlJfD39zfY5u/vj7q6OpSVlZk8JykpCb6+vvqvoKCgNtdNRESG7DEAAQVD8Pz585g9ezaSk5Ph7u4u+TyNRmPwWghhcnujxMREXL16Vf91/vx584smIiIj9hqAgILPBDMzM1FaWorIyEj9tvr6ehw4cADLly+HTqeDi4thr7qAgACUlJQYbCstLUX79u3RuXNnk5+j1Wqh1Wot/w0QEZFdByCgYAjGxMTg2LFjBtumT5+Ofv36Yd68eUYBCADR0dH4+uuvDbalpqYiKioKrq6uVq2XiIgM2XsAAgqGoLe3NwYMGGCwzcvLC507d9ZvT0xMxMWLF7Fx40YAQHx8PJYvX46EhATMmDEDGRkZWLduHTZv3mzz+omInJkjBCCggt6hLSkuLkZRUZH+dXBwMHbu3Im0tDQMGjQIb7zxBt5//30OjyAisiFHCUAA0IjGniVOory8HL6+vrh69Sp8fHyULoeIyK7YQwDK+T2v6pYgERGphz0EoFwMQSIiapUjBiDAECQiolY4agACDEEiImqBIwcgwBAkIqJmOHoAAgxBIiIywRkCEFDBUkpERM6iqqDKLtaddJYABBiCREQ2UVVQhcMhh5vdPzh/sCqC0JkCEODtUCIimzDVApSz3xacLQABhiAREcE5AxBgCBIROT1nDUCAIUhE5NScOQABhiARkdNy9gAEGIJERDbh4m28ULic/ZbGALyOQySIiGzAs48nBucPVsU4QQbgHxiCREQ2wnGA6sPboUREToIBaIwhSETkBBiApjEEiYgcHAOweXwmSIqylwmFiewVA7BlDEFSjL1MKExkrxiArePtUFKMPUwoTGSvGIDSMASJiBwMA1A6hiARkQNhAMrDECQichAMQPkYgkREDoABaB6GIClGbRMKE9krBqD5OESCFKOmCYWJ7BUDsG0YgqQoBh2R+RiAbcfboUREdogBaBkMQSIiO8MAtByGIBGRHWEAWpaiIbhq1SqEh4fDx8cHPj4+iI6Oxq5du5o9Pi0tDRqNxujr5MmTNqyaiEgZDEDLU7RjTPfu3fH222+jd+/eAICPP/4YEyZMQHZ2NsLCwpo979SpU/Dx8dG/7tq1q9VrJSJSEgPQOhQNwbi4OIPXb731FlatWoVDhw61GIJ+fn646aabrFwdEZE6MACtRzXPBOvr67FlyxZUVlYiOjq6xWMjIiIQGBiImJgY7N+/v8VjdTodysvLDb6IiOwFA9C6FB8neOzYMURHR6O6uhodOnTA1q1b0b9/f5PHBgYGYu3atYiMjIROp8OmTZsQExODtLQ0DB8+3OQ5SUlJWLBggTW/BbIBLr5LzogBaH0aIYRQsoCamhoUFRXhypUrSElJwYcffoj09PRmg7CpuLg4aDQabN++3eR+nU4HnU6nf11eXo6goCBcvXrV4LkiqRcX3yVnxAA0X3l5OXx9fSX9nle8Jejm5qbvGBMVFYUff/wR7733HtasWSPp/CFDhiA5ObnZ/VqtFlqt1iK1kjK4+C45Gwag7ajmmWAjIYRBy6012dnZCAwMtGJFRES2wwC0LUVbgvPnz8fYsWMRFBSEiooKbNmyBWlpadi9ezcAIDExERcvXsTGjRsBAMuWLUOvXr0QFhaGmpoaJCcnIyUlBSkpKUp+G0REFsEAtD1FQ/CXX37Bo48+iuLiYvj6+iI8PBy7d+/G6NGjAQDFxcUoKirSH19TU4O5c+fi4sWL8PDwQFhYGHbs2IFx48Yp9S0QEVkEA1AZineMsTU5D0xJHSqyKpAZmdns/sjMSHjf7m3DiogsiwFoWXJ+z6vumSBRU1x8lxwZA1BZivcOJWoNF98lR8UAVB5DkOwCg44cDQNQHXg7lIjIxhiA6sEQJCKyIQagujAEiYhshAGoPgxBIiIbYACqE0OQiMjKGIDqxRAkIrIiBqC6MQSJiKyEAah+DEEiIitgANoHhiARkYUxAO0HQ5CIyIIYgPaFIUhEZCEMQPvDECQisgAGoH3iBNpkc1UFVVwRghwKA9B+MQTJpqoKqnA45HCz+wfnD2YQkl1hANo33g4lmzLVApSzn0hNGID2jyFIRGQGBqBjYAgSEcnEAHQcDEEiIhkYgI6FIUhEJBED0PEwBMmmXLxd2rSfSCkMQMfEIRJkU559PDE4fzDHCZJdYQA6LoYg2RyDjuwJA9Cx8XYoEVEzGICOjyFIRGQCA9A5MASJiJpgADoPPhMkIlVQy8TqDEDnwhAkIsWpZWJ1BqDz4e1QIlKcGiZWZwA6J0ktwYSEBMlvuGTJErOLISJSAgPQeUkKwezsbIPXmZmZqK+vR9++fQEA+fn5cHFxQWRkpKwPX7VqFVatWoWzZ88CAMLCwvDqq69i7NixzZ6Tnp6OhIQEnDhxAt26dcMLL7yA+Ph4WZ9L9kMtz4nIcTEAnZukENy/f7/+z0uWLIG3tzc+/vhjdOzYEQDw22+/Yfr06Rg2bJisD+/evTvefvtt9O7dGwDw8ccfY8KECcjOzkZYWJjR8YWFhRg3bhxmzJiB5ORkHDx4EDNnzkTXrl0xefJkWZ9N6qeW50TkuBiApBFCCDkn3HzzzUhNTTUKqePHjyM2Nhb//e9/21RQp06dsGjRIjzxxBNG++bNm4ft27cjLy9Pvy0+Ph4//fQTMjIyJL1/eXk5fH19cfXqVfj4+LSpVrKuiqwKZEZmNrs/MjMS3rd727AishYl/q4ZgI5Lzu952b1Dy8vL8csvvxiFYGlpKSoqKuS+nV59fT0+//xzVFZWIjo62uQxGRkZiI2NNdg2ZswYrFu3DrW1tXB1dTU6R6fTQafTGdRPpHbOdhvY1hOrMwCpkewQnDRpEqZPn453330XQ4YMAQAcOnQIzz//PB544AHZBRw7dgzR0dGorq5Ghw4dsHXrVvTv39/ksSUlJfD39zfY5u/vj7q6OpSVlSEwMNDonKSkJCxYsEB2XURKccbbwLacWJ0BSDeSHYKrV6/G3Llz8Ze//AW1tbXX36R9ezzxxBNYtGiR7AL69u2LnJwcXLlyBSkpKZg2bRrS09ObDUKNRmPwuvFubtPtjRITEw16t5aXlyMoKEh2nUS2oobhAkrgOEBSguwQ9PT0xMqVK7Fo0SKcOXMGQgj07t0bXl5eZhXg5uam7xgTFRWFH3/8Ee+99x7WrFljdGxAQABKSkoMtpWWlqJ9+/bo3LmzyffXarXQarVm1UZEjoMBSKaYPVi+uLgYxcXFCAkJgZeXF2T2r2mWEMLgGd6NoqOjsWfPHoNtqampiIqKMvk8kOwbF+AlS2EAUnNktwQvXbqEP//5z9i/fz80Gg0KCgpwyy234G9/+xtuuukmvPvuu5Lfa/78+Rg7diyCgoJQUVGBLVu2IC0tDbt37wZw/VbmxYsXsXHjRgDXe4IuX74cCQkJmDFjBjIyMrBu3Tps3rxZ7rdBdoAL8JIlMACpJbJDcM6cOXB1dUVRURFCQ0P12x988EHMmTNHVgj+8ssvePTRR1FcXAxfX1+Eh4dj9+7dGD16NIDrrc2ioiL98cHBwdi5cyfmzJmDFStWoFu3bnj//fc5RtCBMeioLRiA1BrZIZiamor//Oc/6N69u8H2Pn364Ny5c7Lea926dS3u37Bhg9G2ESNGICsrS9bnENkT3ga2DAYgSSE7BCsrK+Hpafy/87KyMnZAIbIA3gZuOwYgSSU7BIcPH46NGzfijTfeAHB9aEJDQwMWLVqEUaNGWbxAImfEoDMfA5DkkB2CixYtwsiRI3HkyBHU1NTghRdewIkTJ3D58mUcPHjQGjUSEUnCACS5ZA+R6N+/P44ePYrBgwdj9OjRqKysxAMPPIDs7Gzceuut1qiRiKhVDEAyh+wJtIuKihAUFGRyhpaioiL06NHDYsVZAyfQJnI8DEC6kZzf87JbgsHBwfj111+Ntl+6dAnBwcFy346IqE0YgNQWskNQCGGyFXjt2jW4u7tbpCgiIikYgNRWkjvGNE5CrdFo8MorrxgMk6ivr8cPP/yAQYMGWbxAIiJTGIBkCZJDMDs7G8D1luCxY8fg5uam3+fm5oaBAwdi7ty5lq+QqAXOtu4eXccAJEuRHIL79+8HAEyfPh3vvfceO5WQ4pxx3T1iAJJlyX4muGzZMtTV1Rltv3z5MldtJ5ty1nX3nBkDkCxNdgg+9NBD2LJli9H2zz77DA899JBFiiIiaooBSNYgOwR/+OEHk9OjjRw5Ej/88INFiiIiuhEDkKxFdgjqdDqTt0Nra2vx+++/W6QoIqJGDECyJtkheMcdd2Dt2rVG21evXo3IyEiLFEVEBDAAyfpkT6D91ltv4Z577sFPP/2EmJgYAMDevXvx448/IjU11eIFEjWH6+45NgYg2YLsuUMBICcnB4sWLUJOTg48PDwQHh6OxMRE9OnTxxo1WhTnDnUsHCfomBiA1BZyfs+bFYL2jCFIpG4MQGorOb/nJd0OLS8v179Ra2MBGSzk6Nj6tB4GINmapBDs2LEjiouL4efnh5tuusnkBNqNE2vX13OAMjkuzlJjPQxAUoKkENy3bx86deoE4I/p04icEWepsQ4GIClFUgiOGDHC5J+JHA1vddoeA5CUJCkEjx49KvkNw8PDzS6GSElSbnWSZTEASWmSQnDQoEHQaDTNLqh7Iz4TJHvFW522xQAkNZA0Y0xhYSF+/vlnFBYWIiUlBcHBwVi5ciWys7ORnZ2NlStX4tZbb0VKSoq16yUiB8AAJLWQ1BLs2bOn/s9TpkzB+++/j3Hjxum3hYeHIygoCK+88gomTpxo8SKJ1IKz1LQdA5DURPa0aceOHUNwcLDR9uDgYOTm5lqkKCK18uzjicH5g9l5xkwMQFIb2SEYGhqKN998E+vWrYO7uzuA6ytLvPnmmwgNDbV4gURqw6AzDwOQ1Eh2CK5evRpxcXEICgrCwIEDAQA//fQTNBoN/u///s/iBRLZCm91Wg8DkNTKrLlDq6qqkJycjJMnT0IIgf79++ORRx6Bl5eXNWq0KM4dSi3hOEHLYwCSrXEC7RYwBIlshwFISpDze172oroAsGnTJgwdOhTdunXDuXPnAABLly7Ftm3bzHk7InJADECyB7JDcNWqVUhISMDYsWPx22+/6QfHd+zYEcuWLZP1XklJSbjjjjvg7e0NPz8/TJw4EadOnWrxnLS0NGg0GqOvkydPyv1WiMhKGIBkL2SH4L/+9S988MEHeOmll9C+/R/9aqKionDs2DFZ75Weno5Zs2bh0KFD2LNnD+rq6hAbG4vKyspWzz116hSKi4v1X/awoC+RM2AAkj2R3Tu0sLAQERERRtu1Wq2k8LrR7t27DV6vX78efn5+yMzMxPDhw1s8t3FZJyJSDwYg2RvZIRgcHIycnByDWWQAYNeuXejfv3+birl69SoA6JdtaklERASqq6vRv39/vPzyyxg1apTJ43Q6HXQ6nf51a4sCkzTm9qJk70vHxQAkeyQ7BJ9//nnMmjUL1dXVEELg8OHD2Lx5M5KSkvDhhx+aXYgQAgkJCRg6dCgGDBjQ7HGBgYFYu3YtIiMjodPpsGnTJsTExCAtLc1k6zEpKQkLFiwwuy4yZu7CslyQ1nExAMlemTVE4oMPPsCbb76J8+fPAwBuvvlmvP7663jiiSfMLmTWrFnYsWMHvvvuO3Tv3l3WuXFxcdBoNNi+fbvRPlMtwaCgIA6RaIOKrApkRmY2uz8yMxLet3tb7DxSNwYgqY2cIRKyWoJ1dXX45JNPEBcXhxkzZqCsrAwNDQ3w8/NrU8FPP/00tm/fjgMHDsgOQAAYMmQIkpOTTe7TarXQarVtqo+ITGMAkr2T1Tu0ffv2ePLJJ/Utqy5durQpAIUQeOqpp/Dll19i3759JifmliI7OxuBgYFm10FE8jEAyRHIfiZ45513Ijs726hjjDlmzZqFf//739i2bRu8vb1RUlICAPD19YWHhwcAIDExERcvXsTGjRsBAMuWLUOvXr0QFhaGmpoaJCcnIyUlhWsZEtkQA5AchewQnDlzJp577jlcuHABkZGRRvOFhoeHS36vVatWAQBGjhxpsH39+vV4/PHHAQDFxcUoKirS76upqcHcuXNx8eJFeHh4ICwsDDt27DBY35CIrIcBSI5EdseYdu2M76BqNBoIIaDRaPQzyKgV5w5tO/YOdV4MQLIHVusYA1wfLE/OzdyFZbkgrX1jAJIjkhWCFRUVyM/PR21tLQYPHowuXbpYqy5SOXMDi0FnnxiA5Kgkh+DRo0cxduxYlJSUQAgBHx8ffPHFF7jnnnusWR8RKYwBSI5M8hCJF198ET169MC3336LI0eOYMSIEXjqqaesWRsRKYwBSI5OckvwyJEj2LlzJ6KiogAAH330Efz8/HDt2jV06MB/FPaA83aqn5r+jhiA5Awkh2BZWRl69Oihf925c2d4enri119/ZQjaAfbMVD81/R0xAMlZSA5BjUaDiooKuLu7A4B+SERFRYXBygwcdqBOploXcvaT9anl74gBSM5EcggKIRASEmK0rXFtQXsZJ0hEzWMAkrORHIL79++3Zh3k4NT0rItMYwCSM5IcgiNGjLBmHeTArP2siwHbdgxAclayZ4whksuaz7rU1JnEXjEAyZnJWkqJ7JeLt0ub9quVWjqTWIISf0cMQHJ2bAk6Cc7bqX62/jtiABIxBJ0Kg079OA6QyLbadDt08+bNqKystFQtRGQDDECiP7QpBP/+97/jl19+sVQt5KAc9XmkPWIAEhlq0+1QmevxkpOy5rMuBqx0DEAiY3wmSGaROzbPWs+62OFHGgYgkWltCsFdu3bh5ptvtlQtZCfUNjaPQdcyBiBR89oUgkOHDrVUHU7PnmY9caSxeY6OAUjUMt4OVQG1tazIMTAAiVrHGWNUgC0rsjQGIJE0bAkS2Zi1b30zAImkYwiS1djTc05bsfatbwYgkTxmhWB6ejoWL16MvLw8aDQahIaG4vnnn8ewYcMsXR+pkJSxeXzOaZo1b30zAInkkx2CycnJmD59Oh544AE888wzEELg+++/R0xMDDZs2IBHHnnEGnWSikgZm1eRVdHie/A5p2UxAInMIzsE33rrLfzzn//EnDlz9Ntmz56NJUuW4I033mAImsEeZz1xxlacWjEAicwnOwR//vlnxMXFGW0fP3485s+fb5GinA1nPSFzMQCJ2kZ2CAYFBWHv3r3o3bu3wfa9e/ciKCjIYoU5G0cLuuqiaqVLcHgMQKK2kx2Czz33HJ555hnk5OTgrrvugkajwXfffYcNGzbgvffes0aNZGeqCqpwYtIJpctQJUvd+mYAElmG7BB88sknERAQgHfffRefffYZACA0NBSffvopJkyYYPECyf5I6fSi5HNOJYduWOLWNwOQyHLMGiIxadIkTJo0ydK1kJMI2xqm2O1fNQzd4DhAIvVQdNq0pKQk3HHHHfD29oafnx8mTpyIU6dOtXpeeno6IiMj4e7ujltuuQWrV6+2QbVkKe493BX7bHueoo4BSGR5kkKwU6dOKCsrAwB07NgRnTp1avZLjvT0dMyaNQuHDh3Cnj17UFdXh9jYWFRWVjZ7TmFhIcaNG4dhw4YhOzsb8+fPxzPPPIOUlBRZn01kTxiARNYh6Xbo0qVL4e3trf+zRqOxyIfv3r3b4PX69evh5+eHzMxMDB8+3OQ5q1evRo8ePbBs2TIA159HHjlyBIsXL8bkyZMtUheRmjAAiaxHUghOmzZN/+fHH3/cWrXg6tWrANBiizIjIwOxsbEG28aMGYN169ahtrYWrq6uVquPpLHHwf9qxQAksi7ZHWNcXFxQXFwMPz8/g+2XLl2Cn58f6uvNe6YihEBCQgKGDh2KAQMGNHtcSUkJ/P39Dbb5+/ujrq4OZWVlCAwMNNin0+mg0+n0r8vLy82qj6Tj4H/LYAASWZ/sEBRCmNyu0+ng5uZmdiFPPfUUjh49iu+++67VY5vejm2sydRt2qSkJCxYsMDsusg8ag06e2mlMgCJbENyCL7//vsArgfNhx9+iA4d/vgHWV9fjwMHDqBfv35mFfH0009j+/btOHDgALp3797isQEBASgpKTHYVlpaivbt26Nz585GxycmJiIhIUH/ury8nDPbODF7aKUyAIlsR3IILl26FMD1Vtfq1avh4vLH/5jd3NzQq1cv2UMVhBB4+umnsXXrVqSlpSE4OLjVc6Kjo/H1118bbEtNTUVUVJTJ54FarRZarVZWXeTY1BB0zWEAEtmW5BAsLCwEAIwaNQpffvklOnbs2OYPnzVrFv79739j27Zt8Pb21rfwfH194eHhAeB6S+7ixYvYuHEjACA+Ph7Lly9HQkICZsyYgYyMDKxbtw6bN29ucz3UMlMzrTTOEdp07J9aWlX2hAFIZHsa0dxDPlt8eDNDLdavX6/vhfr444/j7NmzSEtL0+9PT0/HnDlzcOLECXTr1g3z5s1DfHy8pM8sLy+Hr68vrl69Ch8fn7Z+C06jtZlWTHHWhXPNwQAkshw5v+fNmjbtwoUL2L59O4qKilBTU2Owb8mSJZLfR0r+btiwwWjbiBEjkJWVJflzqO3MmUlFzbOvqAkDkEg5skNw7969GD9+PIKDg3Hq1CkMGDAAZ8+ehRACt99+uzVqJHJYDEAiZckOwcTERDz33HNYuHAhvL29kZKSAj8/P0ydOhX33nuvNWokB6LkCg5qwwAkUp7sEMzLy9N3Qmnfvj1+//13dOjQAQsXLsSECRPw5JNPWrxIcgxqWMFBLRiAROogexUJLy8v/Qws3bp1w5kzZ/T7GifZJjLFnldwsCQGIJF6yG4JDhkyBAcPHkT//v1x33334bnnnsOxY8fw5ZdfYsiQIdaokVTAnJlU1DL7ipowAInURXYILlmyBNeuXQMAvP7667h27Ro+/fRT9O7dWz+gnhzPjTOtVOVVIe8vec0eG5ocCu/B3k5za1MqBiCR+sgKwfr6epw/fx7h4eEAAE9PT6xcudIqhZH6SA01z1BPBmATDEAidZL1TNDFxQVjxozBlStXrFQO2QN7mYRaLRiAROol+3bobbfdhp9//lnSPJ+kLGsNRzB3EmpnDE8GIJG6yZ42LTU1FfPmzcMbb7yByMhIeHl5GexX+1RkzjJtmlqHIzjTOEEGIJEyrDptWuOA+PHjxxvM/SmEgEajMXtRXbIstQ5HcLSgaw4DkMg+yA7B/fv3W6MOsrDG1R3I9hiARPZDdgiOGDHCGnWQBVUVVOHEpBOKfr6z3PJsigFIZF/MWkWC1M2atzpbCzi1Pou0BQYgkf1hCJJkUgJOrc8irY0BSGSfZM8dSo7BnOEIzhpwrWEAEtkvSSG4fft21NbWWrsWspGwrWEOe0vS1hiARPZNUghOmjRJP0uMi4sLSktLrVkTWZl7D3elS3AIDEAi+yfpmWDXrl1x6NAhxMXF6ccDkvW0tXelM87MYmsMQCLHICkE4+PjMWHCBGg0Gmg0GgQEBDR7LAfLt40leleaO62ZJThDADMAiRyHpBB8/fXX8dBDD+H06dMYP3481q9fj5tuusnKpTknS3U+sUbQSQk4JQPYFhiARI5FUghu374dY8eORb9+/fDaa69hypQp8PS0719mJJ/UgLP3oGsOA5DI8UiaQNvFxQUlJSXo2rUrXFxcUFxcDD8/P1vUZ3Fqn0C7IqsCmZGZze6PzIyE9+3eNqyIAAYgkT2R83teUu/Qxo4xANgxhpwOA5DIcbFjDFELGIBEjo0dY1TGGXpX2gsGIJHjkzx3aL9+/dgxxgYcvXelvWAAEjkH2SvLA8DRo0eRn58PjUaDPn36IDw83Bq1WYXaO8aQ8hiARPbNaivLHz58GE888QRyc3PRmJ0ajQZhYWFYt24d7rjjDvOrJlIBBiCRc5G8ikRubi5iYmLg4eGB5ORkZGVlITMzE5s2bYJWq0VMTAxyc3OtWSuRVTEAiZyP5NuhU6ZMQX19PVJSUoyGSAgh8MADD8DV1RWfffaZVQq1FN4OJVMYgESOwyq3Q9PS0rBr1y6TYwQ1Gg3mz5+PcePGya+WSGEMQCLnJfl2aEVFBfz9/ZvdHxAQgIqKClkffuDAAcTFxaFbt27QaDT46quvWjw+LS1NP1bxxq+TJ0/K+lyiRgxAIucmOQR79eqFw4ebX93ghx9+QM+ePWV9eGVlJQYOHIjly5fLOu/UqVMoLi7Wf/Xp00fW+UQAA5CIZNwOffDBB5GQkIC+fftiwIABBvuOHTuGuXPnYtq0abI+fOzYsRg7dqyscwDAz8+Pg/WpTRiARATICMHExER88803GDRoEEaPHo3Q0FAA13uNfvPNNxg8eDASExOtVuiNIiIiUF1djf79++Pll1/GqFGjbPK5zqqti/yqDQOQiBpJDkF3d3fs378fS5cuxebNm5Geng4ACAkJwZtvvok5c+ZAq9VarVAACAwMxNq1axEZGQmdTodNmzYhJiYGaWlpGD58uMlzdDoddDqd/nV5eblVa3Q0lljkV00YgER0I8lDJPLz8xESEmK9QjQabN26FRMnTpR1XlxcHDQaDbZv325y/+uvv44FCxYYbecQCWkcaWknBiCRc7D4UkrA9VuQoaGhmDdvHjIyMtpcpKUMGTIEBQUFze5PTEzE1atX9V/nz5+3YXWkFgxAIjJF8u3QS5cuYc+ePdi2bRsmTZoEIQTuv/9+TJgwAbGxsXB3d7dmnc3Kzs5GYGBgs/u1Wq3Vb9OSujEAiag5sp4JxsXFIS4uDkIIZGRkYPv27XjxxRfx8MMP45577sGECRNw//33S151/tq1azh9+rT+dWFhIXJyctCpUyf06NEDiYmJuHjxIjZu3AgAWLZsGXr16oWwsDDU1NQgOTkZKSkpSElJkfltk6VU5VUBUG8nGQYgEbVE8u3QG2k0Gtx11114++23kZubi5ycHAwfPhwbNmxAUFAQVqxYIel9jhw5goiICERERAAAEhISEBERgVdffRUAUFxcjKKiIv3xNTU1mDt3LsLDwzFs2DB899132LFjBx544AFzvg2ygLy/5CEzMhOHQw6jqqBK6XIMMACJqDVmLaXUkkuXLuHy5cuqHcCu1rlDpQ5DsPVwhdZ6h95ITZ1kGIBEzstqSyk1ys/PR1paGkpLS9HQ0KDfrtFo8Morr6Bz587mvK3TkjoMQYnhCjcu8luVV4W8v+RZ9P2tgQFIRFLJDsEPPvgATz75JLp06YKAgACDCbUbQ5DkMdWyM7Vf6nGWpsZnfc1hABKRHLJD8M0338Rbb72FefPmWaMeIrMxAIlILtkh+Ntvv2HKlCnWqIUckK2eYTIAicgcskNwypQpSE1NRXx8vDXqIRVz8XaRtd9WzzAZgERkLtkh2Lt3b7zyyis4dOgQbrvtNri6uhrsf+aZZyxWHKnLjZ1kmjLVsrPFM0wGIBG1hewQXLt2LTp06ID09HT9JNqNNBoNQ9AMUltYcltizWnLLUo1dZJhABJRW8kOwcLCQmvU4dSktrDktsRMcZRVIRiARGQJZo0TBICysjJoNBqOCbQQqcHT1oBSapiFJTEAichSZE2bduXKFcyaNQtdunSBv78//Pz80KVLFzz11FO4cuWKlUok+gMDkIgsSXJL8PLly4iOjsbFixcxdepUhIaGQgiBvLw8bNiwAXv37sX333+Pjh07WrNesiOWeobZiAFIRJYmOQQXLlwINzc3nDlzBv7+/kb7YmNjsXDhQixdutTiRZJ9ssQzzEYMQCKyBskh+NVXX2HNmjVGAQgAAQEB+Oc//4n4+HiGoBXZevJsS+A4QCJSM8khWFxcjLCwsGb3DxgwACUlJRYpioxZqlenpW9RWhsDkIisSXIIdunSBWfPnkX37t1N7i8sLGRPUSuyVK9OS96itDYGIBFZm+QQvPfee/HSSy9hz549cHNzM9in0+nwyiuv4N5777V4gY5KyVubLb2/Wm65MgCJyBYkh+CCBQsQFRWFPn36YNasWejXrx8AIDc3FytXroROp8OmTZusVqgjUeuAdbXUxQAkIluRHILdu3dHRkYGZs6cicTERDQuSK/RaDB69GgsX74cQUFBVivUkah1wLoa6mIAEpEtyZoxJjg4GLt27cJvv/2GgoICANcn1O7UqZNViiPnwgAkIlsza9q0jh07YvDgwZauhVpgb7065WIAEpESzJ47lGzLnnp1ysUAJCKlMATtiNJBd2nnJVQXVcO9h7t+W1sDmAFIREpiCCpArbc2W/vcs6+cNbnd3F6jDEAiUhpDUAFqvbXZWFfF4Qrk/SVP8nkVhyv034vU+hmARKQGDEGFKH1rszmefTxlD4VoGpittQwZgESkFgxBJ3N5z2XUltYabXf1c0Wn0ZYZ6tJSiDIAiUhNGIJ2yNypzS7vuYyjsUeb3R+eGm6xIDSFAUhEasMQtDNtmdrMVAtQzv62YAASkRq1U7oAkscWU5tZuncqA5CI1IotQTJiqvdqdVE1Giob0M6rHRoqGyT3HmUAEpGaMQQVIPWZnqnjqvKqrF4fYNx71ft2b4O6WtLYkmQAEpHaMQRtTOozvdaOs4bfC39HRVZFqx1spIxzZAASkT1Q9JnggQMHEBcXh27dukGj0eCrr75q9Zz09HRERkbC3d0dt9xyC1avXm39Qi1I6jM9c5/tVRdVN7vP1c+1xXPPvnIWmZGZOBxyuNXWnmcfT3jf7m305YgBWFVQhYqsCqOv1q4REamfoi3ByspKDBw4ENOnT8fkyZNbPb6wsBDjxo3DjBkzkJycjIMHD2LmzJno2rWrpPOdXafRnRCeGo7a0lr8Xvh7s9OgAeaHsCMGoBoWGiYi61A0BMeOHYuxY8dKPn716tXo0aMHli1bBgAIDQ3FkSNHsHjxYtWFYHPP/VpqqVnCjZNbm9I4DrAiq6LFEDSHowUgoI6FhonIeuzqmWBGRgZiY2MNto0ZMwbr1q1DbW0tXF1bvt1nK0o8z2uujuae21maIwYgETk+uwrBkpIS+Pv7G2zz9/dHXV0dysrKEBgYaHSOTqeDTqfTvy4vL7d6nWpoHbQWxGFbw1o+v0kv1JY6yzAAiche2VUIAoBGozF4LYQwub1RUlISFixYYPW6LKWxldbW1lprQVx9puXbsqbGAZp6/sUAJCJ7ZlchGBAQgJKSEoNtpaWlaN++PTp37mzynMTERCQkJOhfl5eXIygoyKp1tiZsa5jJZ3c3trZMDUOoyqtqdZC61PA8M/eMjIqvaxqsDEAisnd2FYLR0dH4+uuvDbalpqYiKiqq2eeBWq0WWq3WFuVJ5t7D3WDweXPk9joM2xpms56KzhKAal0AmYgsQ9EQvHbtGk6fPq1/XVhYiJycHHTq1Ak9evRAYmIiLl68iI0bNwIA4uPjsXz5ciQkJGDGjBnIyMjAunXrsHnzZqW+BVVprWeopThLAALqXQCZiCxD0RA8cuQIRo0apX/deNty2rRp2LBhA4qLi1FUVKTfHxwcjJ07d2LOnDlYsWIFunXrhvfff191wyMcufXgTAHYiEFH5LgUDcGRI0fqO7aYsmHDBqNtI0aMQFZWlhWrajs1tB6sEbR1FXVOF4BE5Njs6pmgPbFG0MlpYbYUxNVF1Tgx6YTszz/99GlUHqtkABKRw2AI2hG5Lczmgri1MO29ojd8h/jqX9dV1FktAKWuqEFEZA0a0dL9SAdUXl4OX19fXL16FT4+PkqXI1lzYVF7qRaunY17xrYWIpf3XMbR2KPN7m8cE2jNZ4Ccl5OIrEHO73m2BO2AudOwDc4fDMD0wPn6ytbnxLR2JxjOy0lESmMI2gFzw6DyRKVZz/4AdoIhIufAEHRgDZUNZp9bMKsAVSeqGIBE5NAYgmRS1YkqtL+pPfqs6ANRJ1CRVaHfx04rROQoGIJkkou3C+qu1CHvIdNzlbLTChE5AoaggtQ8PKDXwl44M6f5SbYt0WnFkWfWISL7wBBUiJzhAeaGQTuvdpKOC1kTggvvX0DViSq4+LogdFMotDdbf9JxNcysQ0TOjSGoEDnDA24Mi9aWU7p18a1wC3CTHIAA9AF4YyeYG58BWhODjoiUxBC0E1LDwpx1AtkLlIiclfTmAqmCNZ6Tufi6MACJyCmxJaiQ6qJqs86zxsTYoZtCjQKQnVaIyBkwBC1MSo/PqoIqs2dyaen9W1tUNzQ5FG7d3fQD4Rs7wXSJ62J0LDutEJEzYAhakNQen1KGF5hqabX2/mFbw1p8T7fubvj5+Z8lPwNk0BGRo2MIWpClJoQO2xpmMoBaO7+1adI4FRoRkSF2jFGh1m5rNqe1YREMQCIiQ2wJOhD3Hu5Gz/HqKurYAiQiagZD0MHceBu1cT1ABiARkWkMQQXYYvhB0wVx+63vZ7QaRONnsQMMETkrhqAFSQ03c4cfSH1/UwF4fPzxZs/jihBE5KwYghYkJ9zMCR0p7980AAftGwRRJ1p8X0usCEFEZI8YghZm7RZVS+9vKgBtORk2EZG94RAJB9FcABIRUfPYEnQArXWCqcqrUrhCIiJ1YgjaObmdYIiI6A8MQTtmTicYU7giBBE5K4agnTK3E0xocig8Q//oXMNxgkTkzBiCdqgtnWA8Qz3hfbu3lSskIrIPDEE7Y81eoFLWQiQiciQMQTvSUgA2Bpi5PUGlroVIRORIFA/BlStXYtGiRSguLkZYWBiWLVuGYcOGmTw2LS0No0aNMtqel5eHfv36WbtUSeS0puQc21oAthRgTd/bFEuthUhEZE8UDcFPP/0Uzz77LFauXIm7774ba9aswdixY5Gbm4sePXo0e96pU6fg4+Ojf921a1dblNsqOa0pOce2dgu0tYBq7AzD25pERIYUnTFmyZIleOKJJ/C3v/0NoaGhWLZsGYKCgrBq1aoWz/Pz80NAQID+y8VFHV385bSmpB5riWeAjZ1hGIBERIYUC8GamhpkZmYiNjbWYHtsbCy+//77Fs+NiIhAYGAgYmJisH//fmuWqShOhUZEZF2K3Q4tKytDfX09/P39Dbb7+/ujpKTE5DmBgYFYu3YtIiMjodPpsGnTJsTExCAtLQ3Dhw83eY5Op4NOp9O/Li8vt9w3YUV1FXUMQCIiK1O8Y4xGozF4LYQw2taob9++6Nu3r/51dHQ0zp8/j8WLFzcbgklJSViwYIHlCraQ6qLqFveffPQkdOd1cPF1QZ8VfdDOw7qNdlss9EtEpDaKhWCXLl3g4uJi1OorLS01ah22ZMiQIUhOTm52f2JiIhISEvSvy8vLERQUJL9gC2uobGhxv+789dZr/dV65D2UB6D5YQqWCDBzF/olIrJnioWgm5sbIiMjsWfPHkyaNEm/fc+ePZgwYYLk98nOzkZgYGCz+7VaLbRabZtqlUpOGLXzkt+ya64zjaUCjEFHRM5G0duhCQkJePTRRxEVFYXo6GisXbsWRUVFiI+PB3C9FXfx4kVs3LgRALBs2TL06tULYWFhqKmpQXJyMlJSUpCSkqLkt6EnJ4zce7hbpYbqomqDVmY7r3aor6hna46IyARFQ/DBBx/EpUuXsHDhQhQXF2PAgAHYuXMnevbsCQAoLi5GUVGR/viamhrMnTsXFy9ehIeHB8LCwrBjxw6MGzdOqW/BiBJBI3WwfHhqODqN7mSDioiI7INGCCF/7R07Vl5eDl9fX1y9etVgwL2tVWRVIDMyU9Y5kZmR8L7d22immaq8KuT9JU/Se3D6MyJydHJ+zyveO9RZmdvbUs4UaaZw+jMioj8oOmOMM3Pt6gqv27wAAC4dXBD8j2DcuvjWFs9x8XZhiBERWRBbgjZw4+3L6qJq1JbWouidIlT/fH2sYP21ehTOLzQ6L2xrmL4DTWPHltYWzSUiIukYglbWltuX7j3cLb4AbnVRNRfVJSL6H4aglbXl9mXTtQEtMWvLiUkn9C1MDpsgImfHEFQxUz0+w7aGtfl9T0w6of8ze4sSkTNjxxgnx442ROTM2BK0srqKOou+X0NlA8K2hhnMClN7tRanZ5226OcQETkDhqAV1V6pRcGsAou+Z3OD4sNTw1FfWY+Gygb8Xvg7zr5y1qKfS0TkiBiCVtK4IG7ViarWD7aA+sp6g2d9RETUOoagFTRdEb7f+n7Q3nx9JYsbJ7hu59UODZUNkqc8a0lrSzMREZExhqCFNQ3ApivCNx2jp/Tgdy6WS0TOjCFoQa0FYNOJrwHjsYCmNHaEaanFWFNS0+J7hCaHwjPUcCgExwkSkbNjCFqIlAA0Z+aYxuWPWmsxnpl7psX9nqGenCmGiKgJhqAFtBaAQOvj8VprqfG2JRGR5TEE20hKAErRWkvNs48nwraGmd0DlCFKRGSMIdgGlgpAwPSzwabP7BpXlJDixpYln/0REZnGEDSTJQMQaH4QvLlze/IZIBFR6xiCZrB0ALbE3Lk9q/Kq9LdATb0HW4dERAxB2cwNQEs8k6u9VCv5WCkD8LmCBBE5O4agDG1pAXr28cTg/MEGK8zL6eRSVVCF2lLpISgFV5AgImfHEJSoLQFoapC8nGnO2rI6PRERNY8hKEFbA7CtAcYWGxGRdXBR3Va0tRNMWwOM4/uIiKyHIdgCW/YCbarXG73YcYWIyMoYgs1QMgABwCPYw+oByFYmETk7PhM0wZIBWF1UbdZ57bzk/f/k5mdvxsVlF5vd33RuUo4TJCJiCBqxdAvQ3MVub5wiTcr4wJtG3NRiCHoP9mboERE1wRC8gTVugcpt0TW68Vala2fXVo/3CvMyGIfY9L0YgERExhiC/2OtZ4ByJr0Gri+g6xXmZRBard1S7b2iN0OOiMgMDEEo2wlGymoPrd1SdfVtvaVIRETGnD4Ele4F2nS1B1Ozy/xe+LvN6iEiciaKD5FYuXIlgoOD4e7ujsjISHz77bctHp+eno7IyEi4u7vjlltuwerVq83+bFsEYGvDEG7c3zi7TGZkpsHX2VfOWrQmIiK6TtGW4Keffopnn30WK1euxN133401a9Zg7NixyM3NRY8ePYyOLywsxLhx4zBjxgwkJyfj4MGDmDlzJrp27YrJkyfL+uzaK7Uo+FMBKn6sgIu3C4KeC0LlsUpUHqsEcL1Di1eYFwCg8kSl/pZkTUkN6n+vRzuPdvC41QPuPdxRXVQN3X91aPi9AQ2//3Hr0sXDBe63uiM8Ndyoc0vjc776inpUZFUAML2wrhTmdr4hInJ2GiGEUOrD77zzTtx+++1YtWqVfltoaCgmTpyIpKQko+PnzZuH7du3Iy/vj2WC4uPj8dNPPyEjI0PSZ5aXl8PX1xdpt6dBZNnuW79x9hdLT4gdmRnJBXSJiP6n8ff81atX4ePj0+KxijUhampqkJmZidjYWIPtsbGx+P77702ek5GRYXT8mDFjcOTIEdTWyltm6FrWNbj42m7GlBuf81l6QmzO/EJEZB7FboeWlZWhvr4e/v7+Btv9/f1RUlJi8pySkhKTx9fV1aGsrAyBgYFG5+h0Ouh0Ov3rq1evXt9+kw6BSwJx6q+n2vqtSFJ+rRyi/HrLs+JaBSpR2ab367u2Lzz6esDF2wV1/nUoLy+3RJlERHav8fehlBudivcO1Wg0Bq+FEEbbWjve1PZGSUlJWLBggdH2yVcmA3+VW20bjLDw+/1/Fn4/IiIHU1FRAV9f3xaPUSwEu3TpAhcXF6NWX2lpqVFrr1FAQIDJ49u3b4/OnTubPCcxMREJCQn611euXEHPnj1RVFTU6sVRWnl5OYKCgnD+/PlW72srzZ5qBeyrXtZqHazVOtRQqxACFRUV6NatW6vHKhaCbm5uiIyMxJ49ezBp0iT99j179mDChAkmz4mOjsbXX39tsC01NRVRUVFwdTU9YFyr1UKr1Rpt9/X1Vf0PUyMfHx/WaiX2VC9rtQ7Wah1K1yq1kaNo3/qEhAR8+OGH+Oijj5CXl4c5c+agqKgI8fHxAK634h577DH98fHx8Th37hwSEhKQl5eHjz76COvWrcPcuXOV+haIiMiOKfpM8MEHH8SlS5ewcOFCFBcXY8CAAdi5cyd69uwJACguLkZRUZH++ODgYOzcuRNz5szBihUr0K1bN7z//vuyxwgSEREBKugYM3PmTMycOdPkvg0bNhhtGzFiBLKyssz+PK1Wi9dee83kLVK1Ya3WY0/1slbrYK3WYU+1AgoPliciIlIS59siIiKnxRAkIiKnxRAkIiKn5ZAhqOTyTHLJqTUtLQ0ajcbo6+TJk1av88CBA4iLi0O3bt2g0Wjw1VdftXqOUtdVbq1KXtekpCTccccd8Pb2hp+fHyZOnIhTp1qfyk+Ja2tOrUpd21WrViE8PFw/Vi06Ohq7du1q8Rylfl7l1qrkz+uNkpKSoNFo8Oyzz7Z4nJK/XyURDmbLli3C1dVVfPDBByI3N1fMnj1beHl5iXPnzpk8/ueffxaenp5i9uzZIjc3V3zwwQfC1dVVfPHFF6qrdf/+/QKAOHXqlCguLtZ/1dXVWb3WnTt3ipdeekmkpKQIAGLr1q0tHq/kdZVbq5LXdcyYMWL9+vXi+PHjIicnR9x3332iR48e4tq1a82eo9S1NadWpa7t9u3bxY4dO8SpU6fEqVOnxPz584Wrq6s4fvy4yeOV/HmVW6uSP6+NDh8+LHr16iXCw8PF7Nmzmz1OyesqlcOF4ODBg0V8fLzBtn79+okXX3zR5PEvvPCC6Nevn8G2v//972LIkCFWq7GR3Fobf/h/++03q9fWEinBouR1vZGcEFT6ugohRGlpqQAg0tPTmz1GLddWSq1qurYdO3YUH374ocl9armmjVqqVelrWlFRIfr06SP27NkjRowY0WIIqu26muJQt0OVXp7J2rU2ioiIQGBgIGJiYrB//36r1dgWSl3XtlDDdW1c5aRTp07NHqOWayul1kZKXtv6+nps2bIFlZWViI6ONnmMWq6plFobKXVNZ82ahfvuuw/33HNPq8eq5bq2xKFC0BrLM6mp1sDAQKxduxYpKSn48ssv0bdvX8TExODAgQNWq9NcSl1Xc6jlugohkJCQgKFDh2LAgAHNHqeGayu1ViWv7bFjx9ChQwdotVrEx8dj69at6N+/v8ljlb6mcmpV8ppu2bIFWVlZJhc9N0Xp6yqF4jPGWIO1l2eyJDm19u3bF3379tW/jo6Oxvnz57F48WIMHz7cqnWaQ8nrKodarutTTz2Fo0eP4rvvvmv1WKWvrdRalby2ffv2RU5ODq5cuYKUlBRMmzYN6enpzYaLktdUTq1KXdPz589j9uzZSE1Nhbu7u+TzlP5ZbY1DtQRttTyTUrWaMmTIEBQUFFi6vDZT6rpaiq2v69NPP43t27dj//796N69e4vHKn1t5dRqiq2urZubG3r37o2oqCgkJSVh4MCBeO+990weq/Q1lVOrKba4ppmZmSgtLUVkZCTat2+P9u3bIz09He+//z7at2+P+vp6o3OUvq5SOFQI3rg804327NmDu+66y+Q50dHRRse3tjyTUrWakp2djcDAQEuX12ZKXVdLsdV1FULgqaeewpdffol9+/YhODi41XOUurbm1GqKUj+zQgjodDqT+9T289pSrabY4prGxMTg2LFjyMnJ0X9FRUVh6tSpyMnJgYuLi9E5aruuJinSHceKGocdrFu3TuTm5opnn31WeHl5ibNnzwohhHjxxRfFo48+qj++sQvvnDlzRG5urli3bp3Nh0hIrXXp0qVi69atIj8/Xxw/fly8+OKLAoBISUmxeq0VFRUiOztbZGdnCwBiyZIlIjs7Wz+cQ03XVW6tSl7XJ598Uvj6+oq0tDSD7u5VVVX6Y9Rybc2pValrm5iYKA4cOCAKCwvF0aNHxfz580W7du1EamqqyTqV/HmVW6uSP69NNe0dqqbrKpXDhaAQQqxYsUL07NlTuLm5idtvv92gC/e0adPEiBEjDI5PS0sTERERws3NTfTq1UusWrVKlbW+88474tZbbxXu7u6iY8eOYujQoWLHjh02qbOxW3bTr2nTppmsVQjlrqvcWpW8rqbqBCDWr1+vP0Yt19acWpW6tn/961/1/666du0qYmJi9KFiqk4hlPt5lVurkj+vTTUNQTVdV6m4igQRETkth3omSEREJAdDkIiInBZDkIiInBZDkIiInBZDkIiInBZDkIiInBZDkIiInBZDkIiInBZDkIiIbO7AgQOIi4tDt27doNFo8NVXX8l+j88++wyDBg2Cp6cnevbsiUWLFsl+D4YgkUSvv/46/P39Df7BmtpmTRs2bMBNN91k9c+xJEtfm5EjR+LZZ5+12PvJkZaWBo1GgytXrijy+Y6ksrISAwcOxPLly806f9euXZg6dSri4+Nx/PhxrFy5EkuWLJH/fkrP20bUkqKiIvHXv/5VBAYGCldXV9GjRw/xzDPPiLKyMrPer6qqSnh4eIi8vDz961dffVWEhIQINzc30blzZzF58mRx/Phxg/Nyc3MFALF161ZRXFwsqqurTW6zpJ49e4qlS5ca1f/LL79Y9HOa09DQINasWSMGDx4svLy8hK+vr4iMjBRLly4VlZWVkt+n8RpZStP5KlvTOJfsb7/91ubPtuR70R9M/YzodDrx/PPPi27duglPT08xePBgsX//fv3+hx9+WPzpT38yOGfp0qWie/fuoqGhQfJnsyVIqvXzzz8jKioK+fn52Lx5M06fPo3Vq1dj7969iI6OxuXLl2W/5549exAUFIR+/fpBp9PhnnvuwUcffYQ33ngD+fn52LlzJ+rr63HnnXfi0KFD+vPOnDkDAJgwYQICAgKg1WpNbmuNEAJ1dXWy627k4eEBPz8/s8+X49FHH8Wzzz6LCRMmYP/+/cjJycErr7yCbdu2ITU11SY1kPOaPn06Dh48iC1btuDo0aOYMmUK7r33Xv26iTqdzmhxXw8PD1y4cAHnzp2T/kFtCG8iq7r33ntF9+7dDZbqEUKI4uJi4enpKeLj4/XbevbsKd566y0xffp00aFDBxEUFCTWrFlj9J5//etfxdy5c4UQQrz99ttCo9GInJwcg2Pq6+tFVFSU6N+/v2hoaBCvvfaa0coJpraZ0thy2L17t4iMjBSurq5i37594vTp02L8+PHCz89PeHl5iaioKLFnzx79eSNGjDD5/uvXrxe+vr4Gn7Fy5Upxyy23CFdXVxESEiI2btwo/SI349NPPxUAxFdffWW0r6GhQVy5ckUIIcThw4fFPffcIzp37ix8fHzE8OHDRWZmpsHxaPK//PPnz4sHH3xQdOzYUXh6eorIyEhx6NAhIcT1VQgmTJhgcP7s2bMNViZo2hLctGmTiIyMFB06dBD+/v7i4Ycf1reWCwsLm11NpKGhQbzzzjsiODhYuLu7i/DwcPH5558bfPaOHTtEnz59hLu7uxg5cqRYv349W4JW0PRn5PTp00Kj0YiLFy8aHBcTEyMSExOFEEKsWbNGeHp6im+++UbU19eLU6dOiX79+gkA4vvvv5f+2Rb5Dogs7NKlS0Kj0Yh//OMfJvfPmDFDdOzYUX/bo2fPnqJTp05ixYoVoqCgQCQlJYl27drpb3sKcT3c/Pz8xLfffiuEECI8PFzExsaafP9PPvlEABDZ2dmioqJC/8uvcQ09U9tMaQzB8PBwkZqaKk6fPi3KyspETk6OWL16tTh69KjIz88XL730knB3d9eveXjp0iXRvXt3sXDhQoP3bxqCX375pXB1dRUrVqwQp06dEu+++65wcXER+/btk3fBmxg/frzo27dvq8ft3btXbNq0SeTm5orc3FzxxBNPCH9/f1FeXq4/5sZfcBUVFeKWW24Rw4YNE99++60oKCgQn376qf6XljkhuG7dOrFz505x5swZkZGRIYYMGSLGjh0rhBCirq5OpKSkCADi1KlTori4WB/g8+fPF/369RO7d+8WZ86cEevXrxdarVakpaUJIa7fitdqtWL27Nni5MmTIjk5Wfj7+zMEraBpCH722WcCgPDy8jL4at++vfjzn/8shLj+n5gXXnhBuLu7CxcXF9GxY0fx+uuvCwDihx9+kP7Zlv5miCzh0KFDLT5LWrJkiQCg/x9/z549xV/+8hf9/oaGBuHn52ewdtnBgwdFly5dRH19vRBCCHd392afLWVlZQkA4tNPPxVCCLF161aj1p6pbU01hqCpFlVT/fv3F//617/0r009E2wagnfddZeYMWOGwTFTpkwR48aNa/XzWhIaGirGjx8v+7y6ujrh7e0tvv76a/22G/8e16xZI7y9vcWlS5dMnm9OCDZ1+PBhAUBUVFQIIUw/x7t27Zpwd3c3ajE88cQT4uGHHxZCXF/sNjQ01OD50rx58xiCVtD03/qWLVuEi4uLOHnypCgoKDD4avofzrq6OnHhwgWh0+nEzp07DX4vSMFngmSXxP+WwdRoNPpt4eHh+j9rNBoEBASgtLRUv23btm24//770a5d6z/2pt6/LaKiogxeV1ZW4oUXXkD//v1x0003oUOHDjh58iSKiopkvW9eXh7uvvtug21333038vLyTB7/7bffokOHDvqvTz75xORxQghJ33tpaSni4+MREhICX19f+Pr64tq1a81+Hzk5OYiIiECnTp1afW+psrOzMWHCBPTs2RPe3t4YOXIkALR4LXNzc1FdXY3Ro0cbXI+NGzfqn/Xm5eVhyJAhBtchOjraYnVT8yIiIlBfX4/S0lL07t3b4CsgIMDgWBcXF9x8881wc3PD5s2bER0dLeu5eXtLF09kCb1794ZGo0Fubi4mTpxotP/kyZPo2LEjunTpot/m6upqcIxGo0FDQ4P+9fbt25GUlKR/HRISgtzcXJOff/LkSQBAnz592vJt6Hl5eRm8fv755/Gf//wHixcvRu/eveHh4YE//elPqKmpkf3eTcOqpQCLiopCTk6O/rW/v7/J40JCQpoN0hs9/vjj+PXXX7Fs2TL07NkTWq0W0dHRzX4fHh4eLb5fu3bt9P8BaVRbW9vs8ZWVlYiNjUVsbCySk5PRtWtXFBUVYcyYMS1ey8afix07duDmm2822NfYwalpHWRZ165dw+nTp/WvCwsLkZOTg06dOiEkJARTp07FY489hnfffRcREREoKyvDvn37cNttt2HcuHEoKyvDF198gZEjR6K6uhrr16/H559/jvT0dFl1sCVIqtS5c2eMHj0aK1euxO+//26wr6SkBJ988gkefPBByS21goICnD17FrGxsfptDz30EL755hv89NNPBsc2NDRg6dKl6N+/PwYOHNj2b8aEb7/9Fo8//jgmTZqE2267DQEBATh79qzBMW5ubqivr2/xfUJDQ/Hdd98ZbPv+++8RGhpq8ngPDw+D/1V7e3ubPO6RRx5Bfn4+tm3bZrRPCIGrV6/qv49nnnkG48aNQ1hYGLRaLcrKypqtNzw8HDk5Oc327O3atSuKi4sNtt0Y2k2dPHkSZWVlePvttzFs2DD069fPoPUPXL+OAAyuZf/+/aHValFUVGTU0ggKCtIfc2MPYQBGr8l8R44cQUREBCIiIgAACQkJiIiIwKuvvgoAWL9+PR577DE899xz6Nu3L8aPH48ffvhB//cDAB9//DGioqJw991348SJE0hLS8PgwYPlFdLmm7lEVpKfny+6dOkihg0bJtLT00VRUZHYtWuXGDBggOjTp4/BcyVTz88GDhwoXnvtNSGEEIsWLRL333+/wf7ff/9d3HnnnSIoKEh89tln4ty5c+Lw4cNi4sSJwsvLS2RkZOiPbeszwabPkCZOnCgGDRoksrOzRU5OjoiLixPe3t4Gz7pGjx4txo8fLy5cuCB+/fVXIYTxM8GtW7cKV1dXsWrVKpGfn6/vGHPjeCpzNDQ0iAcffFB4eHiIf/zjH+LHH38UZ8+eFV9//bX4f//v/+mf3wwaNEiMHj1a5ObmikOHDolhw4YJDw8Pg78L3PC8R6fTiZCQEDFs2DDx3XffiTNnzogvvvhC/2xu9+7dQqPRiI8//ljk5+eLV199Vfj4+DT7TLC0tFS4ubmJ559/Xpw5c0Zs27ZNhISE6Ds1CSHEhQsXhEajERs2bBClpaX6Z4UvvfSS6Ny5s9iwYYM4ffq0yMrKEsuXLxcbNmwQQghx7tw54ebmJubMmSNOnjwpPvnkExEQEMBngg6GIUiqdvbsWfH444+LgIAA4erqKoKCgsTTTz9tNFi+tRAcOnSo+OCDD4zev7KyUrz88suid+/ewtXVVXTq1ElMnjxZHDt2zOA4S4dgYWGhGDVqlPDw8BBBQUFi+fLlRh0+MjIyRHh4uNBqtTYfIiHE9d60q1atEnfccYfw9PQUPj4+IjIyUrz33nv6YStZWVkiKipKaLVa0adPH/H5558b/V2gSaeHs2fPismTJwsfHx/h6ekpoqKiDHrzvfrqq8Lf31/4+vqKOXPmiKeeeqrFjjH//ve/Ra9evYRWqxXR0dFi+/btBiEohBALFy4UAQEBQqPRGAyReO+990Tfvn2Fq6ur6Nq1qxgzZoxIT0/Xn/f111+L3r17C61WK4YNGyY++ugjhqCD0QjBG9/k2MrKyhAYGIjz588bPVQnIufGZ4Lk8C5fvowlS5YwAInICFuCRETktNgSJCIip8UQJCIip8UQJCIip8UQJCIip8UQJCIip8UQJCIip8UQJCIip8UQJCIip8UQJCIip/X/Az/FUiQFCLzvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_color = ['r','g','b','m']\n",
    "plt.figure()\n",
    "plt.scatter(x = test_result[:, 1], \n",
    "            y = test_result[:, 0],\n",
    "            marker = 's',\n",
    "            color = plot_color[ParameterNum-301],\n",
    "            s = 15)\n",
    "\n",
    "plt.xlabel(ParameterLabel[ParameterNum-301] + ' - Calculated')\n",
    "plt.ylabel(ParameterLabel[ParameterNum-301] +' - Predicted')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "plt.xlim([0,plt.xlim()[1]])\n",
    "plt.ylim([0,plt.ylim()[1]])\n",
    "if ParameterNum == 304 :\n",
    "    plt.plot([-1e-10, 1e+10], [-1e-10, 1e+10], color = plot_color[ParameterNum-301])\n",
    "else :\n",
    "    plt.plot([-100, 100], [-100, 100], color = plot_color[ParameterNum-301])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e4722-b293-4f69-80d3-f4d12681c50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a28b36-04c4-44ea-90e3-5f5e1c23cbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
